{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKk8sKZfqL39"
      },
      "source": [
        "#  üìù Evaluacion del Modulo 2 - \"IA para profesionales TIC ‚Äì 3¬™ edici√≥n\"\n",
        "\n",
        "#### ü™™ **Alumno**: Nicolas D'Alessandro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Statement"
      ],
      "metadata": {
        "id": "sExqxqv5Q3V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéØ Objetivo\n",
        "\n",
        "El objetivo de este ejercicio es construir y entrenar una red neuronal convolucional (CNN) para clasificar im√°genes de rostros humanos seg√∫n las emociones que muestran. Los estudiantes aprender√°n a trabajar con un dataset de im√°genes, a construir y entrenar un modelo de CNN utilizando TensorFlow y Keras, y a evaluar su desempe√±o.\n",
        "\n",
        "Pod√©is hacer uso de ChatGPT, Gemini, Github Copilot o cualquier otra herramienta, con un uso adecuado y siempre entendiendo lo que hac√©is, y lo que el c√≥digo hace.\n",
        "\n",
        "Descripci√≥n del Ejercicio\n",
        "\n",
        "En este ejercicio, utilizar√°s el dataset FER-2013 (Facial Expression Recognition) disponible en Kaggle. Este dataset contiene im√°genes de rostros etiquetados con diferentes emociones (felicidad, tristeza, enojo, sorpresa, etc.). Tu tarea es entrenar una CNN para clasificar las im√°genes de rostros en las categor√≠as de emoci√≥n correctas.\n",
        "\n",
        "\n",
        "\n",
        "### ü™ú Pasos a Seguir\n",
        "\n",
        "#### 1. Preparaci√≥n del Entorno:\n",
        "\n",
        "- Abre un nuevo notebook en Google Colab.\n",
        "- Aseg√∫rate de tener instaladas las bibliotecas necesarias: TensorFlow, Keras, y otras bibliotecas auxiliares.\n",
        "\n",
        "#### 2. Carga y Exploraci√≥n del Dataset:\n",
        "\n",
        "- Descarga el dataset FER-2013 desde Kaggle.\n",
        "- Carga el dataset en tu notebook y explora las im√°genes y sus etiquetas.\n",
        "\n",
        "#### 3.Preprocesamiento de Datos:\n",
        "\n",
        "- Realiza el preprocesamiento necesario de las im√°genes, como el cambio de tama√±o, la normalizaci√≥n y la divisi√≥n en conjuntos de entrenamiento y validaci√≥n.\n",
        "\n",
        "#### 4.Construcci√≥n del Modelo:\n",
        "\n",
        "- Define una arquitectura de red neuronal convolucional utilizando Keras. Puedes empezar con una arquitectura simple y luego experimentar con arquitecturas m√°s complejas.\n",
        "- Compila el modelo especificando el optimizador, la funci√≥n de p√©rdida y las m√©tricas de evaluaci√≥n.\n",
        "\n",
        "#### 5. Entrenamiento del Modelo:\n",
        "\n",
        "- Entrena tu modelo con el conjunto de entrenamiento.\n",
        "- Eval√∫a el desempe√±o del modelo en el conjunto de validaci√≥n.\n",
        "\n",
        "#### 6. Evaluaci√≥n y Mejora:\n",
        "\n",
        "- Analiza los resultados y ajusta la arquitectura y los hiperpar√°metros del modelo para mejorar su precisi√≥n.\n",
        "- Realiza predicciones sobre nuevas im√°genes de rostros, busca rostros y aj√∫stalos a lo que pide el modelo, y verifica su exactitud.\n",
        "\n",
        "#### 7. Desaf√≠o Extra:\n",
        "\n",
        "- Crear una visualizaci√≥n interactiva que muestre las emociones detectadas en tiempo real usando la webcam del ordenador (Google Colab, si busc√°is, lo permite).\n",
        "\n",
        "#### 8. A entregar en un .zip con vuestro nombre:\n",
        "\n",
        "- Documenta todo el proceso en el notebook.\n",
        "- Incluye gr√°ficos de precisi√≥n y p√©rdida durante el entrenamiento.\n",
        "- Muestra ejemplos de im√°genes correctamente clasificadas y mal clasificadas.\n",
        "- Comenta sobre los resultados obtenidos y las posibles mejoras o lo que consideres en un PDF.\n",
        "\n",
        "\n",
        "\n",
        "### üß™ Evaluaci√≥n\n",
        "\n",
        "La evaluaci√≥n se basar√° en:\n",
        "\n",
        "- La correcta implementaci√≥n del modelo de CNN.\n",
        "- La claridad y detalle en la documentaci√≥n del proceso.\n",
        "- La precisi√≥n del modelo en el conjunto de validaci√≥n.\n",
        "- La creatividad en la mejora del modelo y en el uso de t√©cnicas de aumento de datos.\n"
      ],
      "metadata": {
        "id": "Z3BTGs17PXYW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlJKTJdBnWNt"
      },
      "source": [
        "## Step 1Ô∏è‚É£: Preparing the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1Ô∏è‚É£.0Ô∏è‚É£ Setup a propper Runtime"
      ],
      "metadata": {
        "id": "CV7DXYKYqJhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA0xa0vip_y1",
        "outputId": "1cda35bc-c192-465a-dc02-92101e9ecf38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 17 20:45:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   40C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1Ô∏è‚É£.1Ô∏è‚É£ Ensure we have installed the required libraries"
      ],
      "metadata": {
        "id": "SZEa87RhNSXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras numpy matplotlib opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7MVnsfoNS8_",
        "outputId": "868b5562-3bf9-4484-a60f-7e2203a80bd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtDRHWsZPSam",
        "outputId": "afaaefa3-aae6-4360-ace6-4dee5d68c587"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Collecting kagglehub\n",
            "  Downloading kagglehub-0.3.8-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading kagglehub-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kagglehub\n",
            "  Attempting uninstall: kagglehub\n",
            "    Found existing installation: kagglehub 0.3.7\n",
            "    Uninstalling kagglehub-0.3.7:\n",
            "      Successfully uninstalled kagglehub-0.3.7\n",
            "Successfully installed kagglehub-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkpL-YBrpur8"
      },
      "source": [
        "### 1Ô∏è‚É£.2Ô∏è‚É£ Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOy68DXNp2nk"
      },
      "outputs": [],
      "source": [
        "# Data Manipulation and Analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning Framework (TensorFlow & Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Computer Vision (OpenCV)\n",
        "import cv2\n",
        "\n",
        "# File Handling and Directory Management\n",
        "import os\n",
        "\n",
        "# Kaggle API for Dataset Access\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì Explanation:\n",
        "\n",
        "- **TensorFlow/Keras** ‚Üí Libraries to build and train the CNN.\n",
        "- **NumPy/Matplotlib** ‚Üí Data manipulation and image visualization.\n",
        "- **OpenCV** ‚Üí Image processing."
      ],
      "metadata": {
        "id": "KobN_fHpN7rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2Ô∏è‚É£: Loading & Exploring the Dataset"
      ],
      "metadata": {
        "id": "3a_cwr9hge9i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gWzff_pekE"
      },
      "source": [
        "### 2Ô∏è‚É£.1Ô∏è‚É£ Set the Path to the Files in Kagglehub and Load them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üîπ Verify Dataset Path"
      ],
      "metadata": {
        "id": "080JXzjyTwXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifECURvhO6VO",
        "outputId": "f1101f6b-d01d-4469-f719-68fd4fb0478b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3Ô∏è‚É£: Data Pre-processing"
      ],
      "metadata": {
        "id": "yJCyzERdjD2B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY2bjXJ3o-Et"
      },
      "source": [
        "### 3Ô∏è‚É£.1Ô∏è‚É£ Verify Data Classes and Format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üîπ Set Up Image Data Generators"
      ],
      "metadata": {
        "id": "WdKJDd5_Tkwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leGOkWXhpHN8",
        "outputId": "e206bb2c-8e5c-4854-ae3c-60611102d41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # Normalize pixel values (0-1)\n",
        "    validation_split=0.2,   # 80% training, 20% validation\n",
        "    rotation_range=10,      # Augmentation: small rotations\n",
        "    zoom_range=0.1,         # Augmentation: zoom\n",
        "    horizontal_flip=True    # Augmentation: flip images\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùìWhat‚Äôs happening?\n",
        "\n",
        "- `rescale=1./255` ‚Üí Normalizes pixel values from `[0,255]` to `[0,1]`.\n",
        "- `validation_split=0.2` ‚Üí Splits 20% for validation.\n",
        "- Augmentations (rotation, zoom, flip) help prevent overfitting."
      ],
      "metadata": {
        "id": "zfeYHHBFT-vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üîπ Check Data Distribution"
      ],
      "metadata": {
        "id": "BJB0ZrdbTlyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp2zRyOVpHDV",
        "outputId": "ff7706a0-216b-4000-b1b5-6e0e29770e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
          ]
        }
      ],
      "source": [
        "# View detected classes\n",
        "print(\"Classes:\", train_generator.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####üîπ Verify Sample Images"
      ],
      "metadata": {
        "id": "CC4ItVgVT3QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch of images\n",
        "x_batch, y_batch = next(train_generator)\n",
        "\n",
        "# Plot sample images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(x_batch[i])\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "kVami3ZZT4G_",
        "outputId": "68d1d9b5-17ce-4294-ac1a-f8417d7662f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå **STEP 1, 2, 3 Conclusion**: Loading & Exploring the Dataset"
      ],
      "metadata": {
        "id": "txPzADAIScWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the images organized into **7 kinds of emotions**:\n",
        "\n",
        "| Class | Numeric Label |\n",
        "|--------|----------------|\n",
        "| **Angry** (Enojo) | 0 |\n",
        "| **Disgust** (Disgusto) | 1 |\n",
        "| **Fear** (Miedo) | 2 |\n",
        "| **Happy** (Felicidad) | 3 |\n",
        "| **Neutral** (Neutralidad) | 4 |\n",
        "| **Sad** (Tristeza) | 5 |\n",
        "| **Surprise** (Sorpresa) | 6 |"
      ],
      "metadata": {
        "id": "UqOwOsmRUsB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå **Dataset summary**:\n",
        "- **Training**: **22,968 images** ‚úÖ\n",
        "- **Validation**: **5,741 images** ‚úÖ\n",
        "- **Test**: **7,178 images** ‚úÖ"
      ],
      "metadata": {
        "id": "np-D4Z_LUoKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **‚úÖ Summary of Steps Completed So Far**  \n",
        "\n",
        "1Ô∏è‚É£ **Set Up the Environment**  \n",
        "   - Installed required libraries (`tensorflow`, `keras`, `numpy`, `opencv`, `matplotlib`, `kagglehub`).  \n",
        "   - Imported necessary Python modules.  \n",
        "\n",
        "2Ô∏è‚É£ **Loaded the FER-2013 Dataset**  \n",
        "   - Verified dataset structure (`train/`, `test/` folders with images per class).  \n",
        "   - Listed class labels: **Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise**.  \n",
        "\n",
        "3Ô∏è‚É£ **Preprocessed & Augmented Data**  \n",
        "   - Used `ImageDataGenerator` to load **images efficiently**.  \n",
        "   - Applied **data augmentation**: rotations, zoom, and horizontal flips.  \n",
        "   - **Normalized pixel values** (0-255 ‚Üí 0-1).  \n",
        "   - Split dataset: **80% training / 20% validation**.  \n",
        "\n",
        "4Ô∏è‚É£ **Explored Data**  \n",
        "   - Verified class distribution.  \n",
        "   - Displayed **sample images** to confirm correct loading.\n",
        "   "
      ],
      "metadata": {
        "id": "m27usQf2V2Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4Ô∏è‚É£: Build the CNN Architecture"
      ],
      "metadata": {
        "id": "NCkLrfnrgc9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ÑπÔ∏è INFO: Key Considerations for the CNN Architecture\n",
        "\n",
        "- **Feature Extraction**: Use **multiple convolutional layers** to extract patterns (edges, textures, shapes).  \n",
        "- **Dimensionality Reduction**: Apply **MaxPooling layers** to reduce feature map size and computation cost.  \n",
        "- **Fully Connected Layers**: Translate learned features into **emotion classification**.  \n",
        "- **Regularization**: Use **Dropout layers** to **prevent overfitting**.  \n",
        "- **Activation Functions**: Use **ReLU for hidden layers** and **Softmax for output (7 classes)**."
      ],
      "metadata": {
        "id": "pocr5qttgc6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4Ô∏è‚É£.1Ô∏è‚É£ Define the CNN Model"
      ],
      "metadata": {
        "id": "q8TzUNT5lk8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ÑπÔ∏è Explanation of the Architecture\n",
        "\n",
        "- **Feature Extraction**:\n",
        "  - **3 convolutional blocks** with increasing filters (**64 ‚Üí 128 ‚Üí 256**).\n",
        "  - **Batch Normalization** to stabilize training and speed up convergence.\n",
        "  - **MaxPooling** to **reduce spatial dimensions**.\n",
        "\n",
        "- **Classification Layers**:\n",
        "  - **Flatten** layer converts feature maps into a single vector.\n",
        "  - **2 Fully Connected (`Dense`) layers** with **Dropout** for regularization.\n",
        "  - **Softmax output layer** (7 neurons) for **multi-class classification**.\n",
        "\n",
        "- **Optimization & Loss Function**:\n",
        "  - **Adam Optimizer** with a small **learning rate (0.0001)**.\n",
        "  - **Categorical Cross-Entropy Loss** (since we have 7 classes)."
      ],
      "metadata": {
        "id": "h3AYOcaQgc04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    # Convolutional Block 1\n",
        "    Conv2D(64, (3,3), activation='relu', input_shape=(48,48,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Convolutional Block 3\n",
        "    Conv2D(256, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Flattening Layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully Connected Layer 1\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Regularization\n",
        "\n",
        "    # Fully Connected Layer 2\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output Layer (7 emotions)\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "F4FPd1WolcFE",
        "outputId": "ca9815cc-8932-4093-9006-d5120e78d465"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)          ‚îÇ           \u001b[38;5;34m1,792\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)          ‚îÇ             \u001b[38;5;34m256\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         ‚îÇ          \u001b[38;5;34m73,856\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         ‚îÇ             \u001b[38;5;34m512\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)           ‚îÇ         \u001b[38;5;34m295,168\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)           ‚îÇ           \u001b[38;5;34m1,024\u001b[0m ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 ‚îÇ       \u001b[38;5;34m2,097,664\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)                    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ         \u001b[38;5;34m131,328\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   ‚îÇ           \u001b[38;5;34m1,799\u001b[0m ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                         </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape                </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_1                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalization_2                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 ‚îÇ                             ‚îÇ                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,603,399\u001b[0m (9.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,603,399</span> (9.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,602,503\u001b[0m (9.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,602,503</span> (9.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå **STEP 4 Conclusion**: Defining the CNN Model"
      ],
      "metadata": {
        "id": "6DdwJKaeuVU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ùì **What We Did?**\n",
        "\n",
        "1Ô∏è‚É£ **Designed a Deep Convolutional Neural Network (CNN)**\n",
        "   - Built a multi-layer CNN to classify facial emotions from images.  \n",
        "   - Incorporated **Convolutional (Conv2D) layers** for feature extraction.  \n",
        "   - Used **Batch Normalization** for stable and faster training.  \n",
        "   - Applied **MaxPooling layers** to reduce dimensionality.  \n",
        "\n",
        "2Ô∏è‚É£ **Added Fully Connected (Dense) Layers**\n",
        "   - Flattened feature maps into a single vector (`Flatten`).  \n",
        "   - **Dense layers (512 & 256 neurons)** learn complex patterns.  \n",
        "   - Applied **Dropout (50%)** to prevent overfitting.  \n",
        "   - Used **Softmax output layer (7 neurons)** for classification into 7 emotions.  \n",
        "\n",
        "3Ô∏è‚É£ **Compiled the Model**\n",
        "   - **Optimizer**: Adam (learning rate = 0.0001)  \n",
        "   - **Loss Function**: Categorical Crossentropy  \n",
        "   - **Metric**: Accuracy"
      ],
      "metadata": {
        "id": "Z2QbibuFgcyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‚ùì **What Have We Obtained?**\n",
        "\n",
        "‚úÖ **Successfully Built a Deep CNN Model for Facial Emotion Recognition**  \n",
        "- **Model Summary:**\n",
        "  - **2.6M Trainable Parameters**  \n",
        "  - **9.93MB total size**  \n",
        "  - **3 Convolutional Blocks** with increasing filters (**64 ‚Üí 128 ‚Üí 256**)  \n",
        "  - **MaxPooling layers** for size reduction  \n",
        "  - **Dense layers (512, 256 neurons)** with Dropout (50%)"
      ],
      "metadata": {
        "id": "ga6wQnDCtYoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5Ô∏è‚É£: Training the CNN Model & Evaluating Performance"
      ],
      "metadata": {
        "id": "vPVJv7YLgcvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ÑπÔ∏è INFO: Key Considerations Before Training\n",
        "\n",
        "- **How do we ensure a robust training process?**  \n",
        "  - **Monitor Model Performance**: Track accuracy and loss over epochs.  \n",
        "  - **Prevent Overfitting**: Use **early stopping** and **learning rate reduction**.  \n",
        "  - **Efficient Training**: Utilize **GPU acceleration** in Colab for faster computation."
      ],
      "metadata": {
        "id": "m5rLLAANgcsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5Ô∏è‚É£.1Ô∏è‚É£ Define Callbacks for Efficient Training"
      ],
      "metadata": {
        "id": "zcVU9GlLrQEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####‚ùì We'll use **callbacks** to optimize training dynamically:\n",
        "\n",
        "  - **EarlyStopping**: Stops training when validation loss stops improving.  \n",
        "  - **ReduceLROnPlateau**: Reduces the learning rate if validation loss stagnates."
      ],
      "metadata": {
        "id": "_kfM-y9vrgMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùì **Why?**\n",
        "- **Stops training early** to avoid overfitting.  \n",
        "- **Reduces learning rate** dynamically for fine-tuned adjustments."
      ],
      "metadata": {
        "id": "AR4r_CcUqpMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', patience=5, restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"best_model.h5\", monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, reduce_lr, checkpoint]"
      ],
      "metadata": {
        "id": "lnCwI9zzqhH1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5Ô∏è‚É£.2Ô∏è‚É£ Train the **CNN** Model\n",
        "Now, we train the model using the **training dataset (`train_generator`)** and evaluate on **validation data (`val_generator`)**."
      ],
      "metadata": {
        "id": "qKpuKTfdq3U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,  # Optimized number of epochs\n",
        "    batch_size = 64,  # Defined previously in ImageDataGenerator\n",
        "    callbacks=callbacks  # Use defined callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVHSKfedq54D",
        "outputId": "2a91f061-334b-4c77-99cd-81506ed32d99"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3366 - loss: 1.6956\n",
            "Epoch 1: val_loss improved from inf to 1.58726, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.3366 - loss: 1.6956 - val_accuracy: 0.3870 - val_loss: 1.5873 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 1.6244\n",
            "Epoch 2: val_loss improved from 1.58726 to 1.52033, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 1.6244 - val_accuracy: 0.4170 - val_loss: 1.5203 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3840 - loss: 1.5866\n",
            "Epoch 3: val_loss improved from 1.52033 to 1.48530, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 1s/step - accuracy: 0.3840 - loss: 1.5866 - val_accuracy: 0.4313 - val_loss: 1.4853 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4030 - loss: 1.5404\n",
            "Epoch 4: val_loss improved from 1.48530 to 1.46726, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.4030 - loss: 1.5404 - val_accuracy: 0.4393 - val_loss: 1.4673 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4214 - loss: 1.5026\n",
            "Epoch 5: val_loss improved from 1.46726 to 1.43382, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - accuracy: 0.4213 - loss: 1.5027 - val_accuracy: 0.4520 - val_loss: 1.4338 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4352 - loss: 1.4650\n",
            "Epoch 6: val_loss did not improve from 1.43382\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 1s/step - accuracy: 0.4352 - loss: 1.4650 - val_accuracy: 0.4477 - val_loss: 1.4385 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4467 - loss: 1.4476\n",
            "Epoch 7: val_loss improved from 1.43382 to 1.41381, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.4467 - loss: 1.4476 - val_accuracy: 0.4531 - val_loss: 1.4138 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4475 - loss: 1.4373\n",
            "Epoch 8: val_loss improved from 1.41381 to 1.37501, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.4475 - loss: 1.4373 - val_accuracy: 0.4837 - val_loss: 1.3750 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4735 - loss: 1.4004\n",
            "Epoch 9: val_loss improved from 1.37501 to 1.35054, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.4735 - loss: 1.4004 - val_accuracy: 0.4828 - val_loss: 1.3505 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4760 - loss: 1.3739\n",
            "Epoch 10: val_loss improved from 1.35054 to 1.32422, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.4760 - loss: 1.3739 - val_accuracy: 0.4994 - val_loss: 1.3242 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 1.3338\n",
            "Epoch 11: val_loss improved from 1.32422 to 1.32118, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 1.3338 - val_accuracy: 0.5050 - val_loss: 1.3212 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5020 - loss: 1.3138\n",
            "Epoch 12: val_loss did not improve from 1.32118\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.5020 - loss: 1.3138 - val_accuracy: 0.4980 - val_loss: 1.3351 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4997 - loss: 1.3157\n",
            "Epoch 13: val_loss improved from 1.32118 to 1.28444, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.4997 - loss: 1.3157 - val_accuracy: 0.5086 - val_loss: 1.2844 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5092 - loss: 1.2884\n",
            "Epoch 14: val_loss did not improve from 1.28444\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.5092 - loss: 1.2884 - val_accuracy: 0.5060 - val_loss: 1.3067 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5118 - loss: 1.2800\n",
            "Epoch 15: val_loss did not improve from 1.28444\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.5118 - loss: 1.2800 - val_accuracy: 0.5142 - val_loss: 1.2972 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5Ô∏è‚É£.3Ô∏è‚É£ Evaluate Model Performance\n",
        "\n",
        "- After training, we analyze its performance on the **validation set**."
      ],
      "metadata": {
        "id": "SsHrBLpXsMAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on validation data\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f\"üìä Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"üìâ Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdH_-7tZsVJS",
        "outputId": "5a32243c-d888-489c-9951-3c0e53ac35ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 265ms/step - accuracy: 0.5153 - loss: 1.2757\n",
            "üìä Validation Accuracy: 0.5170\n",
            "üìâ Validation Loss: 1.2717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5Ô∏è‚É£.4Ô∏è‚É£ Visualize Training Progress\n",
        "\n",
        "- We plot **accuracy** and **loss** curves to assess model performance.\n"
      ],
      "metadata": {
        "id": "nl9gP7nDserC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy plot\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training & Validation Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wCPObQOqsoyD",
        "outputId": "9e4f7d74-a6fc-4443-d8e2-2bfde619bd2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss plot\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "c3njMhsVsrJU",
        "outputId": "588e24b9-c764-49e3-894a-eb1c19650436"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå **STEP 5 Conclusion**:  Final Training Analysis & Model Performance"
      ],
      "metadata": {
        "id": "WZV0NyDdqvHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ùì **What We Did?**  \n",
        "\n",
        "1Ô∏è‚É£ **Set Callbacks** (`EarlyStopping` & `ReduceLROnPlateau`) to optimize training.  \n",
        "2Ô∏è‚É£ **Trained the CNN** using `train_generator` (images) and validated with `val_generator`.  \n",
        "3Ô∏è‚É£ **Evaluated model performance** on the validation set.  \n",
        "4Ô∏è‚É£ **Plotted accuracy and loss curves** to visualize training progress."
      ],
      "metadata": {
        "id": "1A2tE8N5tEWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîπ Concerns**\n",
        "‚ö†Ô∏è **Validation Accuracy (~51%) is still low.**  \n",
        "- This suggests the model **may not be complex enough** to fully capture the patterns in facial emotions.  \n",
        "- The dataset is likely **challenging**, and additional improvements might be needed.  "
      ],
      "metadata": {
        "id": "_I-pgHhYRl3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "unQr-DJUKfCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "oJ6Zj40anKyQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmFIb52bnNYI",
        "outputId": "acf868d0-f4a9-419c-e36f-574986370ea0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atuP59sQnSTW",
        "outputId": "342630bb-7743-454a-fdb6-601b2d4bae69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an improved CNN architecture\n",
        "model = Sequential([\n",
        "    Input(shape=(48,48,3)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "zzEQDi25nWj_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jIJ7eyI-nbvm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.h5\", save_best_only=True, monitor='val_loss', verbose=1)"
      ],
      "metadata": {
        "id": "vY9So-sAtgkC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 30\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RhL_QunmFee",
        "outputId": "18ad7cc2-7b86-460d-9076-0c78ff48a2d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2443 - loss: 2.8862\n",
            "Epoch 1: val_loss improved from inf to 2.49690, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.2443 - loss: 2.8858 - val_accuracy: 0.2674 - val_loss: 2.4969 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2782 - loss: 2.3884\n",
            "Epoch 2: val_loss improved from 2.49690 to 2.35978, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.2782 - loss: 2.3883 - val_accuracy: 0.2540 - val_loss: 2.3598 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3135 - loss: 2.2070\n",
            "Epoch 3: val_loss improved from 2.35978 to 2.35263, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3135 - loss: 2.2070 - val_accuracy: 0.2879 - val_loss: 2.3526 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3405 - loss: 2.2205\n",
            "Epoch 4: val_loss did not improve from 2.35263\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.3406 - loss: 2.2205 - val_accuracy: 0.1803 - val_loss: 2.6458 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3645 - loss: 2.1984\n",
            "Epoch 5: val_loss improved from 2.35263 to 2.30495, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3645 - loss: 2.1985 - val_accuracy: 0.3729 - val_loss: 2.3050 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3894 - loss: 2.2087\n",
            "Epoch 6: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3894 - loss: 2.2087 - val_accuracy: 0.2930 - val_loss: 2.3339 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4012 - loss: 2.1774\n",
            "Epoch 7: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4011 - loss: 2.1776 - val_accuracy: 0.2984 - val_loss: 2.5419 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3854 - loss: 2.3788\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3854 - loss: 2.3787 - val_accuracy: 0.3937 - val_loss: 2.3375 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4232 - loss: 2.2200\n",
            "Epoch 9: val_loss improved from 2.30495 to 2.06239, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4232 - loss: 2.2197 - val_accuracy: 0.4233 - val_loss: 2.0624 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4385 - loss: 2.0071\n",
            "Epoch 10: val_loss improved from 2.06239 to 1.89361, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4385 - loss: 2.0071 - val_accuracy: 0.4734 - val_loss: 1.8936 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4481 - loss: 1.9351\n",
            "Epoch 11: val_loss did not improve from 1.89361\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4481 - loss: 1.9351 - val_accuracy: 0.3146 - val_loss: 2.3201 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4584 - loss: 1.9135\n",
            "Epoch 12: val_loss improved from 1.89361 to 1.85447, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4584 - loss: 1.9135 - val_accuracy: 0.4809 - val_loss: 1.8545 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4670 - loss: 1.8943\n",
            "Epoch 13: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4670 - loss: 1.8943 - val_accuracy: 0.4377 - val_loss: 1.9615 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4602 - loss: 1.9006\n",
            "Epoch 14: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4602 - loss: 1.9007 - val_accuracy: 0.4234 - val_loss: 2.0217 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4637 - loss: 1.9112\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 15: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4638 - loss: 1.9112 - val_accuracy: 0.4464 - val_loss: 1.9170 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4782 - loss: 1.8752\n",
            "Epoch 16: val_loss improved from 1.85447 to 1.73257, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4783 - loss: 1.8750 - val_accuracy: 0.5030 - val_loss: 1.7326 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4941 - loss: 1.7529\n",
            "Epoch 17: val_loss improved from 1.73257 to 1.73059, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4941 - loss: 1.7529 - val_accuracy: 0.4931 - val_loss: 1.7306 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5018 - loss: 1.7106\n",
            "Epoch 18: val_loss improved from 1.73059 to 1.63947, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5018 - loss: 1.7106 - val_accuracy: 0.5206 - val_loss: 1.6395 - learning_rate: 2.5000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5033 - loss: 1.6762\n",
            "Epoch 19: val_loss improved from 1.63947 to 1.61138, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5033 - loss: 1.6763 - val_accuracy: 0.5231 - val_loss: 1.6114 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4997 - loss: 1.6850\n",
            "Epoch 20: val_loss did not improve from 1.61138\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4997 - loss: 1.6850 - val_accuracy: 0.4994 - val_loss: 1.6687 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4964 - loss: 1.6693\n",
            "Epoch 21: val_loss did not improve from 1.61138\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4964 - loss: 1.6692 - val_accuracy: 0.5037 - val_loss: 1.6687 - learning_rate: 2.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5047 - loss: 1.6634\n",
            "Epoch 22: val_loss improved from 1.61138 to 1.59607, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5048 - loss: 1.6633 - val_accuracy: 0.5187 - val_loss: 1.5961 - learning_rate: 2.5000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5022 - loss: 1.6572\n",
            "Epoch 23: val_loss did not improve from 1.59607\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.5022 - loss: 1.6572 - val_accuracy: 0.5271 - val_loss: 1.5974 - learning_rate: 2.5000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5136 - loss: 1.6297\n",
            "Epoch 24: val_loss improved from 1.59607 to 1.58312, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5136 - loss: 1.6297 - val_accuracy: 0.5327 - val_loss: 1.5831 - learning_rate: 2.5000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5188 - loss: 1.6335\n",
            "Epoch 25: val_loss improved from 1.58312 to 1.56269, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5188 - loss: 1.6335 - val_accuracy: 0.5419 - val_loss: 1.5627 - learning_rate: 2.5000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5135 - loss: 1.6195\n",
            "Epoch 26: val_loss did not improve from 1.56269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5135 - loss: 1.6195 - val_accuracy: 0.5236 - val_loss: 1.5855 - learning_rate: 2.5000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5165 - loss: 1.6107\n",
            "Epoch 27: val_loss improved from 1.56269 to 1.56126, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5165 - loss: 1.6107 - val_accuracy: 0.5360 - val_loss: 1.5613 - learning_rate: 2.5000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5192 - loss: 1.6049\n",
            "Epoch 28: val_loss did not improve from 1.56126\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5192 - loss: 1.6049 - val_accuracy: 0.5325 - val_loss: 1.5751 - learning_rate: 2.5000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5152 - loss: 1.6079\n",
            "Epoch 29: val_loss improved from 1.56126 to 1.53240, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.5152 - loss: 1.6079 - val_accuracy: 0.5464 - val_loss: 1.5324 - learning_rate: 2.5000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5195 - loss: 1.6065\n",
            "Epoch 30: val_loss did not improve from 1.53240\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5195 - loss: 1.6065 - val_accuracy: 0.5414 - val_loss: 1.5574 - learning_rate: 2.5000e-04\n",
            "Restoring model weights from the end of the best epoch: 29.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne4gMCpYmNcu",
        "outputId": "04f64b6d-3655-401c-9514-b199d46aa870"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5866 - loss: 1.4048\n",
            "Test Accuracy: 0.5766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "_HHxCM4y0hJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Load Pretrained Model (VGG16) for Transfer Learning\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "for layer in base_model.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define an improved CNN architecture with Transfer Learning\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = RMSprop(learning_rate=0.0005)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 30\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjJ9yUrzYUr",
        "outputId": "edf2ed57-2e92-46a9-d33e-6cd05e3f6a02"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.1880 - loss: 2.5962\n",
            "Epoch 1: val_loss improved from inf to 4.55887, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - accuracy: 0.1881 - loss: 2.5956 - val_accuracy: 0.2625 - val_loss: 4.5589 - learning_rate: 5.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2417 - loss: 2.0313\n",
            "Epoch 2: val_loss improved from 4.55887 to 3.23369, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.2418 - loss: 2.0312 - val_accuracy: 0.2714 - val_loss: 3.2337 - learning_rate: 5.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2737 - loss: 1.8596\n",
            "Epoch 3: val_loss did not improve from 3.23369\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.2737 - loss: 1.8595 - val_accuracy: 0.2534 - val_loss: 5.5233 - learning_rate: 5.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2928 - loss: 1.7926\n",
            "Epoch 4: val_loss did not improve from 3.23369\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.2928 - loss: 1.7926 - val_accuracy: 0.1686 - val_loss: 3.5972 - learning_rate: 5.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2962 - loss: 1.7657\n",
            "Epoch 5: val_loss improved from 3.23369 to 2.94297, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.2963 - loss: 1.7657 - val_accuracy: 0.2500 - val_loss: 2.9430 - learning_rate: 5.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3157 - loss: 1.7407\n",
            "Epoch 6: val_loss improved from 2.94297 to 2.10933, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3157 - loss: 1.7407 - val_accuracy: 0.3130 - val_loss: 2.1093 - learning_rate: 5.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3297 - loss: 1.7301\n",
            "Epoch 7: val_loss improved from 2.10933 to 1.95145, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3297 - loss: 1.7301 - val_accuracy: 0.2895 - val_loss: 1.9514 - learning_rate: 5.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3271 - loss: 1.7042\n",
            "Epoch 8: val_loss improved from 1.95145 to 1.88526, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.3271 - loss: 1.7042 - val_accuracy: 0.2602 - val_loss: 1.8853 - learning_rate: 5.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3384 - loss: 1.7068\n",
            "Epoch 9: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3384 - loss: 1.7068 - val_accuracy: 0.2583 - val_loss: 2.0362 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3449 - loss: 1.6622\n",
            "Epoch 10: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3449 - loss: 1.6622 - val_accuracy: 0.2576 - val_loss: 2.1245 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.3495 - loss: 1.6443\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3495 - loss: 1.6443 - val_accuracy: 0.2919 - val_loss: 2.1796 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3542 - loss: 1.6372\n",
            "Epoch 12: val_loss improved from 1.88526 to 1.68430, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3543 - loss: 1.6372 - val_accuracy: 0.3560 - val_loss: 1.6843 - learning_rate: 2.5000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3768 - loss: 1.6153\n",
            "Epoch 13: val_loss did not improve from 1.68430\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3768 - loss: 1.6153 - val_accuracy: 0.2533 - val_loss: 7.8442 - learning_rate: 2.5000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3800 - loss: 1.6149\n",
            "Epoch 14: val_loss improved from 1.68430 to 1.66495, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.3799 - loss: 1.6148 - val_accuracy: 0.3682 - val_loss: 1.6649 - learning_rate: 2.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3719 - loss: 1.5913\n",
            "Epoch 15: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3719 - loss: 1.5913 - val_accuracy: 0.3202 - val_loss: 1.7170 - learning_rate: 2.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3840 - loss: 1.6009\n",
            "Epoch 16: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3840 - loss: 1.6010 - val_accuracy: 0.0324 - val_loss: 8.3529 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3904 - loss: 1.5787\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 17: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3904 - loss: 1.5788 - val_accuracy: 0.3276 - val_loss: 1.7860 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3825 - loss: 1.5814\n",
            "Epoch 18: val_loss improved from 1.66495 to 1.62765, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3825 - loss: 1.5814 - val_accuracy: 0.3621 - val_loss: 1.6277 - learning_rate: 1.2500e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4043 - loss: 1.5543\n",
            "Epoch 19: val_loss improved from 1.62765 to 1.56853, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4043 - loss: 1.5543 - val_accuracy: 0.4013 - val_loss: 1.5685 - learning_rate: 1.2500e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3993 - loss: 1.5666\n",
            "Epoch 20: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3993 - loss: 1.5666 - val_accuracy: 0.3654 - val_loss: 1.6400 - learning_rate: 1.2500e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3986 - loss: 1.5543\n",
            "Epoch 21: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3986 - loss: 1.5543 - val_accuracy: 0.3815 - val_loss: 1.6100 - learning_rate: 1.2500e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3987 - loss: 1.5577\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 22: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3987 - loss: 1.5577 - val_accuracy: 0.3982 - val_loss: 1.5697 - learning_rate: 1.2500e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4067 - loss: 1.5505\n",
            "Epoch 23: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4067 - loss: 1.5505 - val_accuracy: 0.3897 - val_loss: 1.5873 - learning_rate: 6.2500e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4133 - loss: 1.5197\n",
            "Epoch 24: val_loss improved from 1.56853 to 1.54767, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.4133 - loss: 1.5197 - val_accuracy: 0.4067 - val_loss: 1.5477 - learning_rate: 6.2500e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4021 - loss: 1.5088\n",
            "Epoch 25: val_loss did not improve from 1.54767\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4021 - loss: 1.5089 - val_accuracy: 0.3820 - val_loss: 1.5823 - learning_rate: 6.2500e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4067 - loss: 1.5192\n",
            "Epoch 26: val_loss improved from 1.54767 to 1.52826, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4067 - loss: 1.5193 - val_accuracy: 0.4036 - val_loss: 1.5283 - learning_rate: 6.2500e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4200 - loss: 1.5025\n",
            "Epoch 27: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4200 - loss: 1.5026 - val_accuracy: 0.4022 - val_loss: 1.5583 - learning_rate: 6.2500e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4096 - loss: 1.5510\n",
            "Epoch 28: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4096 - loss: 1.5510 - val_accuracy: 0.3778 - val_loss: 1.6181 - learning_rate: 6.2500e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4080 - loss: 1.5318\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4080 - loss: 1.5318 - val_accuracy: 0.3827 - val_loss: 1.5913 - learning_rate: 6.2500e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4096 - loss: 1.5218\n",
            "Epoch 30: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4096 - loss: 1.5218 - val_accuracy: 0.3836 - val_loss: 1.6051 - learning_rate: 3.1250e-05\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4565 - loss: 1.4022\n",
            "Test Accuracy: 0.4805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "RoexJJCs5ld2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an improved CNN architecture\n",
        "model = Sequential([\n",
        "    Input(shape=(48,48,3)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(2048, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 50\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoMTBni35k4w",
        "outputId": "f2267db7-fbbd-42ce-b640-ea28aa9d8917"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.1577 - loss: 4.9184\n",
            "Epoch 1: val_loss improved from inf to 9.08683, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 121ms/step - accuracy: 0.1577 - loss: 4.9098 - val_accuracy: 0.0293 - val_loss: 9.0868 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1656 - loss: 1.9976\n",
            "Epoch 2: val_loss improved from 9.08683 to 2.43642, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 92ms/step - accuracy: 0.1656 - loss: 1.9976 - val_accuracy: 0.1578 - val_loss: 2.4364 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1580 - loss: 1.9758\n",
            "Epoch 3: val_loss did not improve from 2.43642\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1580 - loss: 1.9758 - val_accuracy: 0.1202 - val_loss: 2.5742 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.1552 - loss: 1.9396\n",
            "Epoch 4: val_loss did not improve from 2.43642\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.1552 - loss: 1.9396 - val_accuracy: 0.1348 - val_loss: 2.4686 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1677 - loss: 1.9522\n",
            "Epoch 5: val_loss improved from 2.43642 to 2.36297, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 92ms/step - accuracy: 0.1677 - loss: 1.9522 - val_accuracy: 0.1280 - val_loss: 2.3630 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1562 - loss: 1.9468\n",
            "Epoch 6: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1563 - loss: 1.9468 - val_accuracy: 0.1275 - val_loss: 2.4431 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1651 - loss: 1.9179\n",
            "Epoch 7: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1651 - loss: 1.9180 - val_accuracy: 0.1317 - val_loss: 2.4928 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1690 - loss: 1.9316\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1690 - loss: 1.9316 - val_accuracy: 0.1261 - val_loss: 3.3769 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1689 - loss: 1.9228\n",
            "Epoch 9: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.1689 - loss: 1.9228 - val_accuracy: 0.1458 - val_loss: 2.4736 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1649 - loss: 1.9509\n",
            "Epoch 10: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1650 - loss: 1.9508 - val_accuracy: 0.1583 - val_loss: 2.4517 - learning_rate: 5.0000e-05\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.1035 - loss: 2.1257\n",
            "Test Accuracy: 0.1633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "a3JqTrjw-yAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import RandomNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.05,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an improved CNN architecture based on Best_model.ipynb\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(48,48,1), bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 4th convolution layer\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 300\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181yWwht5nVz",
        "outputId": "ae3b160b-2e1a-4935-fba8-85f23e02f50d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1486 - loss: 3.5732\n",
            "Epoch 1: val_loss improved from inf to 1.94612, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.1486 - loss: 3.5670 - val_accuracy: 0.1982 - val_loss: 1.9461 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1540 - loss: 1.9652\n",
            "Epoch 2: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.1539 - loss: 1.9652 - val_accuracy: 0.1789 - val_loss: 2.0176 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1273 - loss: 1.9527\n",
            "Epoch 3: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.1273 - loss: 1.9527 - val_accuracy: 0.1829 - val_loss: 2.0551 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1365 - loss: 1.9380\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1365 - loss: 1.9380 - val_accuracy: 0.1531 - val_loss: 2.0683 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1226 - loss: 1.9331\n",
            "Epoch 5: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1226 - loss: 1.9331 - val_accuracy: 0.1874 - val_loss: 2.0998 - learning_rate: 5.0000e-04\n",
            "Epoch 6/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1542 - loss: 1.9148\n",
            "Epoch 6: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.1542 - loss: 1.9149 - val_accuracy: 0.1907 - val_loss: 2.1115 - learning_rate: 5.0000e-04\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.0865 - loss: 2.0062\n",
            "Test Accuracy: 0.1941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Adjusted Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,  # Reduced rotation\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,  # Reduced zoom\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Improved CNN Architecture with Adjustments\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution layer\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CUFKHq9AjDO",
        "outputId": "09edd8f4-3345-4062-e007-a4603dd4b2d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1703 - loss: 2.5496\n",
            "Epoch 1: val_loss improved from inf to 1.99240, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.1703 - loss: 2.5491 - val_accuracy: 0.1331 - val_loss: 1.9924 - learning_rate: 3.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1991 - loss: 2.0747\n",
            "Epoch 2: val_loss improved from 1.99240 to 1.90109, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.1991 - loss: 2.0746 - val_accuracy: 0.2088 - val_loss: 1.9011 - learning_rate: 3.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2309 - loss: 1.9361\n",
            "Epoch 3: val_loss improved from 1.90109 to 1.79929, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2309 - loss: 1.9362 - val_accuracy: 0.2898 - val_loss: 1.7993 - learning_rate: 3.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2674 - loss: 1.8865\n",
            "Epoch 4: val_loss improved from 1.79929 to 1.77704, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2674 - loss: 1.8865 - val_accuracy: 0.3055 - val_loss: 1.7770 - learning_rate: 3.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2932 - loss: 1.8026\n",
            "Epoch 5: val_loss improved from 1.77704 to 1.70129, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2932 - loss: 1.8026 - val_accuracy: 0.3351 - val_loss: 1.7013 - learning_rate: 3.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3181 - loss: 1.7658\n",
            "Epoch 6: val_loss improved from 1.70129 to 1.60121, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.3181 - loss: 1.7657 - val_accuracy: 0.3836 - val_loss: 1.6012 - learning_rate: 3.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3499 - loss: 1.6794\n",
            "Epoch 7: val_loss improved from 1.60121 to 1.57139, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3499 - loss: 1.6794 - val_accuracy: 0.4006 - val_loss: 1.5714 - learning_rate: 3.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3820 - loss: 1.6083\n",
            "Epoch 8: val_loss improved from 1.57139 to 1.52415, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3820 - loss: 1.6083 - val_accuracy: 0.4172 - val_loss: 1.5242 - learning_rate: 3.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3941 - loss: 1.5565\n",
            "Epoch 9: val_loss improved from 1.52415 to 1.50497, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3941 - loss: 1.5565 - val_accuracy: 0.4247 - val_loss: 1.5050 - learning_rate: 3.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4077 - loss: 1.5620\n",
            "Epoch 10: val_loss did not improve from 1.50497\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.4077 - loss: 1.5618 - val_accuracy: 0.4173 - val_loss: 1.5356 - learning_rate: 3.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4242 - loss: 1.5101\n",
            "Epoch 11: val_loss improved from 1.50497 to 1.44812, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4242 - loss: 1.5100 - val_accuracy: 0.4438 - val_loss: 1.4481 - learning_rate: 3.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4391 - loss: 1.4574\n",
            "Epoch 12: val_loss did not improve from 1.44812\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4391 - loss: 1.4574 - val_accuracy: 0.4288 - val_loss: 1.4728 - learning_rate: 3.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4531 - loss: 1.4316\n",
            "Epoch 13: val_loss improved from 1.44812 to 1.37244, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4531 - loss: 1.4315 - val_accuracy: 0.4764 - val_loss: 1.3724 - learning_rate: 3.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4597 - loss: 1.4029\n",
            "Epoch 14: val_loss did not improve from 1.37244\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4597 - loss: 1.4029 - val_accuracy: 0.4839 - val_loss: 1.3726 - learning_rate: 3.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4792 - loss: 1.3674\n",
            "Epoch 15: val_loss improved from 1.37244 to 1.33979, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4792 - loss: 1.3674 - val_accuracy: 0.4879 - val_loss: 1.3398 - learning_rate: 3.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4858 - loss: 1.3508\n",
            "Epoch 16: val_loss did not improve from 1.33979\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4858 - loss: 1.3508 - val_accuracy: 0.4945 - val_loss: 1.3436 - learning_rate: 3.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4898 - loss: 1.3319\n",
            "Epoch 17: val_loss improved from 1.33979 to 1.27815, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4898 - loss: 1.3319 - val_accuracy: 0.5051 - val_loss: 1.2781 - learning_rate: 3.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4971 - loss: 1.3034\n",
            "Epoch 18: val_loss did not improve from 1.27815\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.4971 - loss: 1.3035 - val_accuracy: 0.4922 - val_loss: 1.3387 - learning_rate: 3.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5043 - loss: 1.2916\n",
            "Epoch 19: val_loss did not improve from 1.27815\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5043 - loss: 1.2916 - val_accuracy: 0.5093 - val_loss: 1.2920 - learning_rate: 3.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5065 - loss: 1.2788\n",
            "Epoch 20: val_loss improved from 1.27815 to 1.25062, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.5065 - loss: 1.2788 - val_accuracy: 0.5224 - val_loss: 1.2506 - learning_rate: 3.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5216 - loss: 1.2400\n",
            "Epoch 21: val_loss did not improve from 1.25062\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5215 - loss: 1.2402 - val_accuracy: 0.4781 - val_loss: 1.3598 - learning_rate: 3.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5210 - loss: 1.2324\n",
            "Epoch 22: val_loss did not improve from 1.25062\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5210 - loss: 1.2324 - val_accuracy: 0.4994 - val_loss: 1.3634 - learning_rate: 3.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5175 - loss: 1.2332\n",
            "Epoch 23: val_loss improved from 1.25062 to 1.18797, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5175 - loss: 1.2332 - val_accuracy: 0.5462 - val_loss: 1.1880 - learning_rate: 3.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5385 - loss: 1.1997\n",
            "Epoch 24: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5385 - loss: 1.1997 - val_accuracy: 0.5226 - val_loss: 1.2462 - learning_rate: 3.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5252 - loss: 1.2096\n",
            "Epoch 25: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5252 - loss: 1.2096 - val_accuracy: 0.5403 - val_loss: 1.1969 - learning_rate: 3.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5330 - loss: 1.1798\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5331 - loss: 1.1799 - val_accuracy: 0.4652 - val_loss: 1.4490 - learning_rate: 3.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5449 - loss: 1.1565\n",
            "Epoch 27: val_loss improved from 1.18797 to 1.16197, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5449 - loss: 1.1564 - val_accuracy: 0.5551 - val_loss: 1.1620 - learning_rate: 1.5000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5511 - loss: 1.1399\n",
            "Epoch 28: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5511 - loss: 1.1399 - val_accuracy: 0.5577 - val_loss: 1.1722 - learning_rate: 1.5000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5609 - loss: 1.1229\n",
            "Epoch 29: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5608 - loss: 1.1229 - val_accuracy: 0.5570 - val_loss: 1.1684 - learning_rate: 1.5000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5578 - loss: 1.1183\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5578 - loss: 1.1183 - val_accuracy: 0.5557 - val_loss: 1.1780 - learning_rate: 1.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5661 - loss: 1.0890\n",
            "Epoch 31: val_loss improved from 1.16197 to 1.14360, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5661 - loss: 1.0890 - val_accuracy: 0.5624 - val_loss: 1.1436 - learning_rate: 7.5000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5757 - loss: 1.0726\n",
            "Epoch 32: val_loss improved from 1.14360 to 1.13069, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5756 - loss: 1.0726 - val_accuracy: 0.5762 - val_loss: 1.1307 - learning_rate: 7.5000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5796 - loss: 1.0696\n",
            "Epoch 33: val_loss improved from 1.13069 to 1.10665, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5796 - loss: 1.0696 - val_accuracy: 0.5893 - val_loss: 1.1066 - learning_rate: 7.5000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5738 - loss: 1.0807\n",
            "Epoch 34: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5738 - loss: 1.0807 - val_accuracy: 0.5750 - val_loss: 1.1247 - learning_rate: 7.5000e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5782 - loss: 1.0498\n",
            "Epoch 35: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5782 - loss: 1.0499 - val_accuracy: 0.5673 - val_loss: 1.1396 - learning_rate: 7.5000e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5817 - loss: 1.0479\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
            "\n",
            "Epoch 36: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5817 - loss: 1.0479 - val_accuracy: 0.5743 - val_loss: 1.1480 - learning_rate: 7.5000e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5739 - loss: 1.0548\n",
            "Epoch 37: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5740 - loss: 1.0547 - val_accuracy: 0.5825 - val_loss: 1.1112 - learning_rate: 3.7500e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5836 - loss: 1.0402\n",
            "Epoch 38: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5837 - loss: 1.0402 - val_accuracy: 0.5757 - val_loss: 1.1322 - learning_rate: 3.7500e-05\n",
            "Epoch 38: early stopping\n",
            "Restoring model weights from the end of the best epoch: 33.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5443 - loss: 1.1756\n",
            "Test Accuracy: 0.5943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Augmentation for Robust Training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,  # Increased rotation for better variation\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness for more generalization\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an Improved CNN Model with Regularization\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 4th convolution block (NEW Layer for More Depth)\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Learning rate scheduler (Exponential Decay)\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=0.0003, decay_steps=10000, decay_rate=0.9)\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgGBSZ1xF9cP",
        "outputId": "73501025-6b15-4ced-8ed3-300294708cc5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.1450 - loss: 3.1788\n",
            "Epoch 1: val_loss improved from inf to 2.65162, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 98ms/step - accuracy: 0.1450 - loss: 3.1781 - val_accuracy: 0.0369 - val_loss: 2.6516\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1588 - loss: 2.6089\n",
            "Epoch 2: val_loss improved from 2.65162 to 2.21776, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.1588 - loss: 2.6087 - val_accuracy: 0.2174 - val_loss: 2.2178\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1640 - loss: 2.4480\n",
            "Epoch 3: val_loss did not improve from 2.21776\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.1640 - loss: 2.4480 - val_accuracy: 0.0759 - val_loss: 2.3211\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1692 - loss: 2.4305\n",
            "Epoch 4: val_loss improved from 2.21776 to 2.17819, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.1692 - loss: 2.4303 - val_accuracy: 0.1926 - val_loss: 2.1782\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1820 - loss: 2.3070\n",
            "Epoch 5: val_loss improved from 2.17819 to 2.13014, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.1820 - loss: 2.3070 - val_accuracy: 0.2148 - val_loss: 2.1301\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1939 - loss: 2.2459\n",
            "Epoch 6: val_loss did not improve from 2.13014\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.1939 - loss: 2.2459 - val_accuracy: 0.2050 - val_loss: 2.1953\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2014 - loss: 2.2014\n",
            "Epoch 7: val_loss did not improve from 2.13014\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2014 - loss: 2.2014 - val_accuracy: 0.2097 - val_loss: 2.1389\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2206 - loss: 2.1742\n",
            "Epoch 8: val_loss improved from 2.13014 to 2.12282, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2206 - loss: 2.1741 - val_accuracy: 0.2534 - val_loss: 2.1228\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2246 - loss: 2.1794\n",
            "Epoch 9: val_loss did not improve from 2.12282\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2247 - loss: 2.1792 - val_accuracy: 0.1735 - val_loss: 2.2123\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2392 - loss: 2.0955\n",
            "Epoch 10: val_loss improved from 2.12282 to 2.09369, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2393 - loss: 2.0954 - val_accuracy: 0.2656 - val_loss: 2.0937\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2695 - loss: 2.0577\n",
            "Epoch 11: val_loss improved from 2.09369 to 2.02661, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.2696 - loss: 2.0577 - val_accuracy: 0.2777 - val_loss: 2.0266\n",
            "Epoch 12/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2924 - loss: 2.0163\n",
            "Epoch 12: val_loss improved from 2.02661 to 1.98876, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2924 - loss: 2.0163 - val_accuracy: 0.3219 - val_loss: 1.9888\n",
            "Epoch 13/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3250 - loss: 1.9750\n",
            "Epoch 13: val_loss did not improve from 1.98876\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.3250 - loss: 1.9750 - val_accuracy: 0.2898 - val_loss: 2.0198\n",
            "Epoch 14/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3575 - loss: 1.8810\n",
            "Epoch 14: val_loss improved from 1.98876 to 1.84533, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.3575 - loss: 1.8810 - val_accuracy: 0.3809 - val_loss: 1.8453\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3814 - loss: 1.8215\n",
            "Epoch 15: val_loss improved from 1.84533 to 1.83866, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.3814 - loss: 1.8215 - val_accuracy: 0.4121 - val_loss: 1.8387\n",
            "Epoch 16/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3982 - loss: 1.7994\n",
            "Epoch 16: val_loss improved from 1.83866 to 1.80054, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3982 - loss: 1.7994 - val_accuracy: 0.3999 - val_loss: 1.8005\n",
            "Epoch 17/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4289 - loss: 1.7374\n",
            "Epoch 17: val_loss improved from 1.80054 to 1.72657, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.4289 - loss: 1.7374 - val_accuracy: 0.4241 - val_loss: 1.7266\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4247 - loss: 1.7284\n",
            "Epoch 18: val_loss improved from 1.72657 to 1.64878, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4247 - loss: 1.7283 - val_accuracy: 0.4632 - val_loss: 1.6488\n",
            "Epoch 19/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4412 - loss: 1.6877\n",
            "Epoch 19: val_loss improved from 1.64878 to 1.64264, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.4412 - loss: 1.6876 - val_accuracy: 0.4726 - val_loss: 1.6426\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4508 - loss: 1.6229\n",
            "Epoch 20: val_loss did not improve from 1.64264\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4508 - loss: 1.6230 - val_accuracy: 0.4590 - val_loss: 1.6594\n",
            "Epoch 21/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4609 - loss: 1.6166\n",
            "Epoch 21: val_loss did not improve from 1.64264\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4609 - loss: 1.6167 - val_accuracy: 0.4626 - val_loss: 1.6501\n",
            "Epoch 22/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4629 - loss: 1.6124\n",
            "Epoch 22: val_loss improved from 1.64264 to 1.56227, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4630 - loss: 1.6124 - val_accuracy: 0.4882 - val_loss: 1.5623\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4698 - loss: 1.5933\n",
            "Epoch 23: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4698 - loss: 1.5933 - val_accuracy: 0.4539 - val_loss: 1.6223\n",
            "Epoch 24/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4769 - loss: 1.5934\n",
            "Epoch 24: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4769 - loss: 1.5934 - val_accuracy: 0.4959 - val_loss: 1.5669\n",
            "Epoch 25/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4881 - loss: 1.5530\n",
            "Epoch 25: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4881 - loss: 1.5530 - val_accuracy: 0.4731 - val_loss: 1.6175\n",
            "Epoch 26/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4925 - loss: 1.5220\n",
            "Epoch 26: val_loss improved from 1.56227 to 1.48088, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4925 - loss: 1.5221 - val_accuracy: 0.5135 - val_loss: 1.4809\n",
            "Epoch 27/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4951 - loss: 1.5360\n",
            "Epoch 27: val_loss did not improve from 1.48088\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4951 - loss: 1.5360 - val_accuracy: 0.4961 - val_loss: 1.5361\n",
            "Epoch 28/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4981 - loss: 1.4936\n",
            "Epoch 28: val_loss did not improve from 1.48088\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4981 - loss: 1.4936 - val_accuracy: 0.5086 - val_loss: 1.4955\n",
            "Epoch 29/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5104 - loss: 1.4530\n",
            "Epoch 29: val_loss improved from 1.48088 to 1.47958, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5104 - loss: 1.4531 - val_accuracy: 0.5255 - val_loss: 1.4796\n",
            "Epoch 30/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5062 - loss: 1.4853\n",
            "Epoch 30: val_loss improved from 1.47958 to 1.43269, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5062 - loss: 1.4853 - val_accuracy: 0.5327 - val_loss: 1.4327\n",
            "Epoch 31/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5147 - loss: 1.4393\n",
            "Epoch 31: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5147 - loss: 1.4394 - val_accuracy: 0.5245 - val_loss: 1.4633\n",
            "Epoch 32/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5096 - loss: 1.4717\n",
            "Epoch 32: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5097 - loss: 1.4717 - val_accuracy: 0.4898 - val_loss: 1.5543\n",
            "Epoch 33/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5159 - loss: 1.4559\n",
            "Epoch 33: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5159 - loss: 1.4559 - val_accuracy: 0.5280 - val_loss: 1.4484\n",
            "Epoch 34/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5236 - loss: 1.4316\n",
            "Epoch 34: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5236 - loss: 1.4316 - val_accuracy: 0.4665 - val_loss: 1.6363\n",
            "Epoch 35/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5309 - loss: 1.4254\n",
            "Epoch 35: val_loss improved from 1.43269 to 1.39511, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5309 - loss: 1.4254 - val_accuracy: 0.5482 - val_loss: 1.3951\n",
            "Epoch 36/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5368 - loss: 1.4065\n",
            "Epoch 36: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5367 - loss: 1.4065 - val_accuracy: 0.5419 - val_loss: 1.4097\n",
            "Epoch 37/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5250 - loss: 1.4155\n",
            "Epoch 37: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5250 - loss: 1.4155 - val_accuracy: 0.5388 - val_loss: 1.4197\n",
            "Epoch 38/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5434 - loss: 1.3781\n",
            "Epoch 38: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5434 - loss: 1.3782 - val_accuracy: 0.5196 - val_loss: 1.4895\n",
            "Epoch 39/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5404 - loss: 1.4018\n",
            "Epoch 39: val_loss improved from 1.39511 to 1.34416, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.5404 - loss: 1.4018 - val_accuracy: 0.5725 - val_loss: 1.3442\n",
            "Epoch 40/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5460 - loss: 1.3532\n",
            "Epoch 40: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5460 - loss: 1.3532 - val_accuracy: 0.5449 - val_loss: 1.3951\n",
            "Epoch 41/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5485 - loss: 1.3410\n",
            "Epoch 41: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5484 - loss: 1.3411 - val_accuracy: 0.5374 - val_loss: 1.4106\n",
            "Epoch 42/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5504 - loss: 1.3588\n",
            "Epoch 42: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5504 - loss: 1.3589 - val_accuracy: 0.5506 - val_loss: 1.3793\n",
            "Epoch 43/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5486 - loss: 1.3411\n",
            "Epoch 43: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5486 - loss: 1.3411 - val_accuracy: 0.5248 - val_loss: 1.4587\n",
            "Epoch 44/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5521 - loss: 1.3597\n",
            "Epoch 44: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5521 - loss: 1.3597 - val_accuracy: 0.5645 - val_loss: 1.3472\n",
            "Epoch 44: early stopping\n",
            "Restoring model weights from the end of the best epoch: 39.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5742 - loss: 1.2949\n",
            "Test Accuracy: 0.5917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Optimized Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,  # Balanced rotation\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,  # Reduced zoom\n",
        "    brightness_range=[0.9, 1.1],  # Reduced brightness variation\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Improved CNN Model with Adjustments\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Learning rate scheduler with ReduceLROnPlateau\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaGI_EUYJ9Pv",
        "outputId": "b99d2124-f04b-4910-c70b-2a79cc3e7ba5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.1639 - loss: 2.6336\n",
            "Epoch 1: val_loss improved from inf to 2.03282, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 94ms/step - accuracy: 0.1640 - loss: 2.6331 - val_accuracy: 0.2090 - val_loss: 2.0328 - learning_rate: 3.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2058 - loss: 2.1711\n",
            "Epoch 2: val_loss improved from 2.03282 to 1.91542, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.2058 - loss: 2.1711 - val_accuracy: 0.2555 - val_loss: 1.9154 - learning_rate: 3.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2312 - loss: 2.0901\n",
            "Epoch 3: val_loss did not improve from 1.91542\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2313 - loss: 2.0900 - val_accuracy: 0.2066 - val_loss: 1.9691 - learning_rate: 3.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2821 - loss: 1.9458\n",
            "Epoch 4: val_loss improved from 1.91542 to 1.85483, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2822 - loss: 1.9458 - val_accuracy: 0.3160 - val_loss: 1.8548 - learning_rate: 3.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3104 - loss: 1.8827\n",
            "Epoch 5: val_loss improved from 1.85483 to 1.72125, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3104 - loss: 1.8826 - val_accuracy: 0.3661 - val_loss: 1.7212 - learning_rate: 3.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3388 - loss: 1.8435\n",
            "Epoch 6: val_loss improved from 1.72125 to 1.70264, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3388 - loss: 1.8434 - val_accuracy: 0.3759 - val_loss: 1.7026 - learning_rate: 3.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3674 - loss: 1.7430\n",
            "Epoch 7: val_loss improved from 1.70264 to 1.62609, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3674 - loss: 1.7430 - val_accuracy: 0.4221 - val_loss: 1.6261 - learning_rate: 3.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3933 - loss: 1.6850\n",
            "Epoch 8: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.3933 - loss: 1.6850 - val_accuracy: 0.4116 - val_loss: 1.6515 - learning_rate: 3.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4079 - loss: 1.6420\n",
            "Epoch 9: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4079 - loss: 1.6420 - val_accuracy: 0.3316 - val_loss: 2.0128 - learning_rate: 3.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4224 - loss: 1.5919\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4224 - loss: 1.5919 - val_accuracy: 0.3863 - val_loss: 1.6848 - learning_rate: 3.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4481 - loss: 1.5082\n",
            "Epoch 11: val_loss improved from 1.62609 to 1.52046, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4481 - loss: 1.5082 - val_accuracy: 0.4543 - val_loss: 1.5205 - learning_rate: 1.5000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4591 - loss: 1.4773\n",
            "Epoch 12: val_loss improved from 1.52046 to 1.50334, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4591 - loss: 1.4773 - val_accuracy: 0.4614 - val_loss: 1.5033 - learning_rate: 1.5000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4699 - loss: 1.4582\n",
            "Epoch 13: val_loss improved from 1.50334 to 1.43579, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4699 - loss: 1.4583 - val_accuracy: 0.4877 - val_loss: 1.4358 - learning_rate: 1.5000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4779 - loss: 1.4603\n",
            "Epoch 14: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4779 - loss: 1.4603 - val_accuracy: 0.4719 - val_loss: 1.4896 - learning_rate: 1.5000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4841 - loss: 1.4451\n",
            "Epoch 15: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4841 - loss: 1.4451 - val_accuracy: 0.4767 - val_loss: 1.4822 - learning_rate: 1.5000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4826 - loss: 1.4467\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4826 - loss: 1.4466 - val_accuracy: 0.4525 - val_loss: 1.5400 - learning_rate: 1.5000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4987 - loss: 1.3859\n",
            "Epoch 17: val_loss improved from 1.43579 to 1.39797, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4987 - loss: 1.3859 - val_accuracy: 0.4992 - val_loss: 1.3980 - learning_rate: 7.5000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5043 - loss: 1.3826\n",
            "Epoch 18: val_loss improved from 1.39797 to 1.37618, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5043 - loss: 1.3826 - val_accuracy: 0.5079 - val_loss: 1.3762 - learning_rate: 7.5000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5129 - loss: 1.3676\n",
            "Epoch 19: val_loss improved from 1.37618 to 1.35173, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5128 - loss: 1.3676 - val_accuracy: 0.5184 - val_loss: 1.3517 - learning_rate: 7.5000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5061 - loss: 1.3695\n",
            "Epoch 20: val_loss did not improve from 1.35173\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5061 - loss: 1.3694 - val_accuracy: 0.4928 - val_loss: 1.4369 - learning_rate: 7.5000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5117 - loss: 1.3512\n",
            "Epoch 21: val_loss improved from 1.35173 to 1.33370, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.5117 - loss: 1.3512 - val_accuracy: 0.5255 - val_loss: 1.3337 - learning_rate: 7.5000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5133 - loss: 1.3398\n",
            "Epoch 22: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5133 - loss: 1.3397 - val_accuracy: 0.5152 - val_loss: 1.3509 - learning_rate: 7.5000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5201 - loss: 1.3236\n",
            "Epoch 23: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5201 - loss: 1.3236 - val_accuracy: 0.5222 - val_loss: 1.3495 - learning_rate: 7.5000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5325 - loss: 1.3053\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5325 - loss: 1.3053 - val_accuracy: 0.5027 - val_loss: 1.4121 - learning_rate: 7.5000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5240 - loss: 1.3164\n",
            "Epoch 25: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5240 - loss: 1.3164 - val_accuracy: 0.5281 - val_loss: 1.3383 - learning_rate: 3.7500e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5325 - loss: 1.2997\n",
            "Epoch 26: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5325 - loss: 1.2997 - val_accuracy: 0.5145 - val_loss: 1.3632 - learning_rate: 3.7500e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5228 - loss: 1.2952\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.8750000890577212e-05.\n",
            "\n",
            "Epoch 27: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5228 - loss: 1.2952 - val_accuracy: 0.5182 - val_loss: 1.3469 - learning_rate: 3.7500e-05\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5185 - loss: 1.3318\n",
            "Test Accuracy: 0.5515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions (Increased to 64x64 for better feature extraction)\n",
        "img_size = (64, 64)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Enhanced Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Enhanced CNN Model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=HeNormal(), input_shape=(64,64,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 4th convolution block (Added for feature extraction)\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile Model with RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w26am5zSR9b",
        "outputId": "9e171db9-79c2-4f93-b88c-5e2f6734910a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.1503 - loss: 2.9070\n",
            "Epoch 1: val_loss improved from inf to 2.01130, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 111ms/step - accuracy: 0.1503 - loss: 2.9068 - val_accuracy: 0.2230 - val_loss: 2.0113 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1592 - loss: 2.5697\n",
            "Epoch 2: val_loss improved from 2.01130 to 1.99542, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1592 - loss: 2.5696 - val_accuracy: 0.1761 - val_loss: 1.9954 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1587 - loss: 2.4192\n",
            "Epoch 3: val_loss did not improve from 1.99542\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1588 - loss: 2.4189 - val_accuracy: 0.1113 - val_loss: 2.0749 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1675 - loss: 2.1813\n",
            "Epoch 4: val_loss improved from 1.99542 to 1.95092, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 82ms/step - accuracy: 0.1675 - loss: 2.1814 - val_accuracy: 0.1561 - val_loss: 1.9509 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.1727 - loss: 2.1480\n",
            "Epoch 5: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 83ms/step - accuracy: 0.1727 - loss: 2.1481 - val_accuracy: 0.0848 - val_loss: 2.1698 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1852 - loss: 2.1306\n",
            "Epoch 6: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1852 - loss: 2.1307 - val_accuracy: 0.0625 - val_loss: 2.2612 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1868 - loss: 2.0989\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1868 - loss: 2.0989 - val_accuracy: 0.0294 - val_loss: 2.9709 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1897 - loss: 2.0635\n",
            "Epoch 8: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1897 - loss: 2.0634 - val_accuracy: 0.0228 - val_loss: 2.6410 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1987 - loss: 2.0550\n",
            "Epoch 9: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1987 - loss: 2.0549 - val_accuracy: 0.0214 - val_loss: 2.6994 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1978 - loss: 2.0270\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1979 - loss: 2.0270 - val_accuracy: 0.0726 - val_loss: 2.3006 - learning_rate: 5.0000e-05\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1359 - loss: 2.0298\n",
            "Test Accuracy: 0.2034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-dQBE5SSXWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sExqxqv5Q3V-",
        "mkpL-YBrpur8",
        "BJB0ZrdbTlyv",
        "CC4ItVgVT3QP",
        "txPzADAIScWf",
        "NCkLrfnrgc9w",
        "q8TzUNT5lk8R",
        "6DdwJKaeuVU0",
        "AR4r_CcUqpMa",
        "qKpuKTfdq3U6",
        "SsHrBLpXsMAK",
        "nl9gP7nDserC"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}