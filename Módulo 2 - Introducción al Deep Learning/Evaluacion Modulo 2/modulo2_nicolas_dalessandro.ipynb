{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKk8sKZfqL39"
      },
      "source": [
        "#  📝 Evaluacion del Modulo 2 - \"IA para profesionales TIC – 3ª edición\"\n",
        "\n",
        "#### 🪪 **Alumno**: Nicolas D'Alessandro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Statement"
      ],
      "metadata": {
        "id": "sExqxqv5Q3V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🎯 Objetivo\n",
        "\n",
        "El objetivo de este ejercicio es construir y entrenar una red neuronal convolucional (CNN) para clasificar imágenes de rostros humanos según las emociones que muestran. Los estudiantes aprenderán a trabajar con un dataset de imágenes, a construir y entrenar un modelo de CNN utilizando TensorFlow y Keras, y a evaluar su desempeño.\n",
        "\n",
        "Podéis hacer uso de ChatGPT, Gemini, Github Copilot o cualquier otra herramienta, con un uso adecuado y siempre entendiendo lo que hacéis, y lo que el código hace.\n",
        "\n",
        "Descripción del Ejercicio\n",
        "\n",
        "En este ejercicio, utilizarás el dataset FER-2013 (Facial Expression Recognition) disponible en Kaggle. Este dataset contiene imágenes de rostros etiquetados con diferentes emociones (felicidad, tristeza, enojo, sorpresa, etc.). Tu tarea es entrenar una CNN para clasificar las imágenes de rostros en las categorías de emoción correctas.\n",
        "\n",
        "\n",
        "\n",
        "### 🪜 Pasos a Seguir\n",
        "\n",
        "#### 1. Preparación del Entorno:\n",
        "\n",
        "- Abre un nuevo notebook en Google Colab.\n",
        "- Asegúrate de tener instaladas las bibliotecas necesarias: TensorFlow, Keras, y otras bibliotecas auxiliares.\n",
        "\n",
        "#### 2. Carga y Exploración del Dataset:\n",
        "\n",
        "- Descarga el dataset FER-2013 desde Kaggle.\n",
        "- Carga el dataset en tu notebook y explora las imágenes y sus etiquetas.\n",
        "\n",
        "#### 3.Preprocesamiento de Datos:\n",
        "\n",
        "- Realiza el preprocesamiento necesario de las imágenes, como el cambio de tamaño, la normalización y la división en conjuntos de entrenamiento y validación.\n",
        "\n",
        "#### 4.Construcción del Modelo:\n",
        "\n",
        "- Define una arquitectura de red neuronal convolucional utilizando Keras. Puedes empezar con una arquitectura simple y luego experimentar con arquitecturas más complejas.\n",
        "- Compila el modelo especificando el optimizador, la función de pérdida y las métricas de evaluación.\n",
        "\n",
        "#### 5. Entrenamiento del Modelo:\n",
        "\n",
        "- Entrena tu modelo con el conjunto de entrenamiento.\n",
        "- Evalúa el desempeño del modelo en el conjunto de validación.\n",
        "\n",
        "#### 6. Evaluación y Mejora:\n",
        "\n",
        "- Analiza los resultados y ajusta la arquitectura y los hiperparámetros del modelo para mejorar su precisión.\n",
        "- Realiza predicciones sobre nuevas imágenes de rostros, busca rostros y ajústalos a lo que pide el modelo, y verifica su exactitud.\n",
        "\n",
        "#### 7. Desafío Extra:\n",
        "\n",
        "- Crear una visualización interactiva que muestre las emociones detectadas en tiempo real usando la webcam del ordenador (Google Colab, si buscáis, lo permite).\n",
        "\n",
        "#### 8. A entregar en un .zip con vuestro nombre:\n",
        "\n",
        "- Documenta todo el proceso en el notebook.\n",
        "- Incluye gráficos de precisión y pérdida durante el entrenamiento.\n",
        "- Muestra ejemplos de imágenes correctamente clasificadas y mal clasificadas.\n",
        "- Comenta sobre los resultados obtenidos y las posibles mejoras o lo que consideres en un PDF.\n",
        "\n",
        "\n",
        "\n",
        "### 🧪 Evaluación\n",
        "\n",
        "La evaluación se basará en:\n",
        "\n",
        "- La correcta implementación del modelo de CNN.\n",
        "- La claridad y detalle en la documentación del proceso.\n",
        "- La precisión del modelo en el conjunto de validación.\n",
        "- La creatividad en la mejora del modelo y en el uso de técnicas de aumento de datos.\n"
      ],
      "metadata": {
        "id": "Z3BTGs17PXYW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlJKTJdBnWNt"
      },
      "source": [
        "## Step 1️⃣: Preparing the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣.0️⃣ Setup a propper Runtime"
      ],
      "metadata": {
        "id": "CV7DXYKYqJhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA0xa0vip_y1",
        "outputId": "1cda35bc-c192-465a-dc02-92101e9ecf38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 17 20:45:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   40C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣.1️⃣ Ensure we have installed the required libraries"
      ],
      "metadata": {
        "id": "SZEa87RhNSXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras numpy matplotlib opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7MVnsfoNS8_",
        "outputId": "868b5562-3bf9-4484-a60f-7e2203a80bd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtDRHWsZPSam",
        "outputId": "afaaefa3-aae6-4360-ace6-4dee5d68c587"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Collecting kagglehub\n",
            "  Downloading kagglehub-0.3.8-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Downloading kagglehub-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kagglehub\n",
            "  Attempting uninstall: kagglehub\n",
            "    Found existing installation: kagglehub 0.3.7\n",
            "    Uninstalling kagglehub-0.3.7:\n",
            "      Successfully uninstalled kagglehub-0.3.7\n",
            "Successfully installed kagglehub-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkpL-YBrpur8"
      },
      "source": [
        "### 1️⃣.2️⃣ Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jOy68DXNp2nk"
      },
      "outputs": [],
      "source": [
        "# Data Manipulation and Analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning Framework (TensorFlow & Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Computer Vision (OpenCV)\n",
        "import cv2\n",
        "\n",
        "# File Handling and Directory Management\n",
        "import os\n",
        "\n",
        "# Kaggle API for Dataset Access\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓ Explanation:\n",
        "\n",
        "- **TensorFlow/Keras** → Libraries to build and train the CNN.\n",
        "- **NumPy/Matplotlib** → Data manipulation and image visualization.\n",
        "- **OpenCV** → Image processing."
      ],
      "metadata": {
        "id": "KobN_fHpN7rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2️⃣: Loading & Exploring the Dataset"
      ],
      "metadata": {
        "id": "3a_cwr9hge9i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gWzff_pekE"
      },
      "source": [
        "### 2️⃣.1️⃣ Set the Path to the Files in Kagglehub and Load them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🔹 Verify Dataset Path"
      ],
      "metadata": {
        "id": "080JXzjyTwXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifECURvhO6VO",
        "outputId": "f1101f6b-d01d-4469-f719-68fd4fb0478b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3️⃣: Data Pre-processing"
      ],
      "metadata": {
        "id": "yJCyzERdjD2B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY2bjXJ3o-Et"
      },
      "source": [
        "### 3️⃣.1️⃣ Verify Data Classes and Format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🔹 Set Up Image Data Generators"
      ],
      "metadata": {
        "id": "WdKJDd5_Tkwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leGOkWXhpHN8",
        "outputId": "e206bb2c-8e5c-4854-ae3c-60611102d41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # Normalize pixel values (0-1)\n",
        "    validation_split=0.2,   # 80% training, 20% validation\n",
        "    rotation_range=10,      # Augmentation: small rotations\n",
        "    zoom_range=0.1,         # Augmentation: zoom\n",
        "    horizontal_flip=True    # Augmentation: flip images\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓What’s happening?\n",
        "\n",
        "- `rescale=1./255` → Normalizes pixel values from `[0,255]` to `[0,1]`.\n",
        "- `validation_split=0.2` → Splits 20% for validation.\n",
        "- Augmentations (rotation, zoom, flip) help prevent overfitting."
      ],
      "metadata": {
        "id": "zfeYHHBFT-vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🔹 Check Data Distribution"
      ],
      "metadata": {
        "id": "BJB0ZrdbTlyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp2zRyOVpHDV",
        "outputId": "ff7706a0-216b-4000-b1b5-6e0e29770e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
          ]
        }
      ],
      "source": [
        "# View detected classes\n",
        "print(\"Classes:\", train_generator.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🔹 Verify Sample Images"
      ],
      "metadata": {
        "id": "CC4ItVgVT3QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch of images\n",
        "x_batch, y_batch = next(train_generator)\n",
        "\n",
        "# Plot sample images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(x_batch[i])\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "kVami3ZZT4G_",
        "outputId": "68d1d9b5-17ce-4294-ac1a-f8417d7662f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 **STEP 1, 2, 3 Conclusion**: Loading & Exploring the Dataset"
      ],
      "metadata": {
        "id": "txPzADAIScWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the images organized into **7 kinds of emotions**:\n",
        "\n",
        "| Class | Numeric Label |\n",
        "|--------|----------------|\n",
        "| **Angry** (Enojo) | 0 |\n",
        "| **Disgust** (Disgusto) | 1 |\n",
        "| **Fear** (Miedo) | 2 |\n",
        "| **Happy** (Felicidad) | 3 |\n",
        "| **Neutral** (Neutralidad) | 4 |\n",
        "| **Sad** (Tristeza) | 5 |\n",
        "| **Surprise** (Sorpresa) | 6 |"
      ],
      "metadata": {
        "id": "UqOwOsmRUsB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 **Dataset summary**:\n",
        "- **Training**: **22,968 images** ✅\n",
        "- **Validation**: **5,741 images** ✅\n",
        "- **Test**: **7,178 images** ✅"
      ],
      "metadata": {
        "id": "np-D4Z_LUoKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **✅ Summary of Steps Completed So Far**  \n",
        "\n",
        "1️⃣ **Set Up the Environment**  \n",
        "   - Installed required libraries (`tensorflow`, `keras`, `numpy`, `opencv`, `matplotlib`, `kagglehub`).  \n",
        "   - Imported necessary Python modules.  \n",
        "\n",
        "2️⃣ **Loaded the FER-2013 Dataset**  \n",
        "   - Verified dataset structure (`train/`, `test/` folders with images per class).  \n",
        "   - Listed class labels: **Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise**.  \n",
        "\n",
        "3️⃣ **Preprocessed & Augmented Data**  \n",
        "   - Used `ImageDataGenerator` to load **images efficiently**.  \n",
        "   - Applied **data augmentation**: rotations, zoom, and horizontal flips.  \n",
        "   - **Normalized pixel values** (0-255 → 0-1).  \n",
        "   - Split dataset: **80% training / 20% validation**.  \n",
        "\n",
        "4️⃣ **Explored Data**  \n",
        "   - Verified class distribution.  \n",
        "   - Displayed **sample images** to confirm correct loading.\n",
        "   "
      ],
      "metadata": {
        "id": "m27usQf2V2Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4️⃣: Build the CNN Architecture"
      ],
      "metadata": {
        "id": "NCkLrfnrgc9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ℹ️ INFO: Key Considerations for the CNN Architecture\n",
        "\n",
        "- **Feature Extraction**: Use **multiple convolutional layers** to extract patterns (edges, textures, shapes).  \n",
        "- **Dimensionality Reduction**: Apply **MaxPooling layers** to reduce feature map size and computation cost.  \n",
        "- **Fully Connected Layers**: Translate learned features into **emotion classification**.  \n",
        "- **Regularization**: Use **Dropout layers** to **prevent overfitting**.  \n",
        "- **Activation Functions**: Use **ReLU for hidden layers** and **Softmax for output (7 classes)**."
      ],
      "metadata": {
        "id": "pocr5qttgc6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4️⃣.1️⃣ Define the CNN Model"
      ],
      "metadata": {
        "id": "q8TzUNT5lk8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ℹ️ Explanation of the Architecture\n",
        "\n",
        "- **Feature Extraction**:\n",
        "  - **3 convolutional blocks** with increasing filters (**64 → 128 → 256**).\n",
        "  - **Batch Normalization** to stabilize training and speed up convergence.\n",
        "  - **MaxPooling** to **reduce spatial dimensions**.\n",
        "\n",
        "- **Classification Layers**:\n",
        "  - **Flatten** layer converts feature maps into a single vector.\n",
        "  - **2 Fully Connected (`Dense`) layers** with **Dropout** for regularization.\n",
        "  - **Softmax output layer** (7 neurons) for **multi-class classification**.\n",
        "\n",
        "- **Optimization & Loss Function**:\n",
        "  - **Adam Optimizer** with a small **learning rate (0.0001)**.\n",
        "  - **Categorical Cross-Entropy Loss** (since we have 7 classes)."
      ],
      "metadata": {
        "id": "h3AYOcaQgc04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    # Convolutional Block 1\n",
        "    Conv2D(64, (3,3), activation='relu', input_shape=(48,48,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Convolutional Block 3\n",
        "    Conv2D(256, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Flattening Layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully Connected Layer 1\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Regularization\n",
        "\n",
        "    # Fully Connected Layer 2\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output Layer (7 emotions)\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "F4FPd1WolcFE",
        "outputId": "ca9815cc-8932-4093-9006-d5120e78d465"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m1,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m295,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │           \u001b[38;5;34m1,799\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,603,399\u001b[0m (9.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,603,399</span> (9.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,602,503\u001b[0m (9.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,602,503</span> (9.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 **STEP 4 Conclusion**: Defining the CNN Model"
      ],
      "metadata": {
        "id": "6DdwJKaeuVU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ❓ **What We Did?**\n",
        "\n",
        "1️⃣ **Designed a Deep Convolutional Neural Network (CNN)**\n",
        "   - Built a multi-layer CNN to classify facial emotions from images.  \n",
        "   - Incorporated **Convolutional (Conv2D) layers** for feature extraction.  \n",
        "   - Used **Batch Normalization** for stable and faster training.  \n",
        "   - Applied **MaxPooling layers** to reduce dimensionality.  \n",
        "\n",
        "2️⃣ **Added Fully Connected (Dense) Layers**\n",
        "   - Flattened feature maps into a single vector (`Flatten`).  \n",
        "   - **Dense layers (512 & 256 neurons)** learn complex patterns.  \n",
        "   - Applied **Dropout (50%)** to prevent overfitting.  \n",
        "   - Used **Softmax output layer (7 neurons)** for classification into 7 emotions.  \n",
        "\n",
        "3️⃣ **Compiled the Model**\n",
        "   - **Optimizer**: Adam (learning rate = 0.0001)  \n",
        "   - **Loss Function**: Categorical Crossentropy  \n",
        "   - **Metric**: Accuracy"
      ],
      "metadata": {
        "id": "Z2QbibuFgcyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###❓ **What Have We Obtained?**\n",
        "\n",
        "✅ **Successfully Built a Deep CNN Model for Facial Emotion Recognition**  \n",
        "- **Model Summary:**\n",
        "  - **2.6M Trainable Parameters**  \n",
        "  - **9.93MB total size**  \n",
        "  - **3 Convolutional Blocks** with increasing filters (**64 → 128 → 256**)  \n",
        "  - **MaxPooling layers** for size reduction  \n",
        "  - **Dense layers (512, 256 neurons)** with Dropout (50%)"
      ],
      "metadata": {
        "id": "ga6wQnDCtYoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5️⃣: Training the CNN Model & Evaluating Performance"
      ],
      "metadata": {
        "id": "vPVJv7YLgcvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ℹ️ INFO: Key Considerations Before Training\n",
        "\n",
        "- **How do we ensure a robust training process?**  \n",
        "  - **Monitor Model Performance**: Track accuracy and loss over epochs.  \n",
        "  - **Prevent Overfitting**: Use **early stopping** and **learning rate reduction**.  \n",
        "  - **Efficient Training**: Utilize **GPU acceleration** in Colab for faster computation."
      ],
      "metadata": {
        "id": "m5rLLAANgcsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5️⃣.1️⃣ Define Callbacks for Efficient Training"
      ],
      "metadata": {
        "id": "zcVU9GlLrQEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####❓ We'll use **callbacks** to optimize training dynamically:\n",
        "\n",
        "  - **EarlyStopping**: Stops training when validation loss stops improving.  \n",
        "  - **ReduceLROnPlateau**: Reduces the learning rate if validation loss stagnates."
      ],
      "metadata": {
        "id": "_kfM-y9vrgMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ **Why?**\n",
        "- **Stops training early** to avoid overfitting.  \n",
        "- **Reduces learning rate** dynamically for fine-tuned adjustments."
      ],
      "metadata": {
        "id": "AR4r_CcUqpMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', patience=5, restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"best_model.h5\", monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, reduce_lr, checkpoint]"
      ],
      "metadata": {
        "id": "lnCwI9zzqhH1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5️⃣.2️⃣ Train the **CNN** Model\n",
        "Now, we train the model using the **training dataset (`train_generator`)** and evaluate on **validation data (`val_generator`)**."
      ],
      "metadata": {
        "id": "qKpuKTfdq3U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,  # Optimized number of epochs\n",
        "    batch_size = 64,  # Defined previously in ImageDataGenerator\n",
        "    callbacks=callbacks  # Use defined callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVHSKfedq54D",
        "outputId": "2a91f061-334b-4c77-99cd-81506ed32d99"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3366 - loss: 1.6956\n",
            "Epoch 1: val_loss improved from inf to 1.58726, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.3366 - loss: 1.6956 - val_accuracy: 0.3870 - val_loss: 1.5873 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 1.6244\n",
            "Epoch 2: val_loss improved from 1.58726 to 1.52033, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.3626 - loss: 1.6244 - val_accuracy: 0.4170 - val_loss: 1.5203 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3840 - loss: 1.5866\n",
            "Epoch 3: val_loss improved from 1.52033 to 1.48530, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 1s/step - accuracy: 0.3840 - loss: 1.5866 - val_accuracy: 0.4313 - val_loss: 1.4853 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4030 - loss: 1.5404\n",
            "Epoch 4: val_loss improved from 1.48530 to 1.46726, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.4030 - loss: 1.5404 - val_accuracy: 0.4393 - val_loss: 1.4673 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4214 - loss: 1.5026\n",
            "Epoch 5: val_loss improved from 1.46726 to 1.43382, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - accuracy: 0.4213 - loss: 1.5027 - val_accuracy: 0.4520 - val_loss: 1.4338 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4352 - loss: 1.4650\n",
            "Epoch 6: val_loss did not improve from 1.43382\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 1s/step - accuracy: 0.4352 - loss: 1.4650 - val_accuracy: 0.4477 - val_loss: 1.4385 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4467 - loss: 1.4476\n",
            "Epoch 7: val_loss improved from 1.43382 to 1.41381, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.4467 - loss: 1.4476 - val_accuracy: 0.4531 - val_loss: 1.4138 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4475 - loss: 1.4373\n",
            "Epoch 8: val_loss improved from 1.41381 to 1.37501, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.4475 - loss: 1.4373 - val_accuracy: 0.4837 - val_loss: 1.3750 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4735 - loss: 1.4004\n",
            "Epoch 9: val_loss improved from 1.37501 to 1.35054, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.4735 - loss: 1.4004 - val_accuracy: 0.4828 - val_loss: 1.3505 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4760 - loss: 1.3739\n",
            "Epoch 10: val_loss improved from 1.35054 to 1.32422, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.4760 - loss: 1.3739 - val_accuracy: 0.4994 - val_loss: 1.3242 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 1.3338\n",
            "Epoch 11: val_loss improved from 1.32422 to 1.32118, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 1.3338 - val_accuracy: 0.5050 - val_loss: 1.3212 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5020 - loss: 1.3138\n",
            "Epoch 12: val_loss did not improve from 1.32118\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.5020 - loss: 1.3138 - val_accuracy: 0.4980 - val_loss: 1.3351 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4997 - loss: 1.3157\n",
            "Epoch 13: val_loss improved from 1.32118 to 1.28444, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.4997 - loss: 1.3157 - val_accuracy: 0.5086 - val_loss: 1.2844 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5092 - loss: 1.2884\n",
            "Epoch 14: val_loss did not improve from 1.28444\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.5092 - loss: 1.2884 - val_accuracy: 0.5060 - val_loss: 1.3067 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5118 - loss: 1.2800\n",
            "Epoch 15: val_loss did not improve from 1.28444\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.5118 - loss: 1.2800 - val_accuracy: 0.5142 - val_loss: 1.2972 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5️⃣.3️⃣ Evaluate Model Performance\n",
        "\n",
        "- After training, we analyze its performance on the **validation set**."
      ],
      "metadata": {
        "id": "SsHrBLpXsMAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on validation data\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f\"📊 Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"📉 Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdH_-7tZsVJS",
        "outputId": "5a32243c-d888-489c-9951-3c0e53ac35ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 265ms/step - accuracy: 0.5153 - loss: 1.2757\n",
            "📊 Validation Accuracy: 0.5170\n",
            "📉 Validation Loss: 1.2717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5️⃣.4️⃣ Visualize Training Progress\n",
        "\n",
        "- We plot **accuracy** and **loss** curves to assess model performance.\n"
      ],
      "metadata": {
        "id": "nl9gP7nDserC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy plot\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training & Validation Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "wCPObQOqsoyD",
        "outputId": "9e4f7d74-a6fc-4443-d8e2-2bfde619bd2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss plot\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "c3njMhsVsrJU",
        "outputId": "588e24b9-c764-49e3-894a-eb1c19650436"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAhYRJREFUeJzs3XdYlfX/x/HnOYe9BZGhCG4UEQeCe6Tlihzlzm1aOTKzb/orZ8N2ZpojV5rbXJV77y3uLQoq4AQEZJ1z//44eYpcKOPmwPtxXffVue9z3/d532icl/f9GRpFURSEEEIIIQoRrdoFCCGEEELkNQlAQgghhCh0JAAJIYQQotCRACSEEEKIQkcCkBBCCCEKHQlAQgghhCh0JAAJIYQQotCRACSEEEKIQkcCkBBCCCEKHQlAQpihnj174ufn90LHjhkzBo1Gk7MFmblt27ah0WjYtm2baVtWf8ZXrlxBo9EwZ86cHK3Jz8+Pnj175ug5hRD/kAAkRA7SaDRZWv79RVvYGAwGvv32W8qVK4etrS1lypThnXfeITExMUvHV6lShZIlS/K0WXzq1q2Lh4cHGRkZOVV2rtizZw9jxowhLi5O7VJM5syZg0aj4dChQ2qXIkSuslC7ACEKknnz5mVanzt3Lhs3bnxke8WKFbP1Ob/88gsGg+GFjv3kk08YPnx4tj4/O3788Uc+/PBD2rRpw4cffsjVq1dZuHAhH330EQ4ODs88vmvXrgwfPpydO3fSoEGDR96/cuUKe/fuZeDAgVhYvPivuOz8jLNqz549jB07lp49e+Li4pLpvXPnzqHVyr9RhcgtEoCEyEFvvvlmpvV9+/axcePGR7b/V3JyMnZ2dln+HEtLyxeqD8DCwiJbwSC7Fi1aREBAAMuXLzc9ivv000+zHDa6dOnCiBEjWLBgwWMD0MKFC1EUha5du2arzuz8jHOCtbW1qp8vREEn/7wQIo81atSIypUrc/jwYRo0aICdnR3/93//B8CqVato1aoV3t7eWFtbU6ZMGT799FP0en2mc/y3fcrDdijffvst06dPp0yZMlhbW1OzZk0OHjyY6djHtQHSaDQMHDiQlStXUrlyZaytrQkICGDdunWP1L9t2zaCg4OxsbGhTJkyTJs27bnaFWm1WgwGQ6b9tVptlkOZj48PDRo0YNmyZaSnpz/y/oIFCyhTpgyhoaFcvXqVd999lwoVKmBra4ubmxvt27fnypUrz/ycx7UBiouLo2fPnjg7O+Pi4kKPHj0e+/jq+PHj9OzZk9KlS2NjY4Onpye9e/fmzp07pn3GjBnDhx9+CECpUqVMj0cf1va4NkCXL1+mffv2uLq6YmdnR61atfjrr78y7fOwPdOSJUv4/PPPKVGiBDY2NjRp0oSLFy8+87qz6ujRo7Ro0QInJyccHBxo0qQJ+/bty7RPeno6Y8eOpVy5ctjY2ODm5ka9evXYuHGjaZ+YmBh69epFiRIlsLa2xsvLi9atW2fpz0iI7JA7QEKo4M6dO7Ro0YJOnTrx5ptv4uHhARjbXzg4ODB06FAcHBzYsmULo0aNIiEhgW+++eaZ512wYAH379+nf//+aDQavv76a9q1a8fly5efeUdj165dLF++nHfffRdHR0cmTpzI66+/TmRkJG5uboDxS6958+Z4eXkxduxY9Ho948aNw93dPcvX3qtXL/r378+0adPo379/lo/7t65du9KvXz/Wr1/Pq6++atp+4sQJTp48yahRowA4ePAge/bsoVOnTpQoUYIrV64wZcoUGjVqxOnTp5/rrpuiKLRu3Zpdu3bx9ttvU7FiRVasWEGPHj0e2Xfjxo1cvnyZXr164enpyalTp5g+fTqnTp1i3759aDQa2rVrx/nz51m4cCE//PADRYsWBXjizzI2NpY6deqQnJzM4MGDcXNz49dff+W1115j2bJltG3bNtP+X375JVqtlmHDhhEfH8/XX39N165d2b9/f5av+UlOnTpF/fr1cXJy4n//+x+WlpZMmzaNRo0asX37dkJDQwFjyBs/fjx9+/YlJCSEhIQEDh06xJEjR3j55ZcBeP311zl16hSDBg3Cz8+PmzdvsnHjRiIjI1+4ob8QWaIIIXLNgAEDlP/+b9awYUMFUKZOnfrI/snJyY9s69+/v2JnZ6ekpKSYtvXo0UPx9fU1rUdERCiA4ubmpty9e9e0fdWqVQqg/PHHH6Zto0ePfqQmQLGyslIuXrxo2nbs2DEFUH766SfTtrCwMMXOzk65fv26aduFCxcUCwuLR875JMOHD1esrKwUnU6nLF++PEvH/Nfdu3cVa2trpXPnzo+cG1DOnTunKMrjf5579+5VAGXu3LmmbVu3blUAZevWraZt//0Zr1y5UgGUr7/+2rQtIyNDqV+/vgIos2fPNm1/3OcuXLhQAZQdO3aYtn3zzTcKoERERDyyv6+vr9KjRw/T+pAhQxRA2blzp2nb/fv3lVKlSil+fn6KXq/PdC0VK1ZUUlNTTfv++OOPCqCcOHHikc/6t9mzZyuAcvDgwSfu06ZNG8XKykq5dOmSaduNGzcUR0dHpUGDBqZtQUFBSqtWrZ54nnv37imA8s033zy1JiFygzwCE0IF1tbW9OrV65Httra2ptf379/n9u3b1K9fn+TkZM6ePfvM83bs2JEiRYqY1uvXrw8YH508S9OmTSlTpoxpvUqVKjg5OZmO1ev1bNq0iTZt2uDt7W3ar2zZsrRo0eKZ5weYOHEi33//Pbt376Zz58506tSJDRs2ZNrH2tqakSNHPvU8RYoUoWXLlqxevZqkpCTAeIdm0aJFBAcHU758eSDzzzM9PZ07d+5QtmxZXFxcOHLkSJZqfmjNmjVYWFjwzjvvmLbpdDoGDRr0yL7//tyUlBRu375NrVq1AJ77c//9+SEhIdSrV8+0zcHBgX79+nHlyhVOnz6daf9evXphZWVlWn+evwtPo9fr2bBhA23atKF06dKm7V5eXnTp0oVdu3aRkJAAgIuLC6dOneLChQuPPZetrS1WVlZs27aNe/fuZasuIZ6XBCAhVFC8ePFMX04PnTp1irZt2+Ls7IyTkxPu7u6mBtTx8fHPPG/JkiUzrT8MQ1n5cvnvsQ+Pf3jszZs3efDgAWXLln1kv8dt+68HDx4wevRo+vbtS3BwMLNnz+all16ibdu27Nq1C4ALFy6QlpZmeoTyNF27diUpKYlVq1YBxh5VV65cydT4+cGDB4waNQofHx+sra0pWrQo7u7uxMXFZenn+W9Xr17Fy8vrkZ5qFSpUeGTfu3fv8t577+Hh4YGtrS3u7u6UKlUKyNqf45M+/3Gf9bBH4dWrVzNtz87fhae5desWycnJT6zFYDAQFRUFwLhx44iLi6N8+fIEBgby4Ycfcvz4cdP+1tbWfPXVV6xduxYPDw8aNGjA119/TUxMTLZqFCIrJAAJoYJ/3yF4KC4ujoYNG3Ls2DHGjRvHH3/8wcaNG/nqq68AstRLSqfTPXa78pQxc3Li2Kw4c+YMcXFxpjshFhYWLFu2jMqVK9OqVSuOHDnC9OnTKVasmKl9yNO8+uqrODs7s2DBAsDY/kmn09GpUyfTPoMGDeLzzz+nQ4cOLFmyhA0bNrBx40bc3NxytYt7hw4d+OWXX3j77bdZvnw5GzZsMDUoz+2u9Q/l9p9nVjRo0IBLly4xa9YsKleuzIwZM6hevTozZsww7TNkyBDOnz/P+PHjsbGxYeTIkVSsWJGjR4/mWZ2icJJG0ELkE9u2bePOnTssX748U/fuiIgIFav6R7FixbCxsXlsT6Ks9C562Ovr4d0BAHt7e9asWUO9evVo1qwZKSkpfPbZZ1nqAm5tbc0bb7zB3LlziY2NZenSpbz00kt4enqa9lm2bBk9evTgu+++M21LSUl5oYEHfX192bx5M4mJiZnuAp07dy7Tfvfu3WPz5s2MHTvW1BgbeOxjoOcZkdvX1/eRzwJMj0Z9fX2zfK7scHd3x87O7om1aLVafHx8TNtcXV3p1asXvXr1IjExkQYNGjBmzBj69u1r2qdMmTJ88MEHfPDBB1y4cIGqVavy3Xff8dtvv+XJNYnCSe4ACZFPPPwX+7//hZ6WlsbPP/+sVkmZ6HQ6mjZtysqVK7lx44Zp+8WLF1m7du0zjw8MDMTDw4NJkyZx8+ZN03Y3Nzdmz57N7du3efDgAWFhYVmuqWvXrqSnp9O/f39u3br1yNg/Op3ukTseP/300yPDCmRFy5YtycjIYMqUKaZter2en3766ZHPhEfvtEyYMOGRc9rb2wNkKZC1bNmSAwcOsHfvXtO2pKQkpk+fjp+fH5UqVcrqpWSLTqfjlVdeYdWqVZm6qsfGxrJgwQLq1auHk5MTQKZu/2Bss1S2bFlSU1MB4/hXKSkpmfYpU6YMjo6Opn2EyC1yB0iIfKJOnToUKVKEHj16MHjwYDQaDfPmzcvTRxbPMmbMGDZs2EDdunV555130Ov1TJo0icqVKxMeHv7UYy0sLJg0aRIdO3YkMDCQ/v374+vry5kzZ5g1axaBgYFcu3aN1q1bs3v3btOX6NM0bNiQEiVKsGrVKmxtbWnXrl2m91999VXmzZuHs7MzlSpVYu/evWzatMnUrf95hIWFUbduXYYPH86VK1eoVKkSy5cvf6RNj5OTk6ktS3p6OsWLF2fDhg2PvZNXo0YNAD7++GM6deqEpaUlYWFhpmD0b8OHD2fhwoW0aNGCwYMH4+rqyq+//kpERAS///57jo8aPWvWrMeOA/Xee+/x2WefsXHjRurVq8e7776LhYUF06ZNIzU1la+//tq0b6VKlWjUqBE1atTA1dWVQ4cOsWzZMgYOHAjA+fPnadKkCR06dKBSpUpYWFiwYsUKYmNjMz3KFCJXqNcBTYiC70nd4AMCAh67/+7du5VatWoptra2ire3t/K///1PWb9+/TO7aD/sBv+47sSAMnr0aNP6k7rBDxgw4JFj/9sVW1EUZfPmzUq1atUUKysrpUyZMsqMGTOUDz74QLGxsXnCTyGzHTt2KM2aNVOcnJwUa2trpXLlysr48eOV5ORkZe3atYpWq1VeeeUVJT09PUvn+/DDDxVA6dChwyPv3bt3T+nVq5dStGhRxcHBQWnWrJly9uzZR64rK93gFUVR7ty5o3Tr1k1xcnJSnJ2dlW7duilHjx59pBv8tWvXlLZt2youLi6Ks7Oz0r59e+XGjRuP/FkoiqJ8+umnSvHixRWtVpupS/zjfvaXLl1S3njjDcXFxUWxsbFRQkJClD///DPTPg+vZenSpZm2P/w78u86H+dhN/gnLVFRUYqiKMqRI0eUZs2aKQ4ODoqdnZ3SuHFjZc+ePZnO9dlnnykhISGKi4uLYmtrq/j7+yuff/65kpaWpiiKoty+fVsZMGCA4u/vr9jb2yvOzs5KaGiosmTJkqfWKERO0ChKPvrnpRDCLLVp0+ap3Z2FECK/kTZAQojn8uDBg0zrFy5cYM2aNTRq1EidgoQQ4gXIHSAhxHPx8vIyzXN19epVpkyZQmpqKkePHqVcuXJqlyeEEFkijaCFEM+lefPmLFy4kJiYGKytralduzZffPGFhB8hhFmRO0BCCCGEKHSkDZAQQgghCh1VA9COHTsICwvD29sbjUbDypUrn7p/z5490Wg0jywBAQGZ9ps8eTJ+fn7Y2NgQGhrKgQMHcvEqhBBCCGFuVG0DlJSURFBQEL17935kALPH+fHHH/nyyy9N6xkZGQQFBdG+fXvTtsWLFzN06FCmTp1KaGgoEyZMoFmzZpw7d45ixYplqS6DwcCNGzdwdHR8rqHqhRBCCKEeRVG4f/8+3t7ezx4cVMUxiDIBlBUrVjzXMStWrFA0Go1y5coV07aQkJBMA7rp9XrF29tbGT9+fJbPGxUV9dSBwGSRRRZZZJFFlvy7PByw82nMuhfYzJkzadq0qWkSwLS0NA4fPsyIESNM+2i1Wpo2bZpp/pz/Sk1NzTTvjPJ3u/CoqKgsDccvhBBCCPUlJCTg4+ODo6PjM/c12wB048YN1q5dy4IFC0zbbt++jV6vx8PDI9O+Hh4ephmTH2f8+PGMHTv2ke1OTk4SgIQQQggzk5XmK2bbC+zXX3/FxcWFNm3aZPtcI0aMID4+3rRERUVlv0AhhBBC5FtmeQdIURRmzZpFt27dsLKyMm0vWrQoOp2O2NjYTPvHxsbi6en5xPNZW1tjbW2da/UKIYQQIn8xyztA27dv5+LFi/Tp0yfTdisrK2rUqMHmzZtN2wwGA5s3b6Z27dp5XaYQQggh8ilV7wAlJiZy8eJF03pERATh4eG4urpSsmRJRowYwfXr15k7d26m42bOnEloaCiVK1d+5JxDhw6lR48eBAcHExISwoQJE0hKSqJXr165fj1CCCGMDAYDaWlpapchChhLS0t0Ol2OnEvVAHTo0CEaN25sWh86dCgAPXr0YM6cOURHRxMZGZnpmPj4eH7//Xd+/PHHx56zY8eO3Lp1i1GjRhETE0PVqlVZt27dIw2jhRBC5I60tDQiIiIwGAxqlyIKIBcXFzw9PbM9Tp/MBfYYCQkJODs7Ex8fL73AhBDiOSiKQmRkJOnp6VkbjE6ILFIUheTkZG7evImLiwteXl6P7PM8399m2QhaCCFE/pSRkUFycjLe3t7Y2dmpXY4oYGxtbQG4efMmxYoVy9bjMInmQgghcoxerwfI1ENXiJz0MFinp6dn6zwSgIQQQuQ4mUdR5Jac+rslAUgIIYQQhY4EICGEECIX+Pn5MWHCBLXLEE8gAUgIIUShptFonrqMGTPmhc578OBB+vXrl63aGjVqxJAhQ7J1DvF40gssj528Ho+7ozUeTjZqlyKEEAKIjo42vV68eDGjRo3i3Llzpm0ODg6m14qioNfrsbB49tenu7t7zhYqcpTcAcpDc3ZH8NqkXYz787TapQghhPibp6enaXF2dkaj0ZjWz549i6OjI2vXrqVGjRpYW1uza9cuLl26ROvWrfHw8MDBwYGaNWuyadOmTOf97yMwjUbDjBkzaNu2LXZ2dpQrV47Vq1dnq/bff/+dgIAArK2t8fPz47vvvsv0/s8//0y5cuWwsbHBw8ODN954w/TesmXLCAwMxNbWFjc3N5o2bUpSUlK26jEnEoDyUM1SrgD8dTyaHedvqVyNEELkPkVRSE7LUGXJyXF+hw8fzpdffsmZM2eoUqUKiYmJtGzZks2bN3P06FGaN29OWFjYI7MX/NfYsWPp0KEDx48fp2XLlnTt2pW7d+++UE2HDx+mQ4cOdOrUiRMnTjBmzBhGjhzJnDlzAONsC4MHD2bcuHGcO3eOdevW0aBBA8B416tz58707t2bM2fOsG3bNtq1a5ejP7P8Th6B5aEAb2d61PFj9u4rjFp1knVDGmBjmTNzmgghRH70IF1PpVHrVfns0+OaYWeVM19z48aN4+WXXzatu7q6EhQUZFr/9NNPWbFiBatXr2bgwIFPPE/Pnj3p3LkzAF988QUTJ07kwIEDNG/e/Llr+v7772nSpAkjR44EoHz58pw+fZpvvvmGnj17EhkZib29Pa+++iqOjo74+vpSrVo1wBiAMjIyaNeuHb6+vgAEBgY+dw3mTO4A5bGhL5enmKM1V+4kM33HZbXLEUIIkQXBwcGZ1hMTExk2bBgVK1bExcUFBwcHzpw588w7QFWqVDG9tre3x8nJiZs3b75QTWfOnKFu3bqZttWtW5cLFy6g1+t5+eWX8fX1pXTp0nTr1o358+eTnJwMQFBQEE2aNCEwMJD27dvzyy+/cO/evReqw1zJHaA85mhjySevVmLwwqNM2nqR1lW98XWzV7ssIYTIFbaWOk6Pa6baZ+cUe/vMv6eHDRvGxo0b+fbbbylbtiy2tra88cYbpKWlPfU8lpaWmdY1Gk2uTRrr6OjIkSNH2LZtGxs2bGDUqFGMGTOGgwcP4uLiwsaNG9mzZw8bNmzgp59+4uOPP2b//v2UKlUqV+rJb+QOkArCqnhRr2xR0jIMjFp1qlA9cxVCFC4ajQY7KwtVltwcjXr37t307NmTtm3bEhgYiKenJ1euXMm1z3ucihUrsnv37kfqKl++vGmOLAsLC5o2bcrXX3/N8ePHuXLlClu2bAGMfzZ169Zl7NixHD16FCsrK1asWJGn16AmuQOkAo1Gw7jWATSfsJPt52+x/lQMzSs/OqutEEKI/KlcuXIsX76csLAwNBoNI0eOzLU7Obdu3SI8PDzTNi8vLz744ANq1qzJp59+SseOHdm7dy+TJk3i559/BuDPP//k8uXLNGjQgCJFirBmzRoMBgMVKlRg//79bN68mVdeeYVixYqxf/9+bt26RcWKFXPlGvIjuQOkktLuDvRvWBqAsX+cJik1Q+WKhBBCZNX3339PkSJFqFOnDmFhYTRr1ozq1avnymctWLCAatWqZVp++eUXqlevzpIlS1i0aBGVK1dm1KhRjBs3jp49ewLg4uLC8uXLeemll6hYsSJTp05l4cKFBAQE4OTkxI4dO2jZsiXly5fnk08+4bvvvqNFixa5cg35kUaR5y+PSEhIwNnZmfj4eJycnHLtc1LS9bz8w3ai7j6gf4PSjGhZeJK3EKJgSklJISIiglKlSmFjIwO+ipz3tL9jz/P9LXeAVGRjqWNMWAAAM3dFcC7mvsoVCSGEEIWDBCCVNanowSuVPMgwKIxceVIaRAshhBB5QAJQPjD6tQBsLXUcuHKX349cV7scIYQQosCTAJQPFHex5b2m5QAYv+YMcclPH0dCCCGEENkjASif6F23FOWKOXAnKY1v1p979gFCCCGEeGESgPIJKwstn7apDMCCA5GER8WpW5AQQghRgEkAykdqlXajXbXiKAp8svIEeoM0iBZCCCFygwSgfGZEy4o42Vhw8noCv+27qnY5QgghRIEkASifcXe05sPm/gB8u/4cN++nqFyREEIIUfBIAMqHuoSUpEoJZ+6nZvDFX2fULkcIIUQWNGrUiCFDhpjW/fz8mDBhwlOP0Wg0rFy5MtufnVPnKUwkAOVDOq2Gz9pURqOBleE32HPpttolCSFEgRUWFkbz5s0f+97OnTvRaDQcP378uc978OBB+vXrl93yMhkzZgxVq1Z9ZHt0dHSuz+M1Z84cXFxccvUz8pIEoHyqSgkX3gz1BWDkypOkZeTOLMNCCFHY9enTh40bN3Lt2rVH3ps9ezbBwcFUqVLluc/r7u6OnZ1dTpT4TJ6enlhbW+fJZxUUEoDysWGvVKCogxWXbiUxY9dltcsRQogC6dVXX8Xd3Z05c+Zk2p6YmMjSpUvp06cPd+7coXPnzhQvXhw7OzsCAwNZuHDhU8/730dgFy5coEGDBtjY2FCpUiU2btz4yDEfffQR5cuXx87OjtKlSzNy5EjS09MB4x2YsWPHcuzYMTQaDRqNxlTzfx+BnThxgpdeeglbW1vc3Nzo168fiYmJpvd79uxJmzZt+Pbbb/Hy8sLNzY0BAwaYPutFREZG0rp1axwcHHBycqJDhw7Exsaa3j927BiNGzfG0dERJycnatSowaFDhwC4evUqYWFhFClSBHt7ewICAlizZs0L15IVFrl6dpEtznaW/F/LigxdcoyJmy8QVsUbH9e8+deEEELkCEWB9GR1PtvSDjSaZ+5mYWFB9+7dmTNnDh9//DGav49ZunQper2ezp07k5iYSI0aNfjoo49wcnLir7/+olu3bpQpU4aQkJBnfobBYKBdu3Z4eHiwf/9+4uPjM7UXesjR0ZE5c+bg7e3NiRMneOutt3B0dOR///sfHTt25OTJk6xbt45NmzYB4Ozs/Mg5kpKSaNasGbVr1+bgwYPcvHmTvn37MnDgwEwhb+vWrXh5ebF161YuXrxIx44dqVq1Km+99dYzr+dx1/cw/Gzfvp2MjAwGDBhAx44d2bZtGwBdu3alWrVqTJkyBZ1OR3h4OJaWlgAMGDCAtLQ0duzYgb29PadPn8bBweG563geqgagHTt28M0333D48GGio6NZsWIFbdq0eeoxqampjBs3jt9++42YmBi8vLwYNWoUvXv3BowJuVevXpmOsba2JiXFPHtTta1WnMUHo9gfcZexf5xmRo9gtUsSQoisS0+GL7zV+ez/uwFW9lnatXfv3nzzzTds376dRo0aAcbHX6+//jrOzs44OzszbNgw0/6DBg1i/fr1LFmyJEsBaNOmTZw9e5b169fj7W38eXzxxRePtNv55JNPTK/9/PwYNmwYixYt4n//+x+2trY4ODhgYWGBp6fnEz9rwYIFpKSkMHfuXOztjdc/adIkwsLC+Oqrr/Dw8ACgSJEiTJo0CZ1Oh7+/P61atWLz5s0vFIA2b97MiRMniIiIwMfHB4C5c+cSEBDAwYMHqVmzJpGRkXz44Yf4+xt7OpcrV850fGRkJK+//jqBgYEAlC5d+rlreF6qPgJLSkoiKCiIyZMnZ/mYDh06sHnzZmbOnMm5c+dYuHAhFSpUyLSPk5MT0dHRpuXqVfMdT0ejMTaIttBq2HQmlo2nY599kBBCiOfi7+9PnTp1mDVrFgAXL15k586d9OnTBwC9Xs+nn35KYGAgrq6uODg4sH79eiIjI7N0/jNnzuDj42MKPwC1a9d+ZL/FixdTt25dPD09cXBw4JNPPsnyZ/z7s4KCgkzhB6Bu3boYDAbOnftnqqWAgAB0Op1p3cvLi5s3bz7XZ/37M318fEzhB6BSpUq4uLhw5oyxN/PQoUPp27cvTZs25csvv+TSpUumfQcPHsxnn31G3bp1GT169As1On9eqt4BatGixXO1Wl+3bh3bt2/n8uXLuLq6AsaE/F8ajeap6djclPNwpG/90kzdfokxq09Rr2xRbK10zz5QCCHUZmlnvBOj1mc/hz59+jBo0CAmT57M7NmzKVOmDA0bNgTgm2++4ccff2TChAkEBgZib2/PkCFDSEvLucmr9+7dS9euXRk7dizNmjXD2dmZRYsW8d133+XYZ/zbw8dPD2k0GgyG3OtwM2bMGLp06cJff/3F2rVrGT16NIsWLaJt27b07duXZs2a8ddff7FhwwbGjx/Pd999x6BBg3KtHrNqBL169WqCg4P5+uuvKV68OOXLl2fYsGE8ePAg036JiYn4+vri4+ND69atOXXqlEoV55zBTcri7WzD9bgHTNp6Qe1yhBAiazQa42MoNZYstP/5tw4dOqDValmwYAFz586ld+/epvZAu3fvpnXr1rz55psEBQVRunRpzp8/n+VzV6xYkaioKKKjo03b9u3bl2mfPXv24Ovry8cff0xwcDDlypV75AmGlZUVer3+mZ917NgxkpKSTNt2796NVqt95IlJTnl4fVFRUaZtp0+fJi4ujkqVKpm2lS9fnvfff58NGzbQrl07Zs+ebXrPx8eHt99+m+XLl/PBBx/wyy+/5EqtD5lVALp8+TK7du3i5MmTrFixggkTJrBs2TLeffdd0z4VKlRg1qxZrFq1it9++w2DwUCdOnUe273xodTUVBISEjIt+Y2dlQWjXwsAYPqOy1y8mfiMI4QQQjwPBwcHOnbsyIgRI4iOjqZnz56m98qVK8fGjRvZs2cPZ86coX///pl6OD1L06ZNKV++PD169ODYsWPs3LmTjz/+ONM+5cqVIzIykkWLFnHp0iUmTpzIihUrMu3j5+dHREQE4eHh3L59m9TU1Ec+q2vXrtjY2NCjRw9OnjzJ1q1bGTRoEN26dTO1/3lRer2e8PDwTMuZM2do2rQpgYGBdO3alSNHjnDgwAG6d+9Ow4YNCQ4O5sGDBwwcOJBt27Zx9epVdu/ezcGDB6lYsSIAQ4YMYf369URERHDkyBG2bt1qei+3mFUAMhgMaDQa5s+fT0hICC1btuT777/n119/Nd0Fql27Nt27d6dq1ao0bNiQ5cuX4+7uzrRp05543vHjx5sauTk7O2d6hpmfvFLJg5f8i5GuVxi16iSKIpOlCiFETurTpw/37t2jWbNmmdrrfPLJJ1SvXp1mzZrRqFEjPD09n9lp59+0Wi0rVqzgwYMHhISE0LdvXz7//PNM+7z22mu8//77DBw4kKpVq7Jnzx5GjhyZaZ/XX3+d5s2b07hxY9zd3R/bFd/Ozo7169dz9+5datasyRtvvEGTJk2YNGnS8/0wHiMxMZFq1aplWsLCwtBoNKxatYoiRYrQoEEDmjZtSunSpVm8eDEAOp2OO3fu0L17d8qXL0+HDh1o0aIFY8eOBYzBasCAAVSsWJHmzZtTvnx5fv7552zX+zQaJZ98i2o0mmf2AuvRowe7d+/m4sWLpm1nzpyhUqVKnD9/PlOL8n9r3749FhYWTxyzITU1NVOKTkhIwMfHh/j4eJycnF7sgnJJ1N1kmn6/ndQMAz92qkrrqsXVLkkIIUxSUlKIiIigVKlS2NjYqF2OKICe9ncsISEBZ2fnLH1/m9UdoLp163Ljxo1MgzmdP38erVZLiRIlHnuMXq/nxIkTeHl5PfG81tbWODk5ZVryKx9XOwa9VBaAT/88Q0LKiw9aJYQQQhRWqgagxMRE0zNEwPRc82GXvxEjRtC9e3fT/l26dMHNzY1evXpx+vRpduzYwYcffkjv3r2xtbUFYNy4cWzYsIHLly9z5MgR3nzzTa5evUrfvn3z/Ppyy1sNSlO6qD23E1P5fkPWG+EJIYQQwkjVAHTo0CHTM0QwjhFQrVo1Ro0aBRgnd/v3+AcODg5s3LiRuLg4goOD6dq1K2FhYUycONG0z71793jrrbeoWLEiLVu2JCEhgT179mRqhW7urC10jGtdGYC5e69w8nq8yhUJIYQQ5iXftAHKT57nGaKaBi08yh/HbhDk48KKd+qg1T5fl08hhMhp0gZI5LZC2QZIZPZJq4o4WFtwLCqORQejnn2AEELkEfm3tcgtOfV3SwKQGfNwsuGDV8oD8NW6s9xOfHQ8CCGEyEsPp1bIyRGShfi35GTj5Lr/Hcn6ecls8GauWy1flh66xunoBL5ce5Zv2wepXZIQohCzsLDAzs6OW7duYWlpiVYr/84WOUNRFJKTk7l58yYuLi6Z5jF7EdIG6DHMpQ3QQ0ci79Hu5z0ALOlfm5BSripXJIQozNLS0oiIiMjVeaVE4eXi4oKnp6dpmpJ/e57vb7kDVABUL1mEziE+LDwQxciVJ/lzcD0sdfKvLiGEOqysrChXrpw8BhM5ztLSMtt3fh6SAFRA/K+ZP+tOxnAu9j5zdl/hrQal1S5JCFGIabVa6QUm8jW5TVBAFLG3YkQL48RxP2w6T3T8A5UrEkIIIfIvCUAFyBs1ShDsW4TkND3j/jitdjlCCCFEviUBqADRajV82qYyOq2GtSdj2HruptolCSGEEPmSBKACpqKXE73q+AEwetUpUtL16hYkhBBC5EMSgAqgIS+Xx8PJmsi7yUzZdkntcoQQQoh8RwJQAeRgbcGoVwMAmLL9EhG3k1SuSAghhMhfJAAVUC0DPalfrihpGQZGrz4l8/IIIYQQ/yIBqIDSaDSMa10ZKwstO87fYs2JGLVLEkIIIfINCUAFWKmi9rzTsAwA4/48RWJqhsoVCSGEEPmDBKAC7p1GZfB1syM2IZUJG8+rXY4QQgiRL0gAKuBsLHWMec3YIHr2niuciU5QuSIhhBBCfRKACoHGFYrRorIneoPCJytPYjBIg2ghhBCFmwSgQmLkq5Wws9Jx+Oo9lh25pnY5QgghhKokABUS3i62DGlaDoCxq0+x7/IdlSsSQggh1CMBqBDpVbcUdcu6kZSmp8esAzJXmBBCiEJLAlAhYqnTMrNHTZr4FyM1w0C/uYf463i02mUJIYQQeU4CUCFjY6ljarcahAV5k65XGLTwCEsORaldlhBCCJGnJAAVQpY6LRM6VqVTTR8MCvxv2XFm745QuywhhBAiz0gAKqR0Wg3j2wXSt14pAMb+cZpJWy7InGFCCCEKBQlAhZhGo+HjVhV5v2l5AL7dcJ4v156VECSEEKLAkwBUyGk0Gt5rWo5PWlUEYNqOyzJYohBCiAJPApAAoG/90nzZLhCNBubvj2ToknDS9Qa1yxJCCCFyhQQgYdIppCQTO1XDQqthZfgN3p1/hJR0vdplCSGEEDlOApDIJCzIm+nda2BloWXj6Vj6/nqI5LQMtcsSQgghcpQEIPGIl/w9mNOrJvZWOnZdvE23mQeIf5CudllCCCFEjlE1AO3YsYOwsDC8vb3RaDSsXLnymcekpqby8ccf4+vri7W1NX5+fsyaNSvTPkuXLsXf3x8bGxsCAwNZs2ZNLl1BwVWnTFF+6xuKs60lh6/eo/P0fdxOTFW7LCGEECJHqBqAkpKSCAoKYvLkyVk+pkOHDmzevJmZM2dy7tw5Fi5cSIUKFUzv79mzh86dO9OnTx+OHj1KmzZtaNOmDSdPnsyNSyjQqpUswqJ+tSjqYMXp6AQ6TttLdPwDtcsSQgghsk2j5JNBXzQaDStWrKBNmzZP3GfdunV06tSJy5cv4+rq+th9OnbsSFJSEn/++adpW61atahatSpTp07NUi0JCQk4OzsTHx+Pk5PTc11HQXT5ViJvztjPjfgUShSxZX7fUHzd7NUuSwghhMjkeb6/zaoN0OrVqwkODubrr7+mePHilC9fnmHDhvHgwT93Jfbu3UvTpk0zHdesWTP27t37xPOmpqaSkJCQaRH/KO3uwNJ36uDnZse1ew9oP3Uv52Pvq12WEEII8cLMKgBdvnyZXbt2cfLkSVasWMGECRNYtmwZ7777rmmfmJgYPDw8Mh3n4eFBTEzME887fvx4nJ2dTYuPj0+uXYO5Ku5iy5K3a+Pv6cjN+6l0nLaXE9fi1S5LCCGEeCFmFYAMBgMajYb58+cTEhJCy5Yt+f777/n1118z3QV6XiNGjCA+Pt60REXJ7OiPU8zRhkX9ahHk48K95HQ6/7KPAxF31S5LCCGEeG5mFYC8vLwoXrw4zs7Opm0VK1ZEURSuXbsGgKenJ7GxsZmOi42NxdPT84nntba2xsnJKdMiHs/Fzor5fUOpVdqVxNQMus/az7ZzN9UuSwghhHguZhWA6taty40bN0hMTDRtO3/+PFqtlhIlSgBQu3ZtNm/enOm4jRs3Urt27TyttSBzsLZgTq8QXvIvRkq6gbfmHmLtiWi1yxJCCCGyTNUAlJiYSHh4OOHh4QBEREQQHh5OZGQkYHw01b17d9P+Xbp0wc3NjV69enH69Gl27NjBhx9+SO/evbG1tQXgvffeY926dXz33XecPXuWMWPGcOjQIQYOHJjn11eQ2VjqmPpmDVpV8SJdrzBgwRGWHb6mdllCCCFElqgagA4dOkS1atWoVq0aAEOHDqVatWqMGjUKgOjoaFMYAnBwcGDjxo3ExcURHBxM165dCQsLY+LEiaZ96tSpw4IFC5g+fTpBQUEsW7aMlStXUrly5by9uELAykLLxE7V6Bjsg0GBYUuP8eueK2qXJYQQQjxTvhkHKD+RcYCej6IofPrnGWbtjgDgw2YVGNC4rMpVCSGEKGwK7DhAIn/SaDSMfLUi7zUpB8A368/x5dqzSLYWQgiRX0kAEjlCo9Hw/svl+bhlRQCmbr/EyFUnMRgkBAkhhMh/JADltdhTEH9d7SpyzVsNSvNF20A0GvhtXyTDlh4jQ29QuywhhBAiEwlAeWn/dJhaDzaOUruSXNUltCQTOlbFQqth+dHrDFhwhNQMvdplCSGEECYSgPJSyVBQFDi5DK4+eW6ygqB11eJMfbMGVhZa1p+Kpe+vh0hOy1C7LCGEEAKQAJS3vIKg+t/jGq37CAwF+9FQ00oezO5ZEzsrHTsv3Kb7zAMkpKSrXZYQQgghASjPvTQSrJ0g+hiE/6Z2NbmubtmizOsTipONBYeu3qPLL/u4k5iqdllCCCEKOQlAec3BHRp+ZHy9eRykFPwZ1Wv4FmFhv1q42Vtx8noCHafv47aEICGEECqSAKSGkH7gVhaSbsGOb9SuJk8EeDuz5O3aeDnbcPFmIj1nH+C+PA4TQgihEglAarCwgmbjja/3TYXbF9WtJ4+UcXdgft9Q052gvr8eIiVdeocJIYTIexKA1FL+FSj7MhjSYcPHaleTZ0q7O/Br7xAcrS3YH3GXgQuOyjhBQggh8pwEIDU1+wK0FnB+HVzYpHY1eaZycWdm9AjG2kLLpjOx/O/34zJitBBCiDwlAUhN7uUhpL/x9foRoC88bWJCS7sxuUt1dFoNy49c57O/zsjcYUIIIfKMBCC1Nfwf2LnB7fNwcIba1eSpppU8+LZ9FQBm7Y5g0pbC0RZKCCGE+iQAqc3WxTg2EMDW8ZB0W9Vy8lrbaiUYHVYJgO82nmfevqsqVySEEKIwkACUH1TvDp6BkBoPWz9Xu5o816tuKQY3KQfAqFUnWRVecCeLFUIIkT9IAMoPtDpo/pXx9eE5EHNC1XLU8H7TcvSo7YuiwAdLjrH17E21SxJCCFGASQDKL/zqQkBbUAywboRx0tRCRKPRMDosgNZVvckwKLwz/zAHr9xVuywhhBAFlASg/OTlcWBhA1d2wpnValeT57RaDd+2D6JxBXdS0g30nnOQ0zcS1C5LCCFEASQBKD9xKQl13zO+3vAJpD9Qtx4VWOq0/Ny1BjX9inA/JYPusw5w5XaS2mUJIYQoYCQA5Td13wOn4hAXCXsnqV2NKmytdMzoUZOKXk7cTkzlzZn7iU1IUbssIYQQBYgEoPzGyt74KAxg5/eQcEPdelTibGvJ3N4h+LnZce3eA7rN3E9ccpraZQkhhCggJADlR5VfB59akJ4Mm8aoXY1q3B2tmdcnFA8na87HJtJrzkGS0zLULksIIUQBIAEoP9JooMWXgAaOL4aoA2pXpBofVzvm9QnFxc6So5Fx9J93mNQMmUFeCCFE9kgAyq+8q0G1rsbXaz8CQ+GdMb28hyOze9bEzkrHzgu3Gbr4GHqZPFUIIUQ2SADKz14aBVaOcOMIHF+kdjWqqlayCNO61cBSp+GvE9F8svKETJ4qhBDihUkAys8cPaDhh8bXm8ZA6n1Vy1Fb/XLu/NipGloNLDwQxdfrz6ldkhBCCDMlASi/C30bXEtDYizs/E7talTXMtCLz9sGAjBl2yWm77ikckVCCCHMkQSg/M7CGpp9YXy9dzLcvaxuPflA55CSfNTcH4Av1pxlycEolSsSQghhbiQAmYPyzaHMS6BPgw0j1a4mX3inURn6NygNwPDlx1l3MlrlioQQQpgTCUDmQKOBZuNBo4Ozf8KlrWpXlC8Mb+FPx2AfDAoMXhjO7ou31S5JCCGEmVA1AO3YsYOwsDC8vb3RaDSsXLnyqftv27YNjUbzyBITE2PaZ8yYMY+87+/vn8tXkgeK+UPIW8bX64aDXgYE1Gg0fNEukBaVPUnTG+g39xDHouLULksIIYQZUDUAJSUlERQUxOTJk5/ruHPnzhEdHW1aihUrlun9gICATO/v2rUrJ8tWT6PhYOsKt87CoVlqV5Mv6LQaJnSqSr2yRUlK09Nz9gEu3izcveWEEEI8m6oBqEWLFnz22We0bdv2uY4rVqwYnp6epkWrzXwZFhYWmd4vWrRoTpatHtsi8NLHxtdbP4fku+rWk09YW+iY1q0GQT4u3EtO580ZB7h2L1ntsoQQQuRjZtkGqGrVqnh5efHyyy+ze/fuR96/cOEC3t7elC5dmq5duxIZGfnU86WmppKQkJBpybeq94RiAZASB1u/ULuafMPe2oI5PWtSrpgDMQkpdJt5gFv3U9UuSwghRD5lVgHIy8uLqVOn8vvvv/P777/j4+NDo0aNOHLkiGmf0NBQ5syZw7p165gyZQoRERHUr1+f+/ef/Fhk/PjxODs7mxYfH5+8uJwXo7OA5uONrw/NhNhT6taTjxSxt2Jen1CKu9gScTuJHrMOkJCSrnZZQggh8iGNkk/mE9BoNKxYsYI2bdo813ENGzakZMmSzJs377Hvx8XF4evry/fff0+fPn0eu09qaiqpqf/cLUhISMDHx4f4+HicnJyeq548s/hNOPMHlGoA3Vcbe4oJACJuJ9F+6h5uJ6YR4ufK3D4h2Fjq1C5LCCFELktISMDZ2TlL399mdQfocUJCQrh48eIT33dxcaF8+fJP3cfa2honJ6dMS773ymegs4aIHXD2L7WryVdKFbVnTq8QHK0tOHDlLgPmHyFdX3gnkxVCCPEosw9A4eHheHl5PfH9xMRELl269NR9zFIRP6gzyPh6w8eQnqJqOflN5eLOzOxZE2sLLZvP3uR/y45jkBnkhRBC/E3VAJSYmEh4eDjh4eEAREREEB4ebmq0PGLECLp3727af8KECaxatYqLFy9y8uRJhgwZwpYtWxgwYIBpn2HDhrF9+3auXLnCnj17aNu2LTqdjs6dO+fpteWJeu+DoxfcuwL7fla7mnwnpJQrP3etjk6rYcXR64z787TMIC+EEAJQOQAdOnSIatWqUa1aNQCGDh1KtWrVGDVqFADR0dGZenClpaXxwQcfEBgYSMOGDTl27BibNm2iSZMmpn2uXbtG586dqVChAh06dMDNzY19+/bh7u6etxeXF6wdoOlY4+sd30KCTAfxX00qevBd+yAA5uy5wsTNT34UKoQQovDIN42g85PnaUSlOoMBZr0C1w5CUBdoO0XtivKlObsjGPPHaQDGvhZAjzp+6hYkhBAixxWqRtCFnlYLzb8yvj62AK4dVreefKpn3VIMaVoOgNGrTzF560V5HCaEEIWYBKCCoEQN490fgLX/M94VEo94r0k5+tQrBcA368/x9m+HuS/jBAkhRKEkAaigaDoarBzg+iE4sVTtavIljUbDyFcr8UXbQCx1GtafiqXN5N1cvJmodmlCCCHymASggsLRE+p/YHy9aTSkypf6k3QJLcmS/rXxdLLh0q0kWk/axbqTMWqXJYQQIg9JACpIar1rHB/ofjTs+kHtavK1aiWL8MegeoSWciUpTc/bvx3mq3Vn0ctYQUIIUShIACpILG3glc+Nr/f8ZBwfSDyRu6M1v/UNpe/f7YKmbLtEz9kHuJeUpnJlQgghcpsEoILGvxWUagj6VNgwUu1q8j1LnZZPXq3ExM7VsLXUsfPCbV79aRcnr8erXZoQQohcJAGooNFooPmXoNHCmdXGucLEM70W5M2KAXXwdbPjetwDXp+yh2WHr6ldlhBCiFwiAagg8qgEwX2Mr9eNAH2GuvWYCX9PJ1YPrMdL/sVIzTAwbOkxRq48SVqGDCsghBAFjQSggqrx/4GNC8SehCO/ql2N2XC2tWRG92DToInz9l2l0/S9xCbIZLNCCFGQSAAqqOxcofHHxtdbPoMH99Stx4xotRqGNC3PzB7BONpYcCQyjld/2sXBK3fVLk0IIUQOkQBUkAX3BveK8OAubPtK7WrMTpOKHvwxsB4VPBy5dT+VztP3MWd3hEyhIYQQBYAEoIJMZwHNxxtfH5gON8+qW48Z8itqz4oBdQgL8ibDoDDmj9N8sOQYD9L0apcmhBAiGyQAFXRlGkOFVqDoYd1wkLsXz83OyoKJnarySauK6LQalh+9Trspe4i8k6x2aUIIIV6QBKDC4JVPQWcFl7fC+XVqV2OWNBoNfeuX5rc+objZW3EmOoGwSbvYdu6m2qUJIYR4ARKACgO3MsZpMgDW/x9kpKpbjxmrXcaNPwfXI8jHhfgH6fSac5BJWy5gkCk0hBDCrEgAKiwaDAMHD7h7GfZNUbsas+blbMuS/rXoHFISRYFvN5yn/2+HSUhJV7s0IYQQWSQBqLCwdoQmo42vt3wGJ5apW4+Zs7bQMb5dIF+2C8RKp2Xj6VjaTNrNhdj7apcmhBAiCyQAFSZBnaHy62BIh9/7wL6paldk9jqFlGTJ27Xxcrbh8u0kWk/ezZoT0WqXJYQQ4hkkABUmWi20mwEh/Y3r6z6CTWOkZ1g2VfVx4Y9B9ahd2o3kND3vzj/C+LVnyNDLFBpCCJFfSQAqbLRaaPEVNBllXN/1A6waKPOFZVNRB2vm9QmhX4PSAEzbfpkesw9wNylN5cqEEEI8jgSgwkijgfofwGs/GWeND/8NFneFNBnXJjssdFr+r2VFJnWphp2Vjt0X7xD20y5OXItXuzQhhBD/IQGoMKveHTrOBwsb4/hAc1tDssx3lV2vVvFmxbt1KVXUnutxD3h96h6WHIpSuywhhBD/IgGosPNvCd1XGWeOv3YAZjWH+GtqV2X2Kng6snJAXZpWLEZahoH/LTvOxytOkJYh7YKEECI/kAAkoGQt6L0OnIrD7XMw8xW4eUbtqsyes60l07sFM/Tl8mg0MH9/JB2n7yUmPkXt0oQQotCTACSMilWEPhugaAVIuG68ExS5X+2qzJ5Wq2Fwk3LM6lETJxsLjkbG8epPOzkaeU/t0oQQolCTACT+4VzCeCeoRAikxMHc1+DcWrWrKhAa+xfjj0H18Pd05HZiGt1nHeDkdWkcLYQQapEAJDKzczW2CSrXDDJSYFFXODJP7aoKBF83e5a/W4eafkW4n5JBt5n7ORcjI0cLIYQaJACJR1nZQaf5ULUrKHpYPRB2fCsDJuYAOysLZvWsSVAJZ+4lp9N1xn4u30pUuywhhCh0JACJx9NZQuvJUG+ocX3Lp7D2IzBIL6bscrSx5NfeIVT0cuJ2YipdZ+wn6q6MwSSEEHlJ1QC0Y8cOwsLC8Pb2RqPRsHLlyqfuv23bNjQazSNLTExMpv0mT56Mn58fNjY2hIaGcuDAgVy8igJMo4Gmo6H5l8b1A9Pg996QkapuXQWAi50V8/qEULaYA9HxKXSZsY/o+AdqlyWEEIWGqgEoKSmJoKAgJk+e/FzHnTt3jujoaNNSrFgx03uLFy9m6NChjB49miNHjhAUFESzZs24efNmTpdfeNR6B16fCVpLOLUC5reHlAS1qzJ7RR2smd83FF83O6LuPqDrL/u5dV/CpRBC5AWNouSPhh0ajYYVK1bQpk2bJ+6zbds2GjduzL1793BxcXnsPqGhodSsWZNJkyYBYDAY8PHxYdCgQQwfPjxLtSQkJODs7Ex8fDxOTk7PeykF16WtsPhNSEsEzyrw5u/gUOzZx4mnunYvmY7T9nE97gEVPBxZ1K8WReyt1C5LCCHMzvN8f5tlG6CqVavi5eXFyy+/zO7du03b09LSOHz4ME2bNjVt02q1NG3alL1796pRasFSpjH0/BPs3SHmuHHAxLuX1a7K7JUoYsf8vqEUc7TmXOx9us3aT/yDdLXLEkKIAs2sApCXlxdTp07l999/5/fff8fHx4dGjRpx5MgRAG7fvo1er8fDwyPTcR4eHo+0E/q31NRUEhISMi3iCbyrQe/1UMQP7kUYQ9CNcLWrMnt+Re1Z8FYobvZWnLyeQK/ZB0hKzVC7LCGEKLDMKgBVqFCB/v37U6NGDerUqcOsWbOoU6cOP/zwQ7bOO378eJydnU2Lj49PDlVcQLmVgd4bwDMQkm7BnFZweZvaVZm9ssUcmdcnFCcbC45ExtHn14OkpOvVLksIIQokswpAjxMSEsLFixcBKFq0KDqdjtjY2Ez7xMbG4unp+cRzjBgxgvj4eNMSFSUzdz+Towf0XAN+9Y1tgn57A07+rnZVZq+StxNz+4TiYG3Bvst36T/vMKkZEoKEECKnmX0ACg8Px8vLCwArKytq1KjB5s2bTe8bDAY2b95M7dq1n3gOa2trnJycMi0iC2ycjA2hK7UBQzos6wP7p6ldldmr6uPC7F41sbXUsf38LQYtOEq6XsZfEkKInKRqAEpMTCQ8PJzw8HAAIiIiCA8PJzIyEjDemenevbtp/wkTJrBq1SouXrzIyZMnGTJkCFu2bGHAgAGmfYYOHcovv/zCr7/+ypkzZ3jnnXdISkqiV69eeXpthYaFNbwxC2q+BSiw9n+weZyMGp1NNf1c+aV7MFYWWjacjmXokmPoDfIzFUKInGLxIgdFRUWh0WgoUaIEAAcOHGDBggVUqlSJfv36Zfk8hw4donHjxqb1oUONow736NGDOXPmEB0dbQpDYOzl9cEHH3D9+nXs7OyoUqUKmzZtynSOjh07cuvWLUaNGkVMTAxVq1Zl3bp1jzSMFjlIq4OW3xgfi235DHZ+B4mx8OqPoHuhv2ICqFeuKFPfrE6/uYf549gNrC20fP16FbRajdqlCSGE2XuhcYDq169Pv3796NatGzExMVSoUIGAgAAuXLjAoEGDGDVqVG7UmmdkHKBsOPwr/DkEFAOUb2G8O2Rlp3ZVZm3NiWgGLjiCQYFutXwZ1zoAjUZCkBBC/FeujwN08uRJQkJCAFiyZAmVK1dmz549zJ8/nzlz5rzIKUVBUaMHdJwPFjZwfi3MawPJd9Wuyqy1DPTiuw5BaDQwb99VvlhzhnwyfqkQQpitFwpA6enpWFtbA7Bp0yZee+01APz9/YmOjs656oR58m8J3VaCjTNE7YfZLSD+mtpVmbW21UrweZtAAH7ZGcEPmy6oXJEQQpi3FwpAAQEBTJ06lZ07d7Jx40aaN28OwI0bN3Bzc8vRAoWZ8q0NvdaBozfcOmscMPHmWbWrMmtdQksy6tVKAEzcfIGft11UuSIhhDBfLxSAvvrqK6ZNm0ajRo3o3LkzQUFBAKxevdr0aEwIPCpBnw1QtDwkXIdZzSDqgNpVmbXe9Urxv+YVAPh63Tlm745QuSIhhDBPLzwZql6vJyEhgSJFipi2XblyBTs7u0yzs5sjaQSdw5LvwoIOcO0gWNhC+9lQoYXaVZm17zecY+IW4x2g8e0C6RxSUuWKhBBCfbneCPrBgwekpqaaws/Vq1eZMGEC586dM/vwI3KBnSt0XwXlmkHGA1jYCea2Nk6fIY15X8j7L5fnrfqlAPi/FSdYcVTaWAkhxPN4oQDUunVr5s6dC0BcXByhoaF89913tGnThilTpuRogaKAsLKHTvMhpB9odMbwM7c1TG8Ep1aAQaZ7eB4ajYb/a1mRN2uVRFHggyXHWHNCOiAIIURWvVAAOnLkCPXr1wdg2bJleHh4cPXqVebOncvEiRNztEBRgOgsjQMmDj4KIf2Nj8Oiw2FpT5gUDIdmQ3qK2lWaDY1Gw7jXKvNGjRIYFBi88ChbzsY++0AhhBAvFoCSk5NxdHQEYMOGDbRr1w6tVkutWrW4evVqjhYoCqAivtDya3j/JDT8CGyLwN3LxgEUf6wCu36AlHi1qzQLWq2Gr16vwqtVvMgwKLz92xF2XbitdllCCJHvvVAAKlu2LCtXriQqKor169fzyiuvAHDz5k1pNCyyzr4oNP4/GHISmo0HpxLGKTQ2jYEfKsPGUXA/Ru0q8z2dVsMPHavyciUP0jIMvDX3EAciZPBJIYR4mhcKQKNGjWLYsGH4+fkREhJimml9w4YNVKtWLUcLFIWAtQPUfhfeC4c2U8G9IqQmwO4fYUIgrB4Mdy6pXWW+ZqnTMqlLNRqUd+dBup7ecw4SHhWndllCCJFvvXA3+JiYGKKjowkKCkKrNeaoAwcO4OTkhL+/f44WmdekG7zKDAa4sB52TYCofX9v1ECl16DuECheXcXi8rcHaXp6zTnAvst3cbKxYGG/WgR4O6tdlhBC5Inn+f5+4QD00LVrxu63D2eGLwgkAOUjV/fC7glwft0/20o1gHrvQ+nGIJOCPiIxNYPuM/dzJDIOV3srFverRTkPR7XLEkKIXJfr4wAZDAbGjRuHs7Mzvr6++Pr64uLiwqefforBYHihooV4LN/a0GUxvLMXqnQCrQVE7IB5bWFaAzj5O+gz1K4yX3GwtmB2rxAqF3fiblIaXWfs58rtJLXLEkKIfOWF7gCNGDGCmTNnMnbsWOrWrQvArl27GDNmDG+99Raff/55jheal+QOUD4WFwV7J8ORXyE92bitiB/UGQxVu4Clrarl5Sf3ktLoNH0f52LvU9zFlsX9a1GiiJ3aZQkhRK7J9Udg3t7eTJ061TQL/EOrVq3i3Xff5fr16897ynxFApAZSL4LB6bD/mnw4O8eT/buEPo21OwLti6qlpdf3LyfQqdp+7h8OwlfNzuW9K+Nh5ON2mUJIUSuyPVHYHfv3n1sQ2d/f3/u3pXutyIP2LlCo+HGsYRafA3OPpB0C7Z8auxCv+ETSLihdpWqK+Zow/y3QvFxteXqnWS6/LKP24mpapclhBCqe6EAFBQUxKRJkx7ZPmnSJKpUqZLtooTIMit7CO1vHF267XQoVgnS7sOen2BCFVg1AG6dV7tKVXk527Kgby08nWy4dCuJbjMPEJecpnZZQgihqhd6BLZ9+3ZatWpFyZIlTWMA7d27l6ioKNasWWOaJsNcySMwM6YocGGDsQt95J6/N2rAv5Wx51iJYDWrU9XlW4l0mGa8A+TjasuYsACaVPRQuywhhMgxuf4IrGHDhpw/f562bdsSFxdHXFwc7dq149SpU8ybN++FihYiR2g0UL4Z9F4LvTdAhZaAAmf/hBlNYM6rcHFToZyFvrS7A/P7huLtbEPU3Qf0+fUQb809xLV7yWqXJoQQeS7b4wD927Fjx6hevTp6vXnP7C13gAqYm2eNo0qfWAKGv7vMh/SD5l+CVqdubSpISs1g4uYLzNwVQYZBwcZSy+Am5ehbrzRWFi/0byIhhMgXcv0OkBBmpZg/tJ0C7x0zzkIPxh5ki7pAaqK6tanA3tqCES0rsua9+oSUciUl3cDX687R4scd7LkoE6kKIQoHCUCi8HAuYZyFvv2vYGFjHF16TktIiFa7MlWU93Bkcb9afN8hiKIOVly6lUSXGft5b9FRbiakqF2eEELkKglAovAJaAM9/gC7ohB9DGY0hdhTalelCo1GQ7vqJdj8QSO61/ZFo4FV4Tdo8t12Zu+OIEMvI7sLIQqm52oD1K5du6e+HxcXx/bt26UNkDAPdyNgfnu4cwGsHKHDr1C2idpVqerEtXg+WXmCY9fiAajk5cRnbStTvWQRlSsTQohny7WRoHv16pWl/WbPnp3VU+ZLEoAKkeS7sPhNuLobNDp49Qeo0UPtqlSlNygsOhjJ1+vOEf8gHYCOwT581MIfV3srlasTQogny9PZ4AsiCUCFTEYqrB4Exxcb1+u9Dy+NAm3hfkJ8JzGVL9eeZenhawC42FkyvLk/HYJ90Go1KlcnhBCPkgCUTRKACiFFgW3jYftXxvWAdtBmCljKvFkHr9xl5MqTnI25D0C1ki582roylYs7q1yZEEJkJgEomyQAFWLhC2D1YDCkg08odFoI9m5qV6W6dL2BX/dc4YeN50lK06PVQPfafgx9pTxONpZqlyeEEIAEoGyTAFTIReyARW9Cajy4loauy8CtjNpV5Qsx8Sl89tdp/jxuHDrA3dGaT1pV5LUgbzQaeSwmhFCXBKBskgAkuHUO5r8BcZFgWwQ6LQDfOmpXlW/sunCbUatOcvl2EgC1S7vxaZsAyhZzVLkyIURhZjYjQe/YsYOwsDC8vY3/ely5cmWWj929ezcWFhZUrVo10/YxY8ag0WgyLf7+/jlbuCj43CtA381QvAY8uAdzW8PxpWpXlW/UK1eUtUPqM+yV8lhbaNl7+Q4tftzJV+vOkpyWoXZ5QgjxTKoGoKSkJIKCgpg8efJzHRcXF0f37t1p0uTxY7YEBAQQHR1tWnbt2pUT5YrCxqEY9PgT/F8FfRos7ws7vimUE6k+jrWFjoEvlWPT0IY08S9Gul5hyrZLvPz9DtafikFuLgsh8jMLNT+8RYsWtGjR4rmPe/vtt+nSpQs6ne6xd40sLCzw9PTMgQpFoWdlBx3mwsZRsHcSbPkM7l2BVyeAThr/Avi42jGzZ002no5lzOpTXI97QP95h3nJvxhjwgIo6WandolCCPEIsxvoZPbs2Vy+fJnRo0c/cZ8LFy7g7e1N6dKl6dq1K5GRkU89Z2pqKgkJCZkWIUy0Omj2ObT8FjRaOPob/PY6PIhTu7J85eVKHmwa2pABjctgqdOw5exNXv5hOxM3XyA1w7xHhxdCFDxmFYAuXLjA8OHD+e2337CwePzNq9DQUObMmcO6deuYMmUKERER1K9fn/v37z/xvOPHj8fZ2dm0+Pj45NYlCHMW8hZ0XgSW9hCxHWY1NzaSFia2Vjo+bObP2vcaUKeMG6kZBr7feJ7mE3ay4/wttcsTQggTswlAer2eLl26MHbsWMqXL//E/Vq0aEH79u2pUqUKzZo1Y82aNcTFxbFkyZInHjNixAji4+NNS1RUVG5cgigIyjeD3mvB0QtunTFOpHr9iNpV5Ttlizkwv28oEztXo5ijNRG3k+g+6wAD5h8hOv6B2uUJIUT+6Qav0WhYsWIFbdq0eez7cXFxFClSBJ1OZ9pmMBhQFAWdTseGDRt46aWXHntszZo1adq0KePHj89SLdINXjxT/HVY0AFiT4KlHbw+A/xbqV1VvnQ/JZ0fNl5gzp4IDArYWekY0LgsXUNL4mInc4sJIXKO2XSDfx5OTk6cOHGC8PBw0/L2229ToUIFwsPDCQ0NfexxiYmJXLp0CS8vrzyuWBRozsWh11oo2xTSk2FRV9g3Re2q8iVHG0tGhVXij0H1qF7SheQ0Pd+sP0ft8Vv4ZOUJLt5MVLtEIUQhpGovsMTERC5evGhaj4iIIDw8HFdXV0qWLMmIESO4fv06c+fORavVUrly5UzHFytWDBsbm0zbhw0bRlhYGL6+vty4cYPRo0ej0+no3Llznl2XKCRsnKDzYlgzDA7PhnXD4W4ENB9vbDgtMgnwdmbZ23VYdew603dEcCY6gd/2RfLbvkgaVXCnd91S1C9XVEaUFkLkCVUD0KFDh2jcuLFpfejQoQD06NGDOXPmEB0d/cweXP917do1OnfuzJ07d3B3d6devXrs27cPd3f3HK1dCAB0FvDqD+BaythV/sA0Y8PoN2aClb3a1eU7Wq2GttVK0KZqcfZH3GXWrgg2noll27lbbDt3i7LFHOhV14921UpgayUhUgiRe/JNG6D8RNoAiRdyagUs7w/6VPAKgi5LwFHGo3qWq3eS+HXPVZYciiIx1TiKtIudJZ1DStK9ti9ezrYqVyiEMBcyF1g2SQASLyzqACzsBMl3wKkEdF0CHgFqV2UW7qeks/TQNebsuULk3WQAdFoNLQO96F3Xj2oli6hcoRAiv5MAlE0SgES23L0M8zvAnQtg7QQdfoUyj++hmGcUBZLvQnwUJN4E72rgkD8fC+sNCpvPxDJrdwT7Lt81ba9W0oVedUvRorInljqz6b8hhMhDEoCySQKQyLbku7D4Tbi6G7R/txOq3j33Ps9ggMQYiIsyhpy4yL//G/XPf9OT/tnf2gmajoEavUCbf8PEqRvxzN59hdXhN0jTGwDwdLKhex1fuoRIN3ohRGYSgLJJApDIERmpsGognPh7EM56Q+GlkS8WODLSIOFa5kDz76ATfx0M6c8+j30xsLCB+L87F5SsDWE/gnuF568pD926n8qC/ZHM23eV24mpANhYamlXvQS96/pRtpijyhUKIfIDCUDZJAFI5BhFgW3jYftXxvWAdtBmCljaZN4vNfFfwSby0aBzPwZ4xv+qGh04FQcXH3D2+c9/S4JzCePnGvRw4BfYPM54V0hnBfWHQb33wSJ/31FJzdDz57FoZu6K4HT0P3P2NSjvTu+6fjQo545WK93ohSisJABlkwQgkePCF8DqQWDIgBI1oXhw5js4D+49+xwWNo8PNg/XHb2M3fKzKi4K/hoKFzYY190rwmsTwSfkxa4xDymKwoGIu8zaHcGG07E8/C1Wxt2eXnVL0a56ceysVB3lQwihAglA2SQBSOSKy9thcTdIjX/8+zbOmQPNf4OOfVHI6UECFQVO/g5rP4Lk24AGavaFJqOMAz2agcg7yfy69wpLDkZx/+9u9M62/3Sj93aRbvRCFBYSgLJJApDINbfOw8EZYGENLiUzBx01A0fyXdjwCYTPN647FYdW30GFFurV9JwSUzNYdiiK2XuucPXOP93om1f2pHfdUlQv6SKjTAtRwEkAyiYJQKLQurQV/hwC964Y1wPaQvOvwNFDzaqei96gsPXsTWbtjmDPpTum7UE+LvSu60fLQC/pRi9EASUBKJskAIlCLS0Ztn8JeyaBojc+mnvlc6j2Zs4/gstlZ6ITmL07gpXhN0jL+Kcbfe96fvSpVxqdNJgWokCRAJRNEoCEAKKPGRtuRx8zrvvVN3aZdyujbl0v4HbiP93ob903dqNvHuDJhE5VsbGUOceEKCgkAGWTBCAh/qbPgP1TYMvnkPEAdNbQ6COoMxh0lmpX99xSM/QsO3yNsatPk6Y3UNOvCL90D5YBFYUoIJ7n+1sehAshnkxnAXUGwbt7oXRj40Svm8fB9EZw/bDa1T03awsdXUN9mdsnBEcbCw5euccbU/dyPe6B2qUJIfKYBCAhxLO5loJuK6DtNLB1hdiTMKMprBthHMTRzNQq7cayt+vg6WTDxZuJtPt5N6dvJDz7QCFEgSEBSAiRNRoNBHWCgQchsAMoBtj3M/xcGy5sUru651bB05EVA+pQwcOR2IRUOkzby56Lt9UuSwiRRyQACSGej31ReP0X6Pq7cYDG+EiY/zr8/hYkmVeA8HK2ZcnbtQkt5UpiagY9Zh9gVfh1tcsSQuQBCUBCiBdTrqmxbVCtAaDRGid9nVQTji0CM+pb4Wxrydw+IbSq4kW6XuG9ReFM33EJ6R8iRMEmAUgI8eKsHaD5F9B3E3hUhgd3YUV/mNf2n8EUzYC1hY6fOlWjT71SAHyx5ixj/ziN3iAhSIiCSgKQECL7iteAftugyWjjpK2Xt8LkWrB7orErvRnQajWMfLUSn7SqCMCcPVcYtPAIKel6lSsTQuQGCUBCiJyhs4T6Q+GdPcZBEzMewMaRMOOlfwZTNAN965dmYudqWOo0rDkRQ/dZB4hPTle7LCFEDpMAJITIWW5loMcf8Nok4zQa0cdgemPYOMo4zYYZeC3Im197h+BobcGBiLu8MXUPN2SsICEKFAlAQoicp9FA9W4w4KBxQlVFD7t/hCm14fI2tavLkjplirL0ndp4Otlw4WYi7X7ew9kYGStIiIJCApAQIvc4ekD7OdB5ETgVNzaMntsa/hhiFm2D/D2dWP5uHcoVcyAmIYX2U/ay55J5dfUXQjyeBCAhRO6r0ALe3Qch/QANHJ4NK98GQ/5vYOztYsuyt+sQ4ufK/dQMes46yOpjN9QuSwiRTRKAhBB5w8YJWn4DHeeB1gJOLDV2mTeDEORsZxwrqGWgJ2l6A4MXHmXGzstqlyWEyAYJQEKIvFUxzPhYzBSCzONOkI2ljkmdq9Ozjh8An/11hk//PI1BxgoSwixJABJC5L2KYfDG7L9D0BJY+Y5ZhCCtVsPosEr8X0t/AGbuimDQoqOkZuT/2oUQmUkAEkKoo9Jr/4Sg44vNJgRpNBr6NSjDj52qYqnT8NfxaLrPPED8AxkrSAhzIgFICKGeSq/BG7NAo/s7BL1rFiEIoHXV4vzaKwQHawv2R9yl/dQ9RMfLWEFCmAsJQEIIdVVqDe1n/x2CFplVCKpTtihL+temmKM152ONYwWdi7mvdllCiCxQNQDt2LGDsLAwvL290Wg0rFy5MsvH7t69GwsLC6pWrfrIe5MnT8bPzw8bGxtCQ0M5cOBAzhUthMh5/w1BqwaYTQiq5G0cK6hsMQei41N4Y+oe9l2+o3ZZQohnUDUAJSUlERQUxOTJk5/ruLi4OLp3706TJk0eeW/x4sUMHTqU0aNHc+TIEYKCgmjWrBk3b97MqbKFELmhUut/HocdW2hWIahEETuWvV2bmn5FuJ+SQfeZB/jzuIwVJER+plEUJV/04dRoNKxYsYI2bdo8c99OnTpRrlw5dDodK1euJDw83PReaGgoNWvWZNKkSQAYDAZ8fHwYNGgQw4cPz1ItCQkJODs7Ex8fj5OT04tcjhDiRZ1aCct6G6fPCOoCrSeBVqd2VVmSkq5nyKJw1p2KQaOBT1pVok+9UmqXJUSh8Tzf32bXBmj27NlcvnyZ0aNHP/JeWloahw8fpmnTpqZtWq2Wpk2bsnfv3rwsUwjxogLawBsz/74TtABWDzKbO0E2ljomd61Oj9q+KAp8+udpPv9LxgoSIj+yULuA53HhwgWGDx/Ozp07sbB4tPTbt2+j1+vx8PDItN3Dw4OzZ88+8bypqamkpqaa1hMSZMJDIVQV0BYUBX7vC+Hzjdte+8ks7gTptBrGvBaAl4stX649yy87I4hJSOXb9lWwtsj/9QtRWJjNHSC9Xk+XLl0YO3Ys5cuXz9Fzjx8/HmdnZ9Pi4+OTo+cXQryAyu3g9RnGO0Hh82H1YDAY1K4qSzQaDW83LMMPHYOw0Gr449gNes46SEKKjBUkRH5hNgHo/v37HDp0iIEDB2JhYYGFhQXjxo3j2LFjWFhYsGXLFooWLYpOpyM2NjbTsbGxsXh6ej7x3CNGjCA+Pt60REVF5fblCCGyonI7eP2Xv0PQb38/DjOPEATQtloJZveqiYO1BXsv36HD1L3ExKeoXZYQAjMKQE5OTpw4cYLw8HDT8vbbb1OhQgXCw8MJDQ3FysqKGjVqsHnzZtNxBoOBzZs3U7t27See29raGicnp0yLECKfqPz63yFIa5YhqH45dxb3r4W7ozVnY+7T7ufdnI+VsYKEUJuqbYASExO5ePGiaT0iIoLw8HBcXV0pWbIkI0aM4Pr168ydOxetVkvlypUzHV+sWDFsbGwybR86dCg9evQgODiYkJAQJkyYQFJSEr169cqz6xJC5LDKrxv/+3tfYwjSAGE/gdY8/g0X4O3M8nfq0GP2AS7fSuKNKXv46vUqtAj0Urs0IQotVQPQoUOHaNy4sWl96NChAPTo0YM5c+YQHR1NZGTkc52zY8eO3Lp1i1GjRhETE0PVqlVZt27dIw2jhRBmpvLrxobRy9+Co78Zt5lRCPJxteP3t+vQd+4hDl+9xzvzj9CqihfjXgvAzcFa7fKEKHTyzThA+YmMAyREPnZimTEEKQao1g3CJppNCAJIzdAzactFft52Cb1BwdXeik9bV6ZVFbkbJER2FehxgIQQhVzgG9Du7zZBR+fBH+bTOwzA2kLHB69UYOW7dfH3dORuUhoDFhzhnd8Oc+t+6rNPIITIERKAhBDmJ/ANaDv9nxD053tmFYIAAks4s3pgPQY3KYeFVsPakzG88sN2Vh+7gdyYFyL3SQASQpinKu3/CUFH5sKfQ8wuBFlZaBn6cnlWDqhLRS8n7iWnM3jhUd7+7TA370t3eSFykwQgIYT5qtIe2k77OwT9apYhCKBycWdWDajL+03LY6HVsP5ULK/8sINV4dflbpAQuUQCkBDCvFXpkDkE/fW+WYYgKwst7zUtx+qB9QjwdiIuOZ33FoXTb95hbibI3SAhcpoEICGE+avSAdpMNYagw3PMNgQBVPJ2YuWAunzwcnksdRo2no7l5R92sPzINbkbJEQOkgAkhCgYgjoaQxCav0PQULMNQZY6LYOalOOPQfUILO5M/IN0hi45Rt9fDxErd4OEyBESgIQQBUdQR+PjMDRweDas+cBsQxCAv6cTK96tw4fNKmCl07L57E1e/n47yw7L3SAhsksCkBCiYAnqCG3/vhN0aJbZhyALnZYBjcvy5+B6BJVwJiElg2FLj9FrzkGi4x+oXZ4QZksCkBCi4AnqBG2m8E8IGmbWIQigvIcjv79Th4+a+2Ol07Lt3C1e+X4HSw5Gyd0gIV6ABCAhRMFUtfO/QtBMYwgy86BgodPyTqMy/DW4HlV9XLifmsH/fj9Oj9kHuREnd4OEeB4SgIQQBVfVztDmZ0wh6K8PzD4EAZT7+27Q/7X0x8pCy47zt3jlhx0sPBApd4OEyCIJQEKIgq1qlwIZgnRaDf0alGHN4PpUL+lCYmoGI5afoPusA1y7l6x2eULkexKAhBAFX9Uu0HoymUKQPl3tqnJE2WIOLH27Dp+0qoi1hZadF27T7IcdzN9/Ve4GCfEUEoCEEIVDta7QehKmEDStIVw7pHZVOUKn1dC3fmnWvlefYN8iJKXp+XjFSbrO2E/UXbkbJMTjSAASQhQe1d6E9nPA1hVunoIZTWHNh5CSoHZlOaK0uwOL+9dm1KuVsLHUsufSHZpN2MG8fVcxGORukBD/plHkHukjEhIScHZ2Jj4+HicnJ7XLEULktKQ7sOFjOLbQuO7oDa2+Bf9W6taVg67cTuJ/y45z4MpdAGqVduXr14Mo6WancmVC5J7n+f6WAPQYEoCEKCQubTXOIH/vinG9Yhi0+AacvNSsKscYDApz917hq3XneJCux9ZSx/AW/nSr5YtWq1G7PCFynASgbJIAJEQhkpYMO76G3RNB0YO1EzQdDTV6g7ZgtBK4esd4N2h/hPFuUEgpV755owq+bvYqVyZEzpIAlE0SgIQohGJOwh+D4fph47pPKIT9CMUqqltXDjEYFObvv8r4tWdJTjPeDRrR0p83Q+VukCg4JABlkwQgIQopgx4OzoTNYyEtEbSWUG8I1B8GljZqV5cjou4m8+GyY+y7bLwbVKeMG1+9XgUfV2kbJMyfBKBskgAkRCEXf83YO+zcGuO6axkImwClGqhaVk4xGBTm7bvKl2vP8iBdj72Vjk9erUSnmj5oNHI3SJgvCUDZJAFICIGiwJk/jEEoMca4reqb8MqnYOeqbm055MrtJIYtPcahq/cAaFjenS9fD8TL2VblyoR4MRKAskkCkBDCJCUeNo01Dp4IYFcUmn8JgW9AAbhbojcozNoVwTcbzpGWYcDRxoIxYQG0q15c7gYJsyMBKJskAAkhHhG5D/54D26dNa6XaQKvfg9F/FQtK6dcvJnIB0uPcSwqDoCmFYvxRbtAijkWjLZPonCQAJRNEoCEEI+VkQZ7foTt34A+FSxsofH/Qa13QWehdnXZlqE3MG3HZSZsOk+6XsHFzpJxrSsTVsVL7gYJsyABKJskAAkhnur2ReMAild2Gtc9AyFsIhSvrmpZOeVsTAIfLDnGqRvGKUJaBnryaevKuDlYq1yZEE8nASibJAAJIZ5JUeDob7DhE0iJA40WQt+Gxh+DtYPa1WVbut7A5K0XmbTlIhkGBTd7Kz5vW5nmlQvGKNmiYJIAlE0SgIQQWZZ4C9aPgBNLjevOPtDqOyjfTN26csjJ6/F8sOQY52LvA9C6qjdjXwvAxc5K5cqEeJQEoGySACSEeG4XNsFf70NcpHE9oC00/wocPdStKwekZuj5cdMFpm6/hEGBYo7WfPl6IC/5m/+1iYJFAlA2SQASQryQtCTYNh72/mycV8zGGV4eB9W6F4h5xcKj4vhgSTiXbiUB0L5GCUaGVcLJxlLlyoQwep7vb1X/j9yxYwdhYWF4e3uj0WhYuXLlU/fftWsXdevWxc3NDVtbW/z9/fnhhx8y7TNmzBg0Gk2mxd/fPxevQggh/mZlD698Bv22gldV4xhCf7wHc1rBrXNqV5dtVX1c+Gtwfd6qXwqNBpYevkazH3aw4/wttUsT4rmpGoCSkpIICgpi8uTJWdrf3t6egQMHsmPHDs6cOcMnn3zCJ598wvTp0zPtFxAQQHR0tGnZtWtXbpQvhBCP5xUEfTdDsy/A0g4i98DUerDtS8hIVbu6bLGx1PFxq0os6V8bXzc7ouNT6D7rAP+34gSJqRlqlydEluWbR2AajYYVK1bQpk2b5zquXbt22NvbM2/ePMB4B2jlypWEh4e/cC3yCEwIkWPiIuGvD+DCBuN60fLGWeZ966hbVw5ITsvg63XnmLPnCgAlitjy9RtVqFOmqLqFiULLbB6BZdfRo0fZs2cPDRs2zLT9woULeHt7U7p0abp27UpkZORTz5OamkpCQkKmRQghcoRLSeiyBN6YDfbF4PZ5mN3COMdYWpLa1WWLnZUFY14LYMFboZQoYsu1ew/o8st+xqw+RXKa3A0S+ZtZBqASJUpgbW1NcHAwAwYMoG/fvqb3QkNDmTNnDuvWrWPKlClERERQv3597t+//8TzjR8/HmdnZ9Pi4+OTF5chhCgsNBqo3A4GHoDq3Y3bDkw3PhaL3K9ubTmgTpmirBvSgC6hJQGYs+cKLX/cyaErd1WuTIgnM8tHYBERESQmJrJv3z6GDx/OpEmT6Ny582P3jYuLw9fXl++//54+ffo8dp/U1FRSU/95Lp+QkICPj488AhNC5I6Lm2DVILh/wziAYp3Bxik1LMx/pOUd52/x0e/HiY5PQaOBvvVK8cErFbCx1KldmigECvwjsFKlShEYGMhbb73F+++/z5gxY564r4uLC+XLl+fixYtP3Mfa2honJ6dMixBC5JqyTeHdvRDUGRQD7J4A0xrCjXC1K8u2BuXdWTekAW/UKIGiwC87I2g1cSfhf0+yKkR+YZYB6N8MBkOmuzf/lZiYyKVLl/DykuHbhRD5iK0LtJ0KHeeDvTvcOgMzmhh7iunT1a4uW5xtLfm2fRAzewTj7mjNpVtJtPt5N9+sP0tqhl7t8oQAVA5AiYmJhIeHm3psRUREEB4ebmq0PGLECLp3727af/Lkyfzxxx9cuHCBCxcuMHPmTL799lvefPNN0z7Dhg1j+/btXLlyhT179tC2bVt0Ot0TH5EJIYSqKr4K7+6Diq+BIcM4kOKMpnDzjNqVZVuTih5sfL8Brat6Y1Bg8tZLtJ60m5PX49UuTQgs1PzwQ4cO0bhxY9P60KFDAejRowdz5swhOjo6Uw8ug8HAiBEjiIiIwMLCgjJlyvDVV1/Rv39/0z7Xrl2jc+fO3LlzB3d3d+rVq8e+fftwd3fPuwsTQojnYV8UOsyFk78bu8xHhxsfib30CdQeAFrzbT/jYmfFj52q0aKyJx+vOMnZmPu0mbybnnX86FO/FF7OtmqXKAqpfNMIOj+RcYCEEKpJiIY/Bv8zbpBPLWjzM7iVUbeuHHAnMZWRq06y5kQMAJY6Da8FFadfg9JU8HRUuTpREMhcYNkkAUgIoSpFgaPzYN0ISEs0jib98jgI7mP2c4opisK287eYuu0S+yP+6SbfuII7/RuWIbSUKxqNRsUKhTmTAJRNEoCEEPnCvauwagBc2WlcL90IXpsELgVjrLLwqDim77jE2pMxPPwmCirhTP+GZWgW4IlOK0FIPB8JQNkkAUgIkW8YDHDwF9g4GjIegLUTNP8SqnYxDrBYAFy5ncSMXZdZeugaqRkGAHzd7OhbvzTta5SQMYRElkkAyiYJQEKIfOf2RVj5Nlw7aFwv38I4p5ijh7p15aDbianM3XuVuXuvEJdsHArA1d6KHrX96F7blyL2VipXKPI7CUDZJAFICJEvGfSwZyJs/QL0aWBbBFp9b5xmowBJTstg6aFr/LLzMtfuPQDAxlJLx2Af+tYvjY+r3YufXFEg5rjxTpprqRyqWOQXEoCySQKQECJfiz0FK/pDzAnjekA7aPUd2LmqW1cOy9AbWHsyhmk7LnHyunGSaq0GWgZ60b9BGQJLOGf9ZHcvw/GlcHwx3L1kbFj+1hYoVjGXqhdqkACUTRKAhBD5XkYa7PwWdnwLih4cPIyPxCq0ULuyHKcoCnsv3WHqjsvsOH/LtL1OGTf6NShNw/Luj+85lnwXTi2HY4vh2oFH33crB/22grV0wS8oJABlkwQgIYTZuH4EVrwNt88Z16u+Cc2/AJvnuDtiRk7fSOCXnZf549gNMgzGry9/T0f6NShNWJA3loY0OL/OeKfnwkYw/D2tiEZr7EVXpSOUrAWzWhgno638Orw+s8A0KC/sJABlkwQgIYRZSU+BrZ/BnkmAAk4loM1k4xd+AXU97gGzdkWw6EAkyWnphGjO0dV2L820+7DOSPxnR88qxtBT+XVw+teckJH7YU5L4/QjLb+FkLfy/iJEjpMAlE0SgIQQZunqXmNPsXtXjOs134KXx4KVvapl5ZqbZ0k5spD0o4twTI0xbY7GjSverSjftA9upas++fg9k2DDx6C1hD7roXiN3K9Z5CoJQNkkAUgIYbZSE2HTaDg4w7juWhraTIWSoerWlVPuxxjnTDu+GKKPmTYr1o5cdn+ZSXeqs/KeHwparHRa2lYrzlsNSlO2mMOj51IUWPwmnP0TnEtC/+0FriF5YSMBKJskAAkhzN6lLbBqICRcBzRQdzA0+j+wtFG7sueXmghn/4Lji+DyNlCMgyWitYByr0CVDlC+OVjaYjAobDoTy/Qdlzl09Z7pFE0revB2w9IE+/0n4KTEGyeevRcB5ZpB50VmP91IYSYBKJskAAkhCoQHccb5xI4tMK67V4S2U8C7mqplZYk+AyK2GXtwnf0T0pP/ea9EiDH0BLQDe7cnnuLw1btM236ZjWdiTVNtVC/pQv+GZXi5ogfah1NtRB+HGU1BnwpNRkP9obl3XSJXSQDKJglAQogC5exf8Md7kHTLeNek/jAIaAt2bsbBFHUWaldopCjGx1rHl8DJZZAY+897rqWNjZmrdDC+fg6XbiUyY+dlfj98nTT9P1NtdAj24Y0aJfBwsoHDv8Ifg429xbqvhlL1c/LKRB6RAJRNEoCEEAVO0h346304verR92xcwL6oMRDZuRnbwZheP2axcc7ZbuNxkXBiqfFuz8Pu/AC2rsbeW1U6QongbH/mzfsp/LrnCvP2XiUhJQMwDqzYuEIxOgSX4OXzY9EeXwj2xeDtXQVqmpHCQgJQNkkAEkIUSIpibEC86wdj26AH9559zONoLYzh5L+B6WkhytIuc4B5EGcMY8cXw9Xd/2y3sDEO5lilE5RtAjrLbF3y4ySnZbDmRAyLD0Zy8Mo/P4Pi9rDCaiTFHlwCv/rQbWX+uTsmskQCUDZJABJCFAr6DEiJg+Q7xiXp9j+vk+/+6/W/lrTEZ572sSxswK6oMRhZOcD1w8Y2NwBowK+e8U5PpdfydBDHS7cSWXIoit8PX+N2YhqlNTdYbfUJDpoUzpZ9i5IdvsTOSkKQuZAAlE0SgIQQ4gnSU+DBf8PR3ceEqIfbbhsnbn0c94oQ1BEC24Nziby9jv9I1xvYcvYmSw5GYXthNZMsJwLwrjIc56BX6VjTh6ASzo+fckPkGxKAskkCkBBC5BBFgbSkzIHpwT3jJKQelfPlFBQx8SncXDyYKjcWE6fY82raF1xT3PH3dKRDsA9tqxWniL2V2mWKx5AAlE0SgIQQopDLSEWZ1QLNjcNctfGnVeLHJGboALDSaXk5wIOOwT7UK1v0n+70QnUSgLJJApAQQgjiImFqfUiJI7V6H5a4D2bxoShOXk8w7VLcxZb2wSVoH+xDcRdbFYsVIAEo2yQACSGEAOD8BljQ3vj69ZkQ+AYnr8ez5FAUK49eN3Wn12igfjl3Ogb70LRSMawtdCoWXXhJAMomCUBCCCFMNo+Dnd+BpT302wbu5QFISdez7mQMiw9GsffyHdPuRewsaVutBB1r+lDB01GlogsnCUDZJAFICCGEiT4D5rWBKzuNPdfe2gxW9pl2uXoniaWHrrH0cBSxCamm7VV9XOhY04ewIG8crKU7fW6TAJRNEoCEEEJkcj8WptU3Ts9RpRO0nfrYHmwZegM7Ltxi8cEoNp+5SYbB+BVra6nj1SpedKzpQw3fItKdPpdIAMomCUBCCCEecWUX/BpmnI0+7Eeo0fOpu9+6n8ryI9dYfCiKy7eSTNvLuNvTqWZJOoeWlLtCOUwCUDZJABJCCPFYu36ATWNAZw19NoB31WceoigKh6/eY/HBKP48Hs2DdD0AzraW9K5bip51/XC2zfkpPwojCUDZJAFICCHEYxkMsKgznF8HRfyg33awdcny4fdT0vnjWDQzdl7m8m3jXSFHawt61PGjd71SuMoAi9kiASibJAAJIYR4ouS7ML2hcZwg/1eh42/PPaK13qCw5kQ0k7Zc5FzsfQDsrHS8WcuXvvVLUczRJjcqN47MfWY1XNgI7v7gVxc8q4C2YHTblwCUTRKAhBBCPNX1IzCrmXGes1c+gzqDXug0BoPChtOx/LTlAqduGAdYtLbQ0jmkJP0blsbLOQcHV4w+DuuGw9XdmbdbO0HJ2sYw5FsPvIJAZ55tkyQAZZMEICGEEM90cAb89QFodNDzL/Ct/cKnUhSFbeduMXHLBY5GxgHGKTfeCC7BOw3L4ONq9+J1Jt6CLZ/CkbmAAhY2ULUrxEdB5D5ITci8v5UDlKwFvnXBrx54VwOdebRRep7vb20e1fRYO3bsICwsDG9vbzQaDStXrnzq/rt27aJu3bq4ublha2uLv78/P/zwwyP7TZ48GT8/P2xsbAgNDeXAgQO5dAVCCCEKreA+UPkNUPSwrJcxaLwgjUZDY/9iLH+nDr/1CSWklCtpegML9kfS+NttfLj0GBG3k559on/LSIM9P8FP1eHIr4AClV+HgYfg1e+h61L46IpxcMdXPofyLcDGGdIS4eIm2DwWZr4MX5aEua1hxzdwdS9kpD7jg82DqneA1q5dy+7du6lRowbt2rVjxYoVtGnT5on7Hz16lLNnz1KlShXs7e3ZtWsX/fv354cffqBfv34ALF68mO7duzN16lRCQ0OZMGECS5cu5dy5cxQrVixLdckdICGEEFmSmgi/vAS3z0HpRvDm8hxrT7P/8h0mbb3Izgu3AdBqICzIm4GNy1LO4ykjTCsKnF8P6/8P7l4ybvMKguZfPfsulUEPsaeMj8mu7DL+98G9zPtY2ECJmuBX3/jYrHgwWOZSm6XnZJaPwDQazTMD0OO0a9cOe3t75s2bB0BoaCg1a9Zk0qRJABgMBnx8fBg0aBDDhw/P0jklAAkhhMiym2fhl8aQngwNP4LG/5ejpz8aeY9JWy6y+exNwNjeunmAJwNfKkuAt/OjtawfAZe2GNfti0GTUcZHXtoXeOhjMMCtM3BlN1zdZfxv8u3M++isoUTwP4/MStQEq2w8ssuGQhOAjh49SosWLfjss8/o27cvaWlp2NnZsWzZskzn6dGjB3Fxcaxateqx50lNTSU19Z9begkJCfj4+EgAEkIIkTXHFsOKfoAG3lwGZZvm+EecvB7PpC0XWXcqxrStacViDHypHFXdDLDtS2O7JEUPOiuo9S7U/wBscvB7TFHg1rl/wtDV3cbRsf9NawnFa/zdqLou+ISCtUPO1fAUzxOAzLKZd4kSJbh16xYZGRmMGTOGvn37AnD79m30ej0eHh6Z9vfw8ODs2bNPPN/48eMZO3ZsrtYshBCiAAvqCJF74fBs+P0teHsnOJfI0Y+oXNyZqd1qcC7mPpO3XuTP4zfYeiYar/O/Ucb6dxwNxu70+L8Kr3wKrqVz9PMB4+2nYv7GpWZfYyC6c8k4T9rV3cZQdP8GRO0zLju/A62FsSH1wztEPqE5G8pekFkGoJ07d5KYmMi+ffsYPnw4ZcuWpXPnzi98vhEjRjB06FDT+sM7QEIIIUSWNf8SbhyB6GOwtJexZ5hFzg9sWMHTkYmdqzGiQjSsH4VXagQY4KzBh6VF3+Wl4A7UKeJGnsw2ptFA0bLGJbiXMRDdizC2H3p4hyg+Cq4dNC67J4BGC15VoVJrqDckL6p8LLMMQKVKlQIgMDCQ2NhYxowZQ+fOnSlatCg6nY7Y2My342JjY/H09Hzi+aytrbG2ts7VmoUQQhRwljbQ/leY1hCuHYBNo6H5+Jz/nDuXYMMneJ1bA4Depgh/uPVm+JVqpNzQMnPGfqqXdGFQk3I0Ku+etxOvajTGO0+upaF6d+O2e1f/aVR9ZRfEXTUGxaLl866uxzDLAPRvBoPB1H7HysqKGjVqsHnzZlMbIIPBwObNmxk4cKCKVQohhCgUXEtB2ymwqAvs+9k4nk6l1jlz7pQEY1f0fVPAkG4cfyikH7pGH9HGtgghcQ+Ytv0SCw9GcSQyjl6zDxJY3JmBL5Xl5YoeaLUqzUBfxNe4VO1iXI+/Zrw75FJSnXr+pmoASkxM5OLFi6b1iIgIwsPDcXV1pWTJkowYMYLr168zd+5cwDi+T8mSJfH39weM4wh9++23DB482HSOoUOH0qNHD4KDgwkJCWHChAkkJSXRq1evvL04IYQQhZN/K6gzGPZMhFUDwaMyuJV58fMZ9BA+HzaPg6S/xxoq08R4d8m9gmk3bxdbxrauzIDGZfll52V+2xfJievx9J93GH9PRwa+VJYWlb3QqRWEHnIuYWwzpTJVe4Ft27aNxo0bP7K9R48ezJkzh549e3LlyhW2bdsGwE8//cS0adOIiIjAwsKCMmXK8NZbb9G/f3+0/+reN2nSJL755htiYmKoWrUqEydOJDQ0NMt1STd4IYQQ2aJPh1/DjA2jPSpD301g+QLTWlzdA2s/gpjjxnW3stDsCyj3yjPnH7uTmMrMXRHM3XuVxNQMAMq42zOgcVleC/LGQqfqWMi5wiy7wecnEoCEEEJkW0I0TKtvvGtTrRu0npT1Y+MiYeMoOLXCuG7tDI0+gppvPXfD6vjkdGbviWDWrggSUoxBqKSrHW2rFadVFS/KP21QRTMjASibJAAJIYTIEZe3wdw2gAKtf4ZqXZ++f1oS7JpgfHyWkWLsMVW9B7z0CdgXzVYp91PSmbfvKjN2RnA3Kc20vWwxB1oGetEq0IvyHg5522g6h0kAyiYJQEIIIXLM9q9h6+dgYWt8FOZZ+dF9FAVOLIWNo43j6IBxqonm48EzMEfLSU7LYM2JGNaciGbnhVuk6/+JAWXc7WkV6EXLKl5U8HA0uzAkASibJAAJIYTIMQYDzH8DLm0G1zLGyUf/PRDgtcOw7iPjODlg7B31yudQMeyZ7XyyK/5BOpvPxLLmRDQ7zt8mTW8wvVf6YRgK9MLf0zzCkASgbJIAJIQQIkcl3TG2B0q4DpXaQPs5cD/GOOP6sYXGfSztocEHUGuAKpOLJqQYw9Bfx2PYcf5W5jBU1J4WgZ60DPSikpdTvg1DEoCySQKQEEKIHBd1EGY3B0OG8e7OxS2QnmR8L6iLcdJSJy91a/zb/ZR0Np+5yV8notl+/hZpGf+EIT83O1r+fWcowDt/hSEJQNkkAUgIIUSu2DcF1g3/Z71EiHEKjRI11KvpGe6npLPl7E3WnIhm27lbpP4nDLX4uwF1fghDEoCySQKQEEKIXKEosGaYcSTk+h9A4Bu53s4nJyWmZhjD0PFotp67mSkMlXS1M/Umq1xcnTAkASibJAAJIYQQT5f0MAydMIahlPR/wpCPq60pDAUWd86zMCQBKJskAAkhhBBZl5SawdZzxjC05WzmMFSiiK2pzVBQidwNQxKAskkCkBBCCPFiktMy2Hr2FmtORrPlzE0epOtN7xV3saXl373Jqvq45HgYkgCUTRKAhBBCiOx7kKZn2zljb7ItZ2+SnPZPGKpb1o35fWvl6Oc9z/e3qrPBCyGEEKLgsrXS0SLQixaBXjxI07P9/E3+OhHD5jOxVPMpomptEoCEEEIIketsrXQ0r+xF88pepKTrM/UgU4MEICGEEELkKRtLHTaWOlVr0Kr66UIIIYQQKpAAJIQQQohCRwKQEEIIIQodCUBCCCGEKHQkAAkhhBCi0JEAJIQQQohCRwKQEEIIIQodCUBCCCGEKHQkAAkhhBCi0JEAJIQQQohCRwKQEEIIIQodCUBCCCGEKHQkAAkhhBCi0JHZ4B9DURQAEhISVK5ECCGEEFn18Hv74ff400gAeoz79+8D4OPjo3IlQgghhHhe9+/fx9nZ+an7aJSsxKRCxmAwcOPGDRwdHdFoNDl67oSEBHx8fIiKisLJySlHz20OCvv1g/wM5PoL9/WD/AwK+/VD7v0MFEXh/v37eHt7o9U+vZWP3AF6DK1WS4kSJXL1M5ycnArtX3yQ6wf5Gcj1F+7rB/kZFPbrh9z5GTzrzs9D0ghaCCGEEIWOBCAhhBBCFDoSgPKYtbU1o0ePxtraWu1SVFHYrx/kZyDXX7ivH+RnUNivH/LHz0AaQQshhBCi0JE7QEIIIYQodCQACSGEEKLQkQAkhBBCiEJHApAQQgghCh0JQHlo8uTJ+Pn5YWNjQ2hoKAcOHFC7pDwzfvx4atasiaOjI8WKFaNNmzacO3dO7bJU8+WXX6LRaBgyZIjapeSp69ev8+abb+Lm5oatrS2BgYEcOnRI7bLyhF6vZ+TIkZQqVQpbW1vKlCnDp59+mqU5i8zVjh07CAsLw9vbG41Gw8qVKzO9rygKo0aNwsvLC1tbW5o2bcqFCxfUKTYXPO3609PT+eijjwgMDMTe3h5vb2+6d+/OjRs31Cs4hz3rz//f3n77bTQaDRMmTMiz+iQA5ZHFixczdOhQRo8ezZEjRwgKCqJZs2bcvHlT7dLyxPbt2xkwYAD79u1j48aNpKen88orr5CUlKR2aXnu4MGDTJs2jSpVqqhdSp66d+8edevWxdLSkrVr13L69Gm+++47ihQponZpeeKrr75iypQpTJo0iTNnzvDVV1/x9ddf89NPP6ldWq5JSkoiKCiIyZMnP/b9r7/+mokTJzJ16lT279+Pvb09zZo1IyUlJY8rzR1Pu/7k5GSOHDnCyJEjOXLkCMuXL+fcuXO89tprKlSaO5715//QihUr2LdvH97e3nlU2d8UkSdCQkKUAQMGmNb1er3i7e2tjB8/XsWq1HPz5k0FULZv3652KXnq/v37Srly5ZSNGzcqDRs2VN577z21S8ozH330kVKvXj21y1BNq1atlN69e2fa1q5dO6Vr164qVZS3AGXFihWmdYPBoHh6eirffPONaVtcXJxibW2tLFy4UIUKc9d/r/9xDhw4oADK1atX86aoPPSk67927ZpSvHhx5eTJk4qvr6/yww8/5FlNcgcoD6SlpXH48GGaNm1q2qbVamnatCl79+5VsTL1xMfHA+Dq6qpyJXlrwIABtGrVKtPfhcJi9erVBAcH0759e4oVK0a1atX45Zdf1C4rz9SpU4fNmzdz/vx5AI4dO8auXbto0aKFypWpIyIigpiYmEz/Lzg7OxMaGlqofy9qNBpcXFzULiVPGAwGunXrxocffkhAQECef75MhpoHbt++jV6vx8PDI9N2Dw8Pzp49q1JV6jEYDAwZMoS6detSuXJltcvJM4sWLeLIkSMcPHhQ7VJUcfnyZaZMmcLQoUP5v//7Pw4ePMjgwYOxsrKiR48eapeX64YPH05CQgL+/v7odDr0ej2ff/45Xbt2Vbs0VcTExAA89vfiw/cKk5SUFD766CM6d+5caCZI/eqrr7CwsGDw4MGqfL4EIJHnBgwYwMmTJ9m1a5fapeSZqKgo3nvvPTZu3IiNjY3a5ajCYDAQHBzMF198AUC1atU4efIkU6dOLRQBaMmSJcyfP58FCxYQEBBAeHg4Q4YMwdvbu1Bcv3iy9PR0OnTogKIoTJkyRe1y8sThw4f58ccfOXLkCBqNRpUa5BFYHihatCg6nY7Y2NhM22NjY/H09FSpKnUMHDiQP//8k61bt1KiRAm1y8kzhw8f5ubNm1SvXh0LCwssLCzYvn07EydOxMLCAr1er3aJuc7Ly4tKlSpl2laxYkUiIyNVqihvffjhhwwfPpxOnToRGBhIt27deP/99xk/frzapani4e++wv578WH4uXr1Khs3biw0d3927tzJzZs3KVmypOl34tWrV/nggw/w8/PLkxokAOUBKysratSowebNm03bDAYDmzdvpnbt2ipWlncURWHgwIGsWLGCLVu2UKpUKbVLylNNmjThxIkThIeHm5bg4GC6du1KeHg4Op1O7RJzXd26dR8Z+uD8+fP4+vqqVFHeSk5ORqvN/CtXp9NhMBhUqkhdpUqVwtPTM9PvxYSEBPbv319ofi8+DD8XLlxg06ZNuLm5qV1SnunWrRvHjx/P9DvR29ubDz/8kPXr1+dJDfIILI8MHTqUHj16EBwcTEhICBMmTCApKYlevXqpXVqeGDBgAAsWLGDVqlU4OjqanvE7Oztja2urcnW5z9HR8ZH2Tvb29ri5uRWadlDvv/8+derU4YsvvqBDhw4cOHCA6dOnM336dLVLyxNhYWF8/vnnlCxZkoCAAI4ePcr3339P79691S4t1yQmJnLx4kXTekREBOHh4bi6ulKyZEmGDBnCZ599Rrly5ShVqhQjR47E29ubNm3aqFd0Dnra9Xt5efHGG29w5MgR/vzzT/R6ven3oqurK1ZWVmqVnWOe9ef/38BnaWmJp6cnFSpUyJsC86y/mVB++uknpWTJkoqVlZUSEhKi7Nu3T+2S8gzw2GX27Nlql6aawtYNXlEU5Y8//lAqV66sWFtbK/7+/sr06dPVLinPJCQkKO+9955SsmRJxcbGRildurTy8ccfK6mpqWqXlmu2bt362P/ve/TooSiKsSv8yJEjFQ8PD8Xa2lpp0qSJcu7cOXWLzkFPu/6IiIgn/l7cunWr2qXniGf9+f9XXneD1yhKAR6GVAghhBDiMaQNkBBCCCEKHQlAQgghhCh0JAAJIYQQotCRACSEEEKIQkcCkBBCCCEKHQlAQgghhCh0JAAJIYQQotCRACSEEE+g0WhYuXKl2mUIIXKBBCAhRL7Us2dPNBrNI0vz5s3VLk0IUQDIXGBCiHyrefPmzJ49O9M2a2trlaoRQhQkcgdICJFvWVtb4+npmWkpUqQIYHw8NWXKFFq0aIGtrS2lS5dm2bJlmY4/ceLE/7d39yDJtXEYwC/tC5UCSwubGhIxoYaKsI+hhMIgMIwIJKRFNJOWlujLhraoNkGoqUhwCKSyqEYhCiILsrZaQipqSCEX72d4QZB4X3qeensMrx8I577/5+N/nC7OuUV0dXVBJpOhoqICDocDiUQia5+1tTUYDAaUlJRAo9FgbGwsq/709IT+/n7I5XJotVqEQqFM7eXlBTabDWq1GjKZDFqt9l1gI6LcxABERD/WzMwMrFYrotEobDYbhoaGEIvFAADJZBI9PT1QKpU4PT1FMBjE4eFhVsDx+Xxwu91wOBy4vLxEKBRCbW1t1jXm5+cxODiIi4sL9Pb2wmaz4fn5OXP9q6srhMNhxGIx+Hw+qFSq7/sCiOjPfdvfrhIR/Qa73S4KCgqEQqHI+iwsLAghhAAgnE5n1jEtLS3C5XIJIYTw+/1CqVSKRCKRqe/s7AipVCri8bgQQojq6moxNTX1rz0AENPT05lxIpEQAEQ4HBZCCNHX1ydGRka+5oaJ6FtxDRAR5azOzk74fL6sufLy8sy20WjMqhmNRpyfnwMAYrEYGhoaoFAoMvW2tjak02nc3NxAIpHg/v4eJpPpP3uor6/PbCsUCpSVleHh4QEA4HK5YLVacXZ2hu7ublgsFrS2tv7RvRLR92IAIqKcpVAo3r2S+ioymexD+xUVFWWNJRIJ0uk0AMBsNuPu7g67u7s4ODiAyWSC2+3G4uLil/dLRF+La4CI6Mc6Pj5+N9br9QAAvV6PaDSKZDKZqUciEUilUuh0OpSWlqKmpgZHR0ef6kGtVsNut2N9fR0rKyvw+/2fOh8RfQ8+ASKinJVKpRCPx7PmCgsLMwuNg8Egmpqa0N7ejo2NDZycnGB1dRUAYLPZMDc3B7vdDq/Xi8fHR3g8HgwPD6OqqgoA4PV64XQ6UVlZCbPZjNfXV0QiEXg8ng/1Nzs7i8bGRhgMBqRSKWxvb2cCGBHlNgYgIspZe3t70Gg0WXM6nQ7X19cA/vmFViAQwOjoKDQaDTY3N1FXVwcAkMvl2N/fx/j4OJqbmyGXy2G1WrG0tJQ5l91ux9vbG5aXlzExMQGVSoWBgYEP91dcXIzJyUnc3t5CJpOho6MDgUDgC+6ciP5vEiGE+NtNEBH9LolEgq2tLVgslr/dChH9QFwDRERERHmHAYiIiIjyDtcAEdGPxLf3RPQZfAJEREREeYcBiIiIiPIOAxARERHlHQYgIiIiyjsMQERERJR3GICIiIgo7zAAERERUd5hACIiIqK8wwBEREREeecXM6Yt5cyXtX0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 **STEP 5 Conclusion**:  Final Training Analysis & Model Performance"
      ],
      "metadata": {
        "id": "WZV0NyDdqvHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ❓ **What We Did?**  \n",
        "\n",
        "1️⃣ **Set Callbacks** (`EarlyStopping` & `ReduceLROnPlateau`) to optimize training.  \n",
        "2️⃣ **Trained the CNN** using `train_generator` (images) and validated with `val_generator`.  \n",
        "3️⃣ **Evaluated model performance** on the validation set.  \n",
        "4️⃣ **Plotted accuracy and loss curves** to visualize training progress."
      ],
      "metadata": {
        "id": "1A2tE8N5tEWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **🔹 Concerns**\n",
        "⚠️ **Validation Accuracy (~51%) is still low.**  \n",
        "- This suggests the model **may not be complex enough** to fully capture the patterns in facial emotions.  \n",
        "- The dataset is likely **challenging**, and additional improvements might be needed.  "
      ],
      "metadata": {
        "id": "_I-pgHhYRl3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "unQr-DJUKfCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "oJ6Zj40anKyQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmFIb52bnNYI",
        "outputId": "acf868d0-f4a9-419c-e36f-574986370ea0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atuP59sQnSTW",
        "outputId": "342630bb-7743-454a-fdb6-601b2d4bae69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an improved CNN architecture\n",
        "model = Sequential([\n",
        "    Input(shape=(48,48,3)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "zzEQDi25nWj_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jIJ7eyI-nbvm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.h5\", save_best_only=True, monitor='val_loss', verbose=1)"
      ],
      "metadata": {
        "id": "vY9So-sAtgkC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 30\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RhL_QunmFee",
        "outputId": "18ad7cc2-7b86-460d-9076-0c78ff48a2d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2443 - loss: 2.8862\n",
            "Epoch 1: val_loss improved from inf to 2.49690, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.2443 - loss: 2.8858 - val_accuracy: 0.2674 - val_loss: 2.4969 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2782 - loss: 2.3884\n",
            "Epoch 2: val_loss improved from 2.49690 to 2.35978, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.2782 - loss: 2.3883 - val_accuracy: 0.2540 - val_loss: 2.3598 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3135 - loss: 2.2070\n",
            "Epoch 3: val_loss improved from 2.35978 to 2.35263, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3135 - loss: 2.2070 - val_accuracy: 0.2879 - val_loss: 2.3526 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3405 - loss: 2.2205\n",
            "Epoch 4: val_loss did not improve from 2.35263\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.3406 - loss: 2.2205 - val_accuracy: 0.1803 - val_loss: 2.6458 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3645 - loss: 2.1984\n",
            "Epoch 5: val_loss improved from 2.35263 to 2.30495, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3645 - loss: 2.1985 - val_accuracy: 0.3729 - val_loss: 2.3050 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3894 - loss: 2.2087\n",
            "Epoch 6: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3894 - loss: 2.2087 - val_accuracy: 0.2930 - val_loss: 2.3339 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4012 - loss: 2.1774\n",
            "Epoch 7: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4011 - loss: 2.1776 - val_accuracy: 0.2984 - val_loss: 2.5419 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3854 - loss: 2.3788\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.30495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.3854 - loss: 2.3787 - val_accuracy: 0.3937 - val_loss: 2.3375 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4232 - loss: 2.2200\n",
            "Epoch 9: val_loss improved from 2.30495 to 2.06239, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4232 - loss: 2.2197 - val_accuracy: 0.4233 - val_loss: 2.0624 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4385 - loss: 2.0071\n",
            "Epoch 10: val_loss improved from 2.06239 to 1.89361, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4385 - loss: 2.0071 - val_accuracy: 0.4734 - val_loss: 1.8936 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4481 - loss: 1.9351\n",
            "Epoch 11: val_loss did not improve from 1.89361\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4481 - loss: 1.9351 - val_accuracy: 0.3146 - val_loss: 2.3201 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4584 - loss: 1.9135\n",
            "Epoch 12: val_loss improved from 1.89361 to 1.85447, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4584 - loss: 1.9135 - val_accuracy: 0.4809 - val_loss: 1.8545 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4670 - loss: 1.8943\n",
            "Epoch 13: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4670 - loss: 1.8943 - val_accuracy: 0.4377 - val_loss: 1.9615 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4602 - loss: 1.9006\n",
            "Epoch 14: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4602 - loss: 1.9007 - val_accuracy: 0.4234 - val_loss: 2.0217 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4637 - loss: 1.9112\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 15: val_loss did not improve from 1.85447\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4638 - loss: 1.9112 - val_accuracy: 0.4464 - val_loss: 1.9170 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4782 - loss: 1.8752\n",
            "Epoch 16: val_loss improved from 1.85447 to 1.73257, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4783 - loss: 1.8750 - val_accuracy: 0.5030 - val_loss: 1.7326 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4941 - loss: 1.7529\n",
            "Epoch 17: val_loss improved from 1.73257 to 1.73059, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4941 - loss: 1.7529 - val_accuracy: 0.4931 - val_loss: 1.7306 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5018 - loss: 1.7106\n",
            "Epoch 18: val_loss improved from 1.73059 to 1.63947, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5018 - loss: 1.7106 - val_accuracy: 0.5206 - val_loss: 1.6395 - learning_rate: 2.5000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5033 - loss: 1.6762\n",
            "Epoch 19: val_loss improved from 1.63947 to 1.61138, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5033 - loss: 1.6763 - val_accuracy: 0.5231 - val_loss: 1.6114 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4997 - loss: 1.6850\n",
            "Epoch 20: val_loss did not improve from 1.61138\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.4997 - loss: 1.6850 - val_accuracy: 0.4994 - val_loss: 1.6687 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4964 - loss: 1.6693\n",
            "Epoch 21: val_loss did not improve from 1.61138\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.4964 - loss: 1.6692 - val_accuracy: 0.5037 - val_loss: 1.6687 - learning_rate: 2.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5047 - loss: 1.6634\n",
            "Epoch 22: val_loss improved from 1.61138 to 1.59607, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5048 - loss: 1.6633 - val_accuracy: 0.5187 - val_loss: 1.5961 - learning_rate: 2.5000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5022 - loss: 1.6572\n",
            "Epoch 23: val_loss did not improve from 1.59607\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.5022 - loss: 1.6572 - val_accuracy: 0.5271 - val_loss: 1.5974 - learning_rate: 2.5000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5136 - loss: 1.6297\n",
            "Epoch 24: val_loss improved from 1.59607 to 1.58312, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5136 - loss: 1.6297 - val_accuracy: 0.5327 - val_loss: 1.5831 - learning_rate: 2.5000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5188 - loss: 1.6335\n",
            "Epoch 25: val_loss improved from 1.58312 to 1.56269, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5188 - loss: 1.6335 - val_accuracy: 0.5419 - val_loss: 1.5627 - learning_rate: 2.5000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5135 - loss: 1.6195\n",
            "Epoch 26: val_loss did not improve from 1.56269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5135 - loss: 1.6195 - val_accuracy: 0.5236 - val_loss: 1.5855 - learning_rate: 2.5000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5165 - loss: 1.6107\n",
            "Epoch 27: val_loss improved from 1.56269 to 1.56126, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.5165 - loss: 1.6107 - val_accuracy: 0.5360 - val_loss: 1.5613 - learning_rate: 2.5000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5192 - loss: 1.6049\n",
            "Epoch 28: val_loss did not improve from 1.56126\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5192 - loss: 1.6049 - val_accuracy: 0.5325 - val_loss: 1.5751 - learning_rate: 2.5000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5152 - loss: 1.6079\n",
            "Epoch 29: val_loss improved from 1.56126 to 1.53240, saving model to best_emotion_cnn.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.5152 - loss: 1.6079 - val_accuracy: 0.5464 - val_loss: 1.5324 - learning_rate: 2.5000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5195 - loss: 1.6065\n",
            "Epoch 30: val_loss did not improve from 1.53240\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.5195 - loss: 1.6065 - val_accuracy: 0.5414 - val_loss: 1.5574 - learning_rate: 2.5000e-04\n",
            "Restoring model weights from the end of the best epoch: 29.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne4gMCpYmNcu",
        "outputId": "04f64b6d-3655-401c-9514-b199d46aa870"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5866 - loss: 1.4048\n",
            "Test Accuracy: 0.5766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "_HHxCM4y0hJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Load Pretrained Model (VGG16) for Transfer Learning\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "for layer in base_model.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define an improved CNN architecture with Transfer Learning\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = RMSprop(learning_rate=0.0005)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 30\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjJ9yUrzYUr",
        "outputId": "edf2ed57-2e92-46a9-d33e-6cd05e3f6a02"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.1880 - loss: 2.5962\n",
            "Epoch 1: val_loss improved from inf to 4.55887, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - accuracy: 0.1881 - loss: 2.5956 - val_accuracy: 0.2625 - val_loss: 4.5589 - learning_rate: 5.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2417 - loss: 2.0313\n",
            "Epoch 2: val_loss improved from 4.55887 to 3.23369, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.2418 - loss: 2.0312 - val_accuracy: 0.2714 - val_loss: 3.2337 - learning_rate: 5.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2737 - loss: 1.8596\n",
            "Epoch 3: val_loss did not improve from 3.23369\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.2737 - loss: 1.8595 - val_accuracy: 0.2534 - val_loss: 5.5233 - learning_rate: 5.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2928 - loss: 1.7926\n",
            "Epoch 4: val_loss did not improve from 3.23369\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.2928 - loss: 1.7926 - val_accuracy: 0.1686 - val_loss: 3.5972 - learning_rate: 5.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2962 - loss: 1.7657\n",
            "Epoch 5: val_loss improved from 3.23369 to 2.94297, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.2963 - loss: 1.7657 - val_accuracy: 0.2500 - val_loss: 2.9430 - learning_rate: 5.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3157 - loss: 1.7407\n",
            "Epoch 6: val_loss improved from 2.94297 to 2.10933, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3157 - loss: 1.7407 - val_accuracy: 0.3130 - val_loss: 2.1093 - learning_rate: 5.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3297 - loss: 1.7301\n",
            "Epoch 7: val_loss improved from 2.10933 to 1.95145, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3297 - loss: 1.7301 - val_accuracy: 0.2895 - val_loss: 1.9514 - learning_rate: 5.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3271 - loss: 1.7042\n",
            "Epoch 8: val_loss improved from 1.95145 to 1.88526, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.3271 - loss: 1.7042 - val_accuracy: 0.2602 - val_loss: 1.8853 - learning_rate: 5.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3384 - loss: 1.7068\n",
            "Epoch 9: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3384 - loss: 1.7068 - val_accuracy: 0.2583 - val_loss: 2.0362 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3449 - loss: 1.6622\n",
            "Epoch 10: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3449 - loss: 1.6622 - val_accuracy: 0.2576 - val_loss: 2.1245 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.3495 - loss: 1.6443\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.88526\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3495 - loss: 1.6443 - val_accuracy: 0.2919 - val_loss: 2.1796 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3542 - loss: 1.6372\n",
            "Epoch 12: val_loss improved from 1.88526 to 1.68430, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3543 - loss: 1.6372 - val_accuracy: 0.3560 - val_loss: 1.6843 - learning_rate: 2.5000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3768 - loss: 1.6153\n",
            "Epoch 13: val_loss did not improve from 1.68430\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3768 - loss: 1.6153 - val_accuracy: 0.2533 - val_loss: 7.8442 - learning_rate: 2.5000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3800 - loss: 1.6149\n",
            "Epoch 14: val_loss improved from 1.68430 to 1.66495, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.3799 - loss: 1.6148 - val_accuracy: 0.3682 - val_loss: 1.6649 - learning_rate: 2.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3719 - loss: 1.5913\n",
            "Epoch 15: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3719 - loss: 1.5913 - val_accuracy: 0.3202 - val_loss: 1.7170 - learning_rate: 2.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3840 - loss: 1.6009\n",
            "Epoch 16: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3840 - loss: 1.6010 - val_accuracy: 0.0324 - val_loss: 8.3529 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3904 - loss: 1.5787\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 17: val_loss did not improve from 1.66495\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3904 - loss: 1.5788 - val_accuracy: 0.3276 - val_loss: 1.7860 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3825 - loss: 1.5814\n",
            "Epoch 18: val_loss improved from 1.66495 to 1.62765, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.3825 - loss: 1.5814 - val_accuracy: 0.3621 - val_loss: 1.6277 - learning_rate: 1.2500e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4043 - loss: 1.5543\n",
            "Epoch 19: val_loss improved from 1.62765 to 1.56853, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4043 - loss: 1.5543 - val_accuracy: 0.4013 - val_loss: 1.5685 - learning_rate: 1.2500e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3993 - loss: 1.5666\n",
            "Epoch 20: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3993 - loss: 1.5666 - val_accuracy: 0.3654 - val_loss: 1.6400 - learning_rate: 1.2500e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3986 - loss: 1.5543\n",
            "Epoch 21: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3986 - loss: 1.5543 - val_accuracy: 0.3815 - val_loss: 1.6100 - learning_rate: 1.2500e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3987 - loss: 1.5577\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 22: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.3987 - loss: 1.5577 - val_accuracy: 0.3982 - val_loss: 1.5697 - learning_rate: 1.2500e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4067 - loss: 1.5505\n",
            "Epoch 23: val_loss did not improve from 1.56853\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4067 - loss: 1.5505 - val_accuracy: 0.3897 - val_loss: 1.5873 - learning_rate: 6.2500e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4133 - loss: 1.5197\n",
            "Epoch 24: val_loss improved from 1.56853 to 1.54767, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.4133 - loss: 1.5197 - val_accuracy: 0.4067 - val_loss: 1.5477 - learning_rate: 6.2500e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4021 - loss: 1.5088\n",
            "Epoch 25: val_loss did not improve from 1.54767\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4021 - loss: 1.5089 - val_accuracy: 0.3820 - val_loss: 1.5823 - learning_rate: 6.2500e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4067 - loss: 1.5192\n",
            "Epoch 26: val_loss improved from 1.54767 to 1.52826, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4067 - loss: 1.5193 - val_accuracy: 0.4036 - val_loss: 1.5283 - learning_rate: 6.2500e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4200 - loss: 1.5025\n",
            "Epoch 27: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4200 - loss: 1.5026 - val_accuracy: 0.4022 - val_loss: 1.5583 - learning_rate: 6.2500e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4096 - loss: 1.5510\n",
            "Epoch 28: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4096 - loss: 1.5510 - val_accuracy: 0.3778 - val_loss: 1.6181 - learning_rate: 6.2500e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4080 - loss: 1.5318\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.4080 - loss: 1.5318 - val_accuracy: 0.3827 - val_loss: 1.5913 - learning_rate: 6.2500e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4096 - loss: 1.5218\n",
            "Epoch 30: val_loss did not improve from 1.52826\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.4096 - loss: 1.5218 - val_accuracy: 0.3836 - val_loss: 1.6051 - learning_rate: 3.1250e-05\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4565 - loss: 1.4022\n",
            "Test Accuracy: 0.4805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "RoexJJCs5ld2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an improved CNN architecture\n",
        "model = Sequential([\n",
        "    Input(shape=(48,48,3)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((3,3), strides=(2,2)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(2048, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 50\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoMTBni35k4w",
        "outputId": "f2267db7-fbbd-42ce-b640-ea28aa9d8917"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.1577 - loss: 4.9184\n",
            "Epoch 1: val_loss improved from inf to 9.08683, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 121ms/step - accuracy: 0.1577 - loss: 4.9098 - val_accuracy: 0.0293 - val_loss: 9.0868 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1656 - loss: 1.9976\n",
            "Epoch 2: val_loss improved from 9.08683 to 2.43642, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 92ms/step - accuracy: 0.1656 - loss: 1.9976 - val_accuracy: 0.1578 - val_loss: 2.4364 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1580 - loss: 1.9758\n",
            "Epoch 3: val_loss did not improve from 2.43642\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1580 - loss: 1.9758 - val_accuracy: 0.1202 - val_loss: 2.5742 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.1552 - loss: 1.9396\n",
            "Epoch 4: val_loss did not improve from 2.43642\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - accuracy: 0.1552 - loss: 1.9396 - val_accuracy: 0.1348 - val_loss: 2.4686 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1677 - loss: 1.9522\n",
            "Epoch 5: val_loss improved from 2.43642 to 2.36297, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 92ms/step - accuracy: 0.1677 - loss: 1.9522 - val_accuracy: 0.1280 - val_loss: 2.3630 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1562 - loss: 1.9468\n",
            "Epoch 6: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1563 - loss: 1.9468 - val_accuracy: 0.1275 - val_loss: 2.4431 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1651 - loss: 1.9179\n",
            "Epoch 7: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1651 - loss: 1.9180 - val_accuracy: 0.1317 - val_loss: 2.4928 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1690 - loss: 1.9316\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1690 - loss: 1.9316 - val_accuracy: 0.1261 - val_loss: 3.3769 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1689 - loss: 1.9228\n",
            "Epoch 9: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.1689 - loss: 1.9228 - val_accuracy: 0.1458 - val_loss: 2.4736 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.1649 - loss: 1.9509\n",
            "Epoch 10: val_loss did not improve from 2.36297\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 89ms/step - accuracy: 0.1650 - loss: 1.9508 - val_accuracy: 0.1583 - val_loss: 2.4517 - learning_rate: 5.0000e-05\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.1035 - loss: 2.1257\n",
            "Test Accuracy: 0.1633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "a3JqTrjw-yAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import RandomNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator for Enhanced Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.05,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",  # Convert images to grayscale\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an improved CNN architecture based on Best_model.ipynb\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(48,48,1), bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 4th convolution layer\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', bias_initializer=RandomNormal(stddev=1), kernel_initializer=RandomNormal(stddev=1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 300\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181yWwht5nVz",
        "outputId": "ae3b160b-2e1a-4935-fba8-85f23e02f50d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1486 - loss: 3.5732\n",
            "Epoch 1: val_loss improved from inf to 1.94612, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.1486 - loss: 3.5670 - val_accuracy: 0.1982 - val_loss: 1.9461 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1540 - loss: 1.9652\n",
            "Epoch 2: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.1539 - loss: 1.9652 - val_accuracy: 0.1789 - val_loss: 2.0176 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1273 - loss: 1.9527\n",
            "Epoch 3: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.1273 - loss: 1.9527 - val_accuracy: 0.1829 - val_loss: 2.0551 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1365 - loss: 1.9380\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1365 - loss: 1.9380 - val_accuracy: 0.1531 - val_loss: 2.0683 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1226 - loss: 1.9331\n",
            "Epoch 5: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.1226 - loss: 1.9331 - val_accuracy: 0.1874 - val_loss: 2.0998 - learning_rate: 5.0000e-04\n",
            "Epoch 6/300\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1542 - loss: 1.9148\n",
            "Epoch 6: val_loss did not improve from 1.94612\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.1542 - loss: 1.9149 - val_accuracy: 0.1907 - val_loss: 2.1115 - learning_rate: 5.0000e-04\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.0865 - loss: 2.0062\n",
            "Test Accuracy: 0.1941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Adjusted Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,  # Reduced rotation\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,  # Reduced zoom\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Improved CNN Architecture with Adjustments\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution layer\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CUFKHq9AjDO",
        "outputId": "09edd8f4-3345-4062-e007-a4603dd4b2d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1703 - loss: 2.5496\n",
            "Epoch 1: val_loss improved from inf to 1.99240, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.1703 - loss: 2.5491 - val_accuracy: 0.1331 - val_loss: 1.9924 - learning_rate: 3.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1991 - loss: 2.0747\n",
            "Epoch 2: val_loss improved from 1.99240 to 1.90109, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.1991 - loss: 2.0746 - val_accuracy: 0.2088 - val_loss: 1.9011 - learning_rate: 3.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2309 - loss: 1.9361\n",
            "Epoch 3: val_loss improved from 1.90109 to 1.79929, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2309 - loss: 1.9362 - val_accuracy: 0.2898 - val_loss: 1.7993 - learning_rate: 3.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2674 - loss: 1.8865\n",
            "Epoch 4: val_loss improved from 1.79929 to 1.77704, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2674 - loss: 1.8865 - val_accuracy: 0.3055 - val_loss: 1.7770 - learning_rate: 3.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2932 - loss: 1.8026\n",
            "Epoch 5: val_loss improved from 1.77704 to 1.70129, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.2932 - loss: 1.8026 - val_accuracy: 0.3351 - val_loss: 1.7013 - learning_rate: 3.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3181 - loss: 1.7658\n",
            "Epoch 6: val_loss improved from 1.70129 to 1.60121, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.3181 - loss: 1.7657 - val_accuracy: 0.3836 - val_loss: 1.6012 - learning_rate: 3.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3499 - loss: 1.6794\n",
            "Epoch 7: val_loss improved from 1.60121 to 1.57139, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3499 - loss: 1.6794 - val_accuracy: 0.4006 - val_loss: 1.5714 - learning_rate: 3.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3820 - loss: 1.6083\n",
            "Epoch 8: val_loss improved from 1.57139 to 1.52415, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3820 - loss: 1.6083 - val_accuracy: 0.4172 - val_loss: 1.5242 - learning_rate: 3.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3941 - loss: 1.5565\n",
            "Epoch 9: val_loss improved from 1.52415 to 1.50497, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.3941 - loss: 1.5565 - val_accuracy: 0.4247 - val_loss: 1.5050 - learning_rate: 3.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4077 - loss: 1.5620\n",
            "Epoch 10: val_loss did not improve from 1.50497\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.4077 - loss: 1.5618 - val_accuracy: 0.4173 - val_loss: 1.5356 - learning_rate: 3.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4242 - loss: 1.5101\n",
            "Epoch 11: val_loss improved from 1.50497 to 1.44812, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.4242 - loss: 1.5100 - val_accuracy: 0.4438 - val_loss: 1.4481 - learning_rate: 3.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4391 - loss: 1.4574\n",
            "Epoch 12: val_loss did not improve from 1.44812\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4391 - loss: 1.4574 - val_accuracy: 0.4288 - val_loss: 1.4728 - learning_rate: 3.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4531 - loss: 1.4316\n",
            "Epoch 13: val_loss improved from 1.44812 to 1.37244, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4531 - loss: 1.4315 - val_accuracy: 0.4764 - val_loss: 1.3724 - learning_rate: 3.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4597 - loss: 1.4029\n",
            "Epoch 14: val_loss did not improve from 1.37244\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4597 - loss: 1.4029 - val_accuracy: 0.4839 - val_loss: 1.3726 - learning_rate: 3.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4792 - loss: 1.3674\n",
            "Epoch 15: val_loss improved from 1.37244 to 1.33979, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4792 - loss: 1.3674 - val_accuracy: 0.4879 - val_loss: 1.3398 - learning_rate: 3.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4858 - loss: 1.3508\n",
            "Epoch 16: val_loss did not improve from 1.33979\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.4858 - loss: 1.3508 - val_accuracy: 0.4945 - val_loss: 1.3436 - learning_rate: 3.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4898 - loss: 1.3319\n",
            "Epoch 17: val_loss improved from 1.33979 to 1.27815, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.4898 - loss: 1.3319 - val_accuracy: 0.5051 - val_loss: 1.2781 - learning_rate: 3.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4971 - loss: 1.3034\n",
            "Epoch 18: val_loss did not improve from 1.27815\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.4971 - loss: 1.3035 - val_accuracy: 0.4922 - val_loss: 1.3387 - learning_rate: 3.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5043 - loss: 1.2916\n",
            "Epoch 19: val_loss did not improve from 1.27815\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5043 - loss: 1.2916 - val_accuracy: 0.5093 - val_loss: 1.2920 - learning_rate: 3.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5065 - loss: 1.2788\n",
            "Epoch 20: val_loss improved from 1.27815 to 1.25062, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.5065 - loss: 1.2788 - val_accuracy: 0.5224 - val_loss: 1.2506 - learning_rate: 3.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5216 - loss: 1.2400\n",
            "Epoch 21: val_loss did not improve from 1.25062\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5215 - loss: 1.2402 - val_accuracy: 0.4781 - val_loss: 1.3598 - learning_rate: 3.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5210 - loss: 1.2324\n",
            "Epoch 22: val_loss did not improve from 1.25062\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5210 - loss: 1.2324 - val_accuracy: 0.4994 - val_loss: 1.3634 - learning_rate: 3.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5175 - loss: 1.2332\n",
            "Epoch 23: val_loss improved from 1.25062 to 1.18797, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5175 - loss: 1.2332 - val_accuracy: 0.5462 - val_loss: 1.1880 - learning_rate: 3.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5385 - loss: 1.1997\n",
            "Epoch 24: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5385 - loss: 1.1997 - val_accuracy: 0.5226 - val_loss: 1.2462 - learning_rate: 3.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5252 - loss: 1.2096\n",
            "Epoch 25: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5252 - loss: 1.2096 - val_accuracy: 0.5403 - val_loss: 1.1969 - learning_rate: 3.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5330 - loss: 1.1798\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.18797\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5331 - loss: 1.1799 - val_accuracy: 0.4652 - val_loss: 1.4490 - learning_rate: 3.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5449 - loss: 1.1565\n",
            "Epoch 27: val_loss improved from 1.18797 to 1.16197, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5449 - loss: 1.1564 - val_accuracy: 0.5551 - val_loss: 1.1620 - learning_rate: 1.5000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5511 - loss: 1.1399\n",
            "Epoch 28: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5511 - loss: 1.1399 - val_accuracy: 0.5577 - val_loss: 1.1722 - learning_rate: 1.5000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5609 - loss: 1.1229\n",
            "Epoch 29: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5608 - loss: 1.1229 - val_accuracy: 0.5570 - val_loss: 1.1684 - learning_rate: 1.5000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5578 - loss: 1.1183\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.16197\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.5578 - loss: 1.1183 - val_accuracy: 0.5557 - val_loss: 1.1780 - learning_rate: 1.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5661 - loss: 1.0890\n",
            "Epoch 31: val_loss improved from 1.16197 to 1.14360, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5661 - loss: 1.0890 - val_accuracy: 0.5624 - val_loss: 1.1436 - learning_rate: 7.5000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5757 - loss: 1.0726\n",
            "Epoch 32: val_loss improved from 1.14360 to 1.13069, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5756 - loss: 1.0726 - val_accuracy: 0.5762 - val_loss: 1.1307 - learning_rate: 7.5000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5796 - loss: 1.0696\n",
            "Epoch 33: val_loss improved from 1.13069 to 1.10665, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5796 - loss: 1.0696 - val_accuracy: 0.5893 - val_loss: 1.1066 - learning_rate: 7.5000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5738 - loss: 1.0807\n",
            "Epoch 34: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5738 - loss: 1.0807 - val_accuracy: 0.5750 - val_loss: 1.1247 - learning_rate: 7.5000e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5782 - loss: 1.0498\n",
            "Epoch 35: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 55ms/step - accuracy: 0.5782 - loss: 1.0499 - val_accuracy: 0.5673 - val_loss: 1.1396 - learning_rate: 7.5000e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5817 - loss: 1.0479\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
            "\n",
            "Epoch 36: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5817 - loss: 1.0479 - val_accuracy: 0.5743 - val_loss: 1.1480 - learning_rate: 7.5000e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5739 - loss: 1.0548\n",
            "Epoch 37: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 56ms/step - accuracy: 0.5740 - loss: 1.0547 - val_accuracy: 0.5825 - val_loss: 1.1112 - learning_rate: 3.7500e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m358/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5836 - loss: 1.0402\n",
            "Epoch 38: val_loss did not improve from 1.10665\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.5837 - loss: 1.0402 - val_accuracy: 0.5757 - val_loss: 1.1322 - learning_rate: 3.7500e-05\n",
            "Epoch 38: early stopping\n",
            "Restoring model weights from the end of the best epoch: 33.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5443 - loss: 1.1756\n",
            "Test Accuracy: 0.5943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Augmentation for Robust Training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,  # Increased rotation for better variation\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness for more generalization\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define an Improved CNN Model with Regularization\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 4th convolution block (NEW Layer for More Depth)\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Learning rate scheduler (Exponential Decay)\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=0.0003, decay_steps=10000, decay_rate=0.9)\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgGBSZ1xF9cP",
        "outputId": "73501025-6b15-4ced-8ed3-300294708cc5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.1450 - loss: 3.1788\n",
            "Epoch 1: val_loss improved from inf to 2.65162, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 98ms/step - accuracy: 0.1450 - loss: 3.1781 - val_accuracy: 0.0369 - val_loss: 2.6516\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1588 - loss: 2.6089\n",
            "Epoch 2: val_loss improved from 2.65162 to 2.21776, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.1588 - loss: 2.6087 - val_accuracy: 0.2174 - val_loss: 2.2178\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1640 - loss: 2.4480\n",
            "Epoch 3: val_loss did not improve from 2.21776\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.1640 - loss: 2.4480 - val_accuracy: 0.0759 - val_loss: 2.3211\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1692 - loss: 2.4305\n",
            "Epoch 4: val_loss improved from 2.21776 to 2.17819, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.1692 - loss: 2.4303 - val_accuracy: 0.1926 - val_loss: 2.1782\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1820 - loss: 2.3070\n",
            "Epoch 5: val_loss improved from 2.17819 to 2.13014, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.1820 - loss: 2.3070 - val_accuracy: 0.2148 - val_loss: 2.1301\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1939 - loss: 2.2459\n",
            "Epoch 6: val_loss did not improve from 2.13014\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.1939 - loss: 2.2459 - val_accuracy: 0.2050 - val_loss: 2.1953\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2014 - loss: 2.2014\n",
            "Epoch 7: val_loss did not improve from 2.13014\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2014 - loss: 2.2014 - val_accuracy: 0.2097 - val_loss: 2.1389\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2206 - loss: 2.1742\n",
            "Epoch 8: val_loss improved from 2.13014 to 2.12282, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2206 - loss: 2.1741 - val_accuracy: 0.2534 - val_loss: 2.1228\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2246 - loss: 2.1794\n",
            "Epoch 9: val_loss did not improve from 2.12282\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2247 - loss: 2.1792 - val_accuracy: 0.1735 - val_loss: 2.2123\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2392 - loss: 2.0955\n",
            "Epoch 10: val_loss improved from 2.12282 to 2.09369, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2393 - loss: 2.0954 - val_accuracy: 0.2656 - val_loss: 2.0937\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2695 - loss: 2.0577\n",
            "Epoch 11: val_loss improved from 2.09369 to 2.02661, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.2696 - loss: 2.0577 - val_accuracy: 0.2777 - val_loss: 2.0266\n",
            "Epoch 12/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2924 - loss: 2.0163\n",
            "Epoch 12: val_loss improved from 2.02661 to 1.98876, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2924 - loss: 2.0163 - val_accuracy: 0.3219 - val_loss: 1.9888\n",
            "Epoch 13/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3250 - loss: 1.9750\n",
            "Epoch 13: val_loss did not improve from 1.98876\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.3250 - loss: 1.9750 - val_accuracy: 0.2898 - val_loss: 2.0198\n",
            "Epoch 14/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3575 - loss: 1.8810\n",
            "Epoch 14: val_loss improved from 1.98876 to 1.84533, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.3575 - loss: 1.8810 - val_accuracy: 0.3809 - val_loss: 1.8453\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3814 - loss: 1.8215\n",
            "Epoch 15: val_loss improved from 1.84533 to 1.83866, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.3814 - loss: 1.8215 - val_accuracy: 0.4121 - val_loss: 1.8387\n",
            "Epoch 16/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3982 - loss: 1.7994\n",
            "Epoch 16: val_loss improved from 1.83866 to 1.80054, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3982 - loss: 1.7994 - val_accuracy: 0.3999 - val_loss: 1.8005\n",
            "Epoch 17/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4289 - loss: 1.7374\n",
            "Epoch 17: val_loss improved from 1.80054 to 1.72657, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.4289 - loss: 1.7374 - val_accuracy: 0.4241 - val_loss: 1.7266\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4247 - loss: 1.7284\n",
            "Epoch 18: val_loss improved from 1.72657 to 1.64878, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4247 - loss: 1.7283 - val_accuracy: 0.4632 - val_loss: 1.6488\n",
            "Epoch 19/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4412 - loss: 1.6877\n",
            "Epoch 19: val_loss improved from 1.64878 to 1.64264, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.4412 - loss: 1.6876 - val_accuracy: 0.4726 - val_loss: 1.6426\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4508 - loss: 1.6229\n",
            "Epoch 20: val_loss did not improve from 1.64264\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4508 - loss: 1.6230 - val_accuracy: 0.4590 - val_loss: 1.6594\n",
            "Epoch 21/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4609 - loss: 1.6166\n",
            "Epoch 21: val_loss did not improve from 1.64264\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4609 - loss: 1.6167 - val_accuracy: 0.4626 - val_loss: 1.6501\n",
            "Epoch 22/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4629 - loss: 1.6124\n",
            "Epoch 22: val_loss improved from 1.64264 to 1.56227, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4630 - loss: 1.6124 - val_accuracy: 0.4882 - val_loss: 1.5623\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4698 - loss: 1.5933\n",
            "Epoch 23: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4698 - loss: 1.5933 - val_accuracy: 0.4539 - val_loss: 1.6223\n",
            "Epoch 24/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4769 - loss: 1.5934\n",
            "Epoch 24: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4769 - loss: 1.5934 - val_accuracy: 0.4959 - val_loss: 1.5669\n",
            "Epoch 25/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4881 - loss: 1.5530\n",
            "Epoch 25: val_loss did not improve from 1.56227\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4881 - loss: 1.5530 - val_accuracy: 0.4731 - val_loss: 1.6175\n",
            "Epoch 26/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4925 - loss: 1.5220\n",
            "Epoch 26: val_loss improved from 1.56227 to 1.48088, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4925 - loss: 1.5221 - val_accuracy: 0.5135 - val_loss: 1.4809\n",
            "Epoch 27/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4951 - loss: 1.5360\n",
            "Epoch 27: val_loss did not improve from 1.48088\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4951 - loss: 1.5360 - val_accuracy: 0.4961 - val_loss: 1.5361\n",
            "Epoch 28/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4981 - loss: 1.4936\n",
            "Epoch 28: val_loss did not improve from 1.48088\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4981 - loss: 1.4936 - val_accuracy: 0.5086 - val_loss: 1.4955\n",
            "Epoch 29/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5104 - loss: 1.4530\n",
            "Epoch 29: val_loss improved from 1.48088 to 1.47958, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5104 - loss: 1.4531 - val_accuracy: 0.5255 - val_loss: 1.4796\n",
            "Epoch 30/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5062 - loss: 1.4853\n",
            "Epoch 30: val_loss improved from 1.47958 to 1.43269, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5062 - loss: 1.4853 - val_accuracy: 0.5327 - val_loss: 1.4327\n",
            "Epoch 31/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5147 - loss: 1.4393\n",
            "Epoch 31: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5147 - loss: 1.4394 - val_accuracy: 0.5245 - val_loss: 1.4633\n",
            "Epoch 32/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5096 - loss: 1.4717\n",
            "Epoch 32: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5097 - loss: 1.4717 - val_accuracy: 0.4898 - val_loss: 1.5543\n",
            "Epoch 33/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5159 - loss: 1.4559\n",
            "Epoch 33: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5159 - loss: 1.4559 - val_accuracy: 0.5280 - val_loss: 1.4484\n",
            "Epoch 34/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5236 - loss: 1.4316\n",
            "Epoch 34: val_loss did not improve from 1.43269\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5236 - loss: 1.4316 - val_accuracy: 0.4665 - val_loss: 1.6363\n",
            "Epoch 35/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5309 - loss: 1.4254\n",
            "Epoch 35: val_loss improved from 1.43269 to 1.39511, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5309 - loss: 1.4254 - val_accuracy: 0.5482 - val_loss: 1.3951\n",
            "Epoch 36/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5368 - loss: 1.4065\n",
            "Epoch 36: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5367 - loss: 1.4065 - val_accuracy: 0.5419 - val_loss: 1.4097\n",
            "Epoch 37/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5250 - loss: 1.4155\n",
            "Epoch 37: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5250 - loss: 1.4155 - val_accuracy: 0.5388 - val_loss: 1.4197\n",
            "Epoch 38/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5434 - loss: 1.3781\n",
            "Epoch 38: val_loss did not improve from 1.39511\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5434 - loss: 1.3782 - val_accuracy: 0.5196 - val_loss: 1.4895\n",
            "Epoch 39/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5404 - loss: 1.4018\n",
            "Epoch 39: val_loss improved from 1.39511 to 1.34416, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.5404 - loss: 1.4018 - val_accuracy: 0.5725 - val_loss: 1.3442\n",
            "Epoch 40/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5460 - loss: 1.3532\n",
            "Epoch 40: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5460 - loss: 1.3532 - val_accuracy: 0.5449 - val_loss: 1.3951\n",
            "Epoch 41/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5485 - loss: 1.3410\n",
            "Epoch 41: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5484 - loss: 1.3411 - val_accuracy: 0.5374 - val_loss: 1.4106\n",
            "Epoch 42/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5504 - loss: 1.3588\n",
            "Epoch 42: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5504 - loss: 1.3589 - val_accuracy: 0.5506 - val_loss: 1.3793\n",
            "Epoch 43/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5486 - loss: 1.3411\n",
            "Epoch 43: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5486 - loss: 1.3411 - val_accuracy: 0.5248 - val_loss: 1.4587\n",
            "Epoch 44/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5521 - loss: 1.3597\n",
            "Epoch 44: val_loss did not improve from 1.34416\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5521 - loss: 1.3597 - val_accuracy: 0.5645 - val_loss: 1.3472\n",
            "Epoch 44: early stopping\n",
            "Restoring model weights from the end of the best epoch: 39.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5742 - loss: 1.2949\n",
            "Test Accuracy: 0.5917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions\n",
        "img_size = (48, 48)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Optimized Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,  # Balanced rotation\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1,  # Reduced zoom\n",
        "    brightness_range=[0.9, 1.1],  # Reduced brightness variation\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Improved CNN Model with Adjustments\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005), input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.00005)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Learning rate scheduler with ReduceLROnPlateau\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaGI_EUYJ9Pv",
        "outputId": "b99d2124-f04b-4910-c70b-2a79cc3e7ba5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.1639 - loss: 2.6336\n",
            "Epoch 1: val_loss improved from inf to 2.03282, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 94ms/step - accuracy: 0.1640 - loss: 2.6331 - val_accuracy: 0.2090 - val_loss: 2.0328 - learning_rate: 3.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2058 - loss: 2.1711\n",
            "Epoch 2: val_loss improved from 2.03282 to 1.91542, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.2058 - loss: 2.1711 - val_accuracy: 0.2555 - val_loss: 1.9154 - learning_rate: 3.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2312 - loss: 2.0901\n",
            "Epoch 3: val_loss did not improve from 1.91542\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.2313 - loss: 2.0900 - val_accuracy: 0.2066 - val_loss: 1.9691 - learning_rate: 3.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2821 - loss: 1.9458\n",
            "Epoch 4: val_loss improved from 1.91542 to 1.85483, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.2822 - loss: 1.9458 - val_accuracy: 0.3160 - val_loss: 1.8548 - learning_rate: 3.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3104 - loss: 1.8827\n",
            "Epoch 5: val_loss improved from 1.85483 to 1.72125, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3104 - loss: 1.8826 - val_accuracy: 0.3661 - val_loss: 1.7212 - learning_rate: 3.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3388 - loss: 1.8435\n",
            "Epoch 6: val_loss improved from 1.72125 to 1.70264, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3388 - loss: 1.8434 - val_accuracy: 0.3759 - val_loss: 1.7026 - learning_rate: 3.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3674 - loss: 1.7430\n",
            "Epoch 7: val_loss improved from 1.70264 to 1.62609, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.3674 - loss: 1.7430 - val_accuracy: 0.4221 - val_loss: 1.6261 - learning_rate: 3.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3933 - loss: 1.6850\n",
            "Epoch 8: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.3933 - loss: 1.6850 - val_accuracy: 0.4116 - val_loss: 1.6515 - learning_rate: 3.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4079 - loss: 1.6420\n",
            "Epoch 9: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4079 - loss: 1.6420 - val_accuracy: 0.3316 - val_loss: 2.0128 - learning_rate: 3.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4224 - loss: 1.5919\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.62609\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4224 - loss: 1.5919 - val_accuracy: 0.3863 - val_loss: 1.6848 - learning_rate: 3.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4481 - loss: 1.5082\n",
            "Epoch 11: val_loss improved from 1.62609 to 1.52046, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4481 - loss: 1.5082 - val_accuracy: 0.4543 - val_loss: 1.5205 - learning_rate: 1.5000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4591 - loss: 1.4773\n",
            "Epoch 12: val_loss improved from 1.52046 to 1.50334, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4591 - loss: 1.4773 - val_accuracy: 0.4614 - val_loss: 1.5033 - learning_rate: 1.5000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4699 - loss: 1.4582\n",
            "Epoch 13: val_loss improved from 1.50334 to 1.43579, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4699 - loss: 1.4583 - val_accuracy: 0.4877 - val_loss: 1.4358 - learning_rate: 1.5000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4779 - loss: 1.4603\n",
            "Epoch 14: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4779 - loss: 1.4603 - val_accuracy: 0.4719 - val_loss: 1.4896 - learning_rate: 1.5000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4841 - loss: 1.4451\n",
            "Epoch 15: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.4841 - loss: 1.4451 - val_accuracy: 0.4767 - val_loss: 1.4822 - learning_rate: 1.5000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4826 - loss: 1.4467\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.43579\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4826 - loss: 1.4466 - val_accuracy: 0.4525 - val_loss: 1.5400 - learning_rate: 1.5000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4987 - loss: 1.3859\n",
            "Epoch 17: val_loss improved from 1.43579 to 1.39797, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4987 - loss: 1.3859 - val_accuracy: 0.4992 - val_loss: 1.3980 - learning_rate: 7.5000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5043 - loss: 1.3826\n",
            "Epoch 18: val_loss improved from 1.39797 to 1.37618, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5043 - loss: 1.3826 - val_accuracy: 0.5079 - val_loss: 1.3762 - learning_rate: 7.5000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5129 - loss: 1.3676\n",
            "Epoch 19: val_loss improved from 1.37618 to 1.35173, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.5128 - loss: 1.3676 - val_accuracy: 0.5184 - val_loss: 1.3517 - learning_rate: 7.5000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5061 - loss: 1.3695\n",
            "Epoch 20: val_loss did not improve from 1.35173\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5061 - loss: 1.3694 - val_accuracy: 0.4928 - val_loss: 1.4369 - learning_rate: 7.5000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5117 - loss: 1.3512\n",
            "Epoch 21: val_loss improved from 1.35173 to 1.33370, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 71ms/step - accuracy: 0.5117 - loss: 1.3512 - val_accuracy: 0.5255 - val_loss: 1.3337 - learning_rate: 7.5000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5133 - loss: 1.3398\n",
            "Epoch 22: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5133 - loss: 1.3397 - val_accuracy: 0.5152 - val_loss: 1.3509 - learning_rate: 7.5000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5201 - loss: 1.3236\n",
            "Epoch 23: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5201 - loss: 1.3236 - val_accuracy: 0.5222 - val_loss: 1.3495 - learning_rate: 7.5000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5325 - loss: 1.3053\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5325 - loss: 1.3053 - val_accuracy: 0.5027 - val_loss: 1.4121 - learning_rate: 7.5000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5240 - loss: 1.3164\n",
            "Epoch 25: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5240 - loss: 1.3164 - val_accuracy: 0.5281 - val_loss: 1.3383 - learning_rate: 3.7500e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5325 - loss: 1.2997\n",
            "Epoch 26: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.5325 - loss: 1.2997 - val_accuracy: 0.5145 - val_loss: 1.3632 - learning_rate: 3.7500e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5228 - loss: 1.2952\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.8750000890577212e-05.\n",
            "\n",
            "Epoch 27: val_loss did not improve from 1.33370\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 69ms/step - accuracy: 0.5228 - loss: 1.2952 - val_accuracy: 0.5182 - val_loss: 1.3469 - learning_rate: 3.7500e-05\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5185 - loss: 1.3318\n",
            "Test Accuracy: 0.5515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from kagglehub import dataset_download\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import HeNormal\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download the latest version of the FER-2013 dataset\n",
        "dataset_path = dataset_download(\"msambare/fer2013\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "print(\"Files in dataset:\", os.listdir(dataset_path))\n",
        "\n",
        "# Image dimensions (Increased to 64x64 for better feature extraction)\n",
        "img_size = (64, 64)\n",
        "batch_size = 64\n",
        "\n",
        "# Define ImageDataGenerator with Enhanced Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training and validation data in grayscale\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"train\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test data (without augmentation) in grayscale\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(dataset_path, \"test\"),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Compute class weights for handling imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define Enhanced CNN Model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st convolution block\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=HeNormal(), input_shape=(64,64,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 2nd convolution block\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution block\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 4th convolution block (Added for feature extraction)\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer=HeNormal()))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile Model with RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(\"best_emotion_cnn.keras\", save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# Train the model with class weights\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "model.save(\"emotion_cnn_improved.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w26am5zSR9b",
        "outputId": "9e171db9-79c2-4f93-b88c-5e2f6734910a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n",
            "Files in dataset: ['train', 'test']\n",
            "Found 22968 images belonging to 7 classes.\n",
            "Found 5741 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.1503 - loss: 2.9070\n",
            "Epoch 1: val_loss improved from inf to 2.01130, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 111ms/step - accuracy: 0.1503 - loss: 2.9068 - val_accuracy: 0.2230 - val_loss: 2.0113 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1592 - loss: 2.5697\n",
            "Epoch 2: val_loss improved from 2.01130 to 1.99542, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1592 - loss: 2.5696 - val_accuracy: 0.1761 - val_loss: 1.9954 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1587 - loss: 2.4192\n",
            "Epoch 3: val_loss did not improve from 1.99542\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1588 - loss: 2.4189 - val_accuracy: 0.1113 - val_loss: 2.0749 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1675 - loss: 2.1813\n",
            "Epoch 4: val_loss improved from 1.99542 to 1.95092, saving model to best_emotion_cnn.keras\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 82ms/step - accuracy: 0.1675 - loss: 2.1814 - val_accuracy: 0.1561 - val_loss: 1.9509 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.1727 - loss: 2.1480\n",
            "Epoch 5: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 83ms/step - accuracy: 0.1727 - loss: 2.1481 - val_accuracy: 0.0848 - val_loss: 2.1698 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1852 - loss: 2.1306\n",
            "Epoch 6: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1852 - loss: 2.1307 - val_accuracy: 0.0625 - val_loss: 2.2612 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1868 - loss: 2.0989\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1868 - loss: 2.0989 - val_accuracy: 0.0294 - val_loss: 2.9709 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1897 - loss: 2.0635\n",
            "Epoch 8: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1897 - loss: 2.0634 - val_accuracy: 0.0228 - val_loss: 2.6410 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1987 - loss: 2.0550\n",
            "Epoch 9: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1987 - loss: 2.0549 - val_accuracy: 0.0214 - val_loss: 2.6994 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1978 - loss: 2.0270\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.95092\n",
            "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.1979 - loss: 2.0270 - val_accuracy: 0.0726 - val_loss: 2.3006 - learning_rate: 5.0000e-05\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1359 - loss: 2.0298\n",
            "Test Accuracy: 0.2034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-dQBE5SSXWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sExqxqv5Q3V-",
        "mkpL-YBrpur8",
        "BJB0ZrdbTlyv",
        "CC4ItVgVT3QP",
        "txPzADAIScWf",
        "NCkLrfnrgc9w",
        "q8TzUNT5lk8R",
        "6DdwJKaeuVU0",
        "AR4r_CcUqpMa",
        "qKpuKTfdq3U6",
        "SsHrBLpXsMAK",
        "nl9gP7nDserC"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}