{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14v0zcA0Pg0OUn701f6N_ne7iZeVrwAaA","timestamp":1611499541807}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mX8gZlVyCCbz"},"source":["# Image classification with Convolutional Neural Networks\n","### Machine Learning in Industrial Environments\n","\n","Let's classify Simpsons's characters.\n","\n","\n","<center><img src=\"https://i.imgur.com/i8zIGqX.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n","\n","The dataset of Simpson's characters have been taken from the series. The author is [Alexandre Attia](http://www.alexattia.fr/) and it is much more complex of Fashion-MNIST with 18 classes and characters in different poses\n","\n","The training dataset can de downloaded here:\n","\n","[Training data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60) (~500MB)\n","\n","The test dataset can de downloaded here:\n","\n","[Test data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8) (~10MB)\n","\n","\n","## In this case, we are going to reduce the dataset to a 10% of the original one. We will see how bad our CNN is going to work now, and we will see that some extra magic can be done with transfer learning, using a good and well-proved CNN architecture.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QI274F8LQC59"},"source":["## Loading the data"]},{"cell_type":"code","metadata":{"id":"D7tKOZ9BFfki"},"source":["import cv2\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import glob\n","\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# An interesting option to see the evolution of the training\n","# process while we are fitting the model, so we can stop\n","# the training if, e.g., we detect overfitting\n","class PlotLearning(keras.callbacks.Callback):\n","    \"\"\"\n","    Callback to plot the learning curves of the model during training.\n","    \"\"\"\n","    def on_train_begin(self, logs={}):\n","        self.metrics = {}\n","        for metric in logs:\n","            self.metrics[metric] = []\n","\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        # Storing metrics\n","        for metric in logs:\n","            if metric in self.metrics:\n","                self.metrics[metric].append(logs.get(metric))\n","            else:\n","                self.metrics[metric] = [logs.get(metric)]\n","\n","        # Plotting\n","        metrics = [x for x in logs if 'val' not in x]\n","\n","        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n","        clear_output(wait=True)\n","\n","        for i, metric in enumerate(metrics):\n","            axs[i].plot(range(1, epoch + 2),\n","                        self.metrics[metric],\n","                        label=metric)\n","            if logs['val_' + metric]:\n","                axs[i].plot(range(1, epoch + 2),\n","                            self.metrics['val_' + metric],\n","                            label='val_' + metric)\n","\n","            axs[i].legend()\n","            axs[i].grid()\n","\n","        plt.tight_layout()\n","        plt.show()"],"metadata":{"id":"ZCCG0hrEymWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VZ6YNXm8SAR"},"source":["# Let's import the files\n","!wget -cq \"https://www.dropbox.com/s/r4d90m3kafifx5g/simpsons_train.tar.gz\"\n","!wget -cq \"https://www.dropbox.com/s/8n895y90r5qe6gv/simpsons_test.tar.gz\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"scI2n6Ek7rPM"},"source":["# Uncompress\n","!mkdir datasets\n","!tar -xzf simpsons_train.tar.gz -C ./datasets\n","!tar -xzf simpsons_test.tar.gz -C ./datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMFhe3COFwSD"},"source":["# The 18 characters we are going to work with.\n","MAP_CHARACTERS = {\n","    0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n","    3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel',\n","    7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson',\n","    11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak',\n","    14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'\n","}\n","\n","# We will work with 64x64 images\n","IMG_SIZE = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bJ0NsbCbupF"},"source":["def load_train_set(dirname, map_characters, verbose=True):\n","    \"\"\"This function loads the training data images.\n","\n","    As they have different sizes, we resize them to IMG_SIZE x IMG_SIZE with opencv\n","\n","    Args:\n","        dirname: complete path to the data\n","        map_characters: variable that maps characters\n","        verbose: if it is True, shows info about loaded images\n","\n","    Returns:\n","        X, y: X is an array with all of the images with a size of IMG_SIZE x IMG_SIZE\n","              y is an array with the labels that correspond to each image\n","    \"\"\"\n","    X_train = []\n","    y_train = []\n","    for label, character in map_characters.items():\n","        files = os.listdir(os.path.join(dirname, character))\n","        images = [file for file in files if file.endswith(\"jpg\")]\n","        if verbose:\n","          print(\"Reading {} images found at {}\".format(len(images), character))\n","        for image_name in images:\n","            image = cv2.imread(os.path.join(dirname, character, image_name))\n","            X_train.append(cv2.resize(image,(IMG_SIZE, IMG_SIZE)))\n","            y_train.append(label)\n","    return np.array(X_train), np.array(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NslxhnnDK6uA"},"source":["def load_test_set(dirname, map_characters, verbose=True):\n","    \"\"\"Equivalent to the previous function but for the test images.\"\"\"\n","    X_test = []\n","    y_test = []\n","    reverse_dict = {v: k for k, v in map_characters.items()}\n","    for filename in glob.glob(dirname + '/*.*'):\n","        char_name = \"_\".join(filename.split('/')[-1].split('_')[:-1])\n","        if char_name in reverse_dict:\n","            image = cv2.imread(filename)\n","            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n","            X_test.append(image)\n","            y_test.append(reverse_dict[char_name])\n","    if verbose:\n","        print(\"Read {} images from test\".format(len(X_test)))\n","    return np.array(X_test), np.array(y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVWqKxFcbwTu"},"source":["# We load the data. Specific for colab.\n","DATASET_TRAIN_PATH_COLAB = \"./datasets/simpsons\"\n","DATASET_TEST_PATH_COLAB = \"./datasets/simpsons_testset\"\n","\n","X, y = load_train_set(DATASET_TRAIN_PATH_COLAB, MAP_CHARACTERS)\n","X_t, y_t = load_test_set(DATASET_TEST_PATH_COLAB, MAP_CHARACTERS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GY4vTFyfffv"},"source":["# Let's shuffle by permutation the elements in test and training to maximize statistical accuracy.\n","perm = np.random.permutation(len(X))\n","X, y = X[perm], y[perm]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## We take just the 10% of the dataset"],"metadata":{"id":"IbyUcFCMoyb3"}},{"cell_type":"code","source":["# What the results are if we use a small training dataset?\n","# For example a just 10% of the original one?\n","X = X[0:int(len(X)*0.1)]\n","y = y[0:int(len(y)*0.1)]"],"metadata":{"id":"a9vQ1iuVVtab"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xgvrz4VNgJ9b"},"source":["def plot_acc(history, title=\"Model Accuracy\"):\n","    \"\"\"Accuracy on training\"\"\"\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title(title)\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Val'], loc='upper left')\n","    plt.show()\n","\n","def plot_loss(history, title=\"Model Loss\"):\n","    \"\"\"Loss on training\"\"\"\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title(title)\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Val'], loc='upper right')\n","    plt.show()\n","\n","def plot_compare_losses(history1, history2, name1=\"Red 1\",\n","                        name2=\"Red 2\", title=\"Graph title\"):\n","    \"\"\"Comparing losses\"\"\"\n","    plt.plot(history1.history['loss'], color=\"green\")\n","    plt.plot(history1.history['val_loss'], 'r--', color=\"green\")\n","    plt.plot(history2.history['loss'], color=\"blue\")\n","    plt.plot(history2.history['val_loss'], 'r--', color=\"blue\")\n","    plt.title(title)\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train ' + name1, 'Val ' + name1,\n","                'Train ' + name2, 'Val ' + name2],\n","               loc='upper right')\n","    plt.show()\n","\n","def plot_compare_accs(history1, history2, name1=\"Red 1\",\n","                      name2=\"Red 2\", title=\"Graph title\"):\n","    \"\"\"Comparing accuracies\"\"\"\n","    plt.plot(history1.history['accuracy'], color=\"green\")\n","    plt.plot(history1.history['val_accuracy'], 'r--', color=\"green\")\n","    plt.plot(history2.history['accuracy'], color=\"blue\")\n","    plt.plot(history2.history['val_accuracy'], 'r--', color=\"blue\")\n","    plt.title(title)\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train ' + name1, 'Val ' + name1,\n","                'Train ' + name2, 'Val ' + name2],\n","               loc='lower right')\n","    plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awX0zpZ8caPB"},"source":["**Information about the data ...**"]},{"cell_type":"code","metadata":{"id":"vZwsiZz8XvMp"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mtBucMSfcnvd"},"source":["So, 1899 images of 64x64 in RGB (3 channels)"]},{"cell_type":"code","metadata":{"id":"40ImhlG2bPIu"},"source":["y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"of2st0XVcBXe"},"source":["print('The character ', y[0], ' is ', MAP_CHARACTERS[y[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfK83C0scD4Q"},"source":["def visualize_example(x):\n","    plt.figure()\n","    plt.imshow(x)\n","    plt.colorbar()\n","    plt.grid(False)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_Ccxpglc7rG"},"source":["visualize_example(X[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NcphQrjndkkP"},"source":["**Normalization**"]},{"cell_type":"code","metadata":{"id":"DomE18kVdnRP"},"source":["X = X / 255.0\n","X_t = X_t / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAn4DNugd_Sn"},"source":["visualize_example(X[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M02IYOSqjASF"},"source":["**Creating the model**"]},{"cell_type":"code","source":["from keras import Sequential"],"metadata":{"id":"poJaEEdnva1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoW9uE-Lew7e"},"source":["# Variables with required data\n","batch_size = 128\n","num_classes = 18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qi7jX2pdeEWe"},"source":["# One-hot enconding\n","y = keras.utils.to_categorical(y, num_classes)\n","y_t = keras.utils.to_categorical(y_t, num_classes)\n","\n","X = X.astype('float32')\n","X_t = X_t.astype('float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEvC3C2ke5d0"},"source":["model = Sequential()\n","\n","#################################################\n","# A good try\n","#################################################\n","\n","\n","lr = 0.001\n","epochs = 100\n","\n","model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(64,64,3), name='input')) #, input_shape=(64,64,3)\n","model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n","\n","model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n","model.add(keras.layers.Dropout(0.4))\n","\n","model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n","model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n","\n","model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n","model.add(keras.layers.Dropout(0.5))\n","\n","model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n","model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n","\n","model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n","model.add(keras.layers.Dropout(0.5))\n","\n","model.add(keras.layers.Flatten())\n","model.add(keras.layers.Dense(512, activation='relu'))\n","model.add(keras.layers.Dropout(0.5))\n","model.add(keras.layers.Dense(128, activation='relu'))\n","\n","model.add(keras.layers.Dense(18, activation='softmax', name=\"output\"))  # 18 categorías\n","\n","\n","# Take a look at the model summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ap5b24-TibRI"},"source":["from tensorflow.keras.optimizers import Adam\n","\n","adam = Adam(lr=lr)\n","\n","model.compile(loss='categorical_crossentropy',\n","             optimizer=adam,\n","             metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWGxqEZcidtt"},"source":["# Callback for early stopping\n","callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","# Training with 20% of validation\n","history = model.fit(X, y,\n","                   batch_size = batch_size,\n","                   epochs=epochs,\n","                   verbose=1,\n","                   callbacks=[PlotLearning(), callback_early_stopping],\n","                   validation_split = 0.2)  # 20% validation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcVP1FKWmLt1"},"source":["plot_acc(history)\n","plot_loss(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_QaFvgdyyjv"},"source":["**Evaluation with test data**"]},{"cell_type":"code","metadata":{"id":"_OX9RfyTmPDB"},"source":["score = model.evaluate(X_t, y_t, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3ZtG6ttZDZe"},"source":["**Visualizing the model**"]},{"cell_type":"code","metadata":{"id":"TWTHI1Y5ZHBd"},"source":["from tensorflow.keras.utils import plot_model\n","plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkT0ehURSiST"},"source":["model.save('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's solve it with transfer learning"],"metadata":{"id":"lLD-nVGJYrmd"}},{"cell_type":"code","source":["from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.applications.mobilenet import preprocess_input"],"metadata":{"id":"Mvz4rK6cYqJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2"],"metadata":{"id":"De6TzYQbY8Mr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Resize a numpy array of images to a new size\n","def resize_images(images, new_size):\n","    resized_images = np.zeros((images.shape[0], new_size[0], new_size[1], images.shape[3]))\n","    for i in range(images.shape[0]):\n","        resized_images[i] = cv2.resize(images[i], new_size)\n","    return resized_images"],"metadata":{"id":"usvRn0A6hQRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"wgofESmcac3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = resize_images(X, (224, 224))"],"metadata":{"id":"fTZpu5VXZUVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"Fk283ptSb1y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transfer_model = MobileNet(weights='imagenet',include_top=False)"],"metadata":{"id":"pIQ85m4wiMSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for layer in transfer_model.layers:\n","    transfer_model.trainable=False"],"metadata":{"id":"fA-PxP68l6w0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = transfer_model.output\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","x = keras.layers.Dropout(0.5)(x)\n","x = keras.layers.Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n","x = keras.layers.Dropout(0.4)(x)\n","x = keras.layers.Dense(1024,activation='relu')(x) #dense layer 2\n","x = keras.layers.Dropout(0.3)(x)\n","x = keras.layers.Dense(512,activation='relu')(x) #dense layer 3\n","preds = keras.layers.Dense(18,activation='softmax')(x)"],"metadata":{"id":"6xoy-eWGiTF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.Model(inputs=transfer_model.input, outputs=preds)"],"metadata":{"id":"PKNkNCsXi8KZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"],"metadata":{"id":"8Svey-3Bjk9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"jTOTrRWBj0He"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callback for early stopping\n","callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","# Training with 20% of validation\n","history = model.fit(X, y,\n","                   batch_size = batch_size,\n","                   epochs=epochs,\n","                   verbose=1,\n","                   callbacks=[PlotLearning(), callback_early_stopping],\n","                   validation_split = 0.2)  # 20% validation"],"metadata":{"id":"mPRHWR5Oj45i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghHc-eV1qPDq"},"source":["**Evaluation with test data**"]},{"cell_type":"code","source":["X_t = resize_images(X_t, (224, 224))"],"metadata":{"id":"VpptDRJHqcpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pC3bDKvfqPDq"},"source":["score = model.evaluate(X_t, y_t, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]}]}